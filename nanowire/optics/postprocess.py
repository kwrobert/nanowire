import os
import os.path as osp
import glob
import sys
import time
import copy
import logging
import multiprocessing as mp
import matplotlib
try:
    os.environ['DISPLAY']
except KeyError:
    matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.cm as cmx
import matplotlib.lines as mlines
import matplotlib.patches as mpatches
import scipy.constants as consts
import scipy.integrate as intg
import numpy as np
import naturalneighbor as nn
from mpl_toolkits.mplot3d import Axes3D
from itertools import repeat
from scipy import interpolate
from sympy import Circle
from nanowire.utils.config import Config, load_confs
from nanowire.utils.utils import (
    cartesian_product,
    arithmetic_arange
)
from nanowire.utils.logging import (
    IdFilter,
)
from nanowire.optics.utils.utils import (
    get_nk,
    get_incident_power,
    get_incident_amplitude,
)
from nanowire.optics.simulate import Simulator
from nanowire.utils.data_manager import DataManager, HDF5DataManager
from nanowire.utils.geometry import (
    Layer,
    get_mask_by_shape,
    get_mask_by_material,
    get_layers
)

# Configure logging for this module
# Get numeric level safely
logfile = 'logs/postprocess.log'
debug = getattr(logging, 'debug'.upper(), None)
info = getattr(logging, 'info'.upper(), None)
warn = getattr(logging, 'warn'.upper(), None)
# Set formatting
formatter = logging.Formatter('%(asctime)s [%(name)s:%(levelname)s]'
                              ' - %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')
# Create logger
logger = logging.getLogger(__name__)
logger.setLevel(info)
log_dir, logfile = osp.split(osp.expandvars(logfile))
# Set up file handler
try:
    os.makedirs(log_dir)
except OSError:
    # Log dir already exists
    pass
output_file = osp.join(log_dir, logfile)
fhandler = logging.FileHandler(output_file)
fhandler.setFormatter(formatter)
fhandler.setLevel(debug)
# We dont want log records generated by Simulation instances making it to the
# global simulate.log file
fhandler.addFilter(IdFilter(reject=True))
logger.addHandler(fhandler)
# Logging to console
ch = logging.StreamHandler()
ch.setLevel(debug)
ch.setFormatter(formatter)
ch.addFilter(IdFilter(reject=True))
logger.addHandler(ch)


# This will log any uncaught exceptions
def handle_exception(exc_type, exc_value, exc_traceback):
    if issubclass(exc_type, KeyboardInterrupt):
        sys.__excepthook__(exc_type, exc_value, exc_traceback)
        return

    logger.critical("Uncaught exception", exc_info=(exc_type, exc_value,
                                                    exc_traceback))


sys.excepthook = handle_exception


class Simulation:
    """
    An object that represents a simulation. It stores a DataManager object for
    managing the reading and writing of all data in the ``self.data``
    attribute. It also stores a Config object, which is a dict-like object
    representing the configuration for the simulation, in the ``self.conf``
    attribute. Many of the methods are for performing calculations on the data.
    """

    def __init__(self, outdir, conf=None, simulator=None):
        """
        :param :class:`~utils.config.Config`: Config object for this simulation
        """

        if conf is None and simulator is None:
            raise ValueError('Must pass in either a Config object or a'
                             ' Simulator object')
        if conf is not None:
            self.conf = conf
        else:
            self.conf = copy.deepcopy(simulator.conf)
        self.ID = self.conf['General']['sim_dir'][-10:]
        self.path = osp.abspath(osp.normpath(osp.expandvars(outdir)))
        self.base, self.dir = osp.split(self.path)
        self.fhandler = logging.FileHandler(osp.join(self.dir, 'postprocess.log'))
        self.fhandler.addFilter(IdFilter(ID=self.ID))
        formatter = logging.Formatter('%(asctime)s [%(name)s:%(levelname)s] - %(message)s',datefmt='%m/%d/%Y %I:%M:%S %p')
        self.fhandler.setFormatter(formatter)
        log = logging.getLogger(__name__)
        log.addHandler(self.fhandler)
        # Store the logger adapter to the module level logger as an attribute.
        # We use this to log in any methods, and the sim_id of this instance
        # will get stored in the log record
        self.log = logging.LoggerAdapter(log, {'ID': self.ID})
        self.log.debug('Logger initialized')
        if not self.conf['General']['save_as']:
            self.data = {}
            simulator.setup()
            self.data.update(simulator.data)
            sdict = self.conf['General']['sample_dict']
            if sdict:
                results = simulator.compute_fields_by_layer(sdict)
                self.data.update(results)
                for f in ('Ex', 'Ey', 'Ez'):
                    ks = ['{}_{}'.format(lname, f) for lname in simulator.layers.keys()]
                    self.data[f] = np.concatenate([self.data[k] for k in ks])
            else:
                Ex, Ey, Ez, Hx, Hy, Hz = simulator.compute_fields()
                self.data.update({'Ex': Ex, 'Ey': Ey, 'Ez': Ez,
                                  'Hx': Hx, 'Hy': Hy, 'Hz': Hz})
            flux_arr = simulator.compute_fluxes()
            self.data.update({'fluxes': flux_arr})
        else:
            self.data = self._get_data_manager()
        self.failed = False
        self.avgs = {}
        # Compute and store dx, dy, dz at attributes
        if type(self.conf['General']['z_samples']) == list:
            self.zsamps = len(self.conf['General']['z_samples'])
        else:
            self.zsamps = int(self.conf['General']['z_samples'])
        self.xsamps = int(self.conf['General']['x_samples'])
        self.ysamps = int(self.conf['General']['y_samples'])
        self.X = self.data['xcoords']
        self.Y = self.data['ycoords']
        self.Z = self.data['zcoords']
        self.dx = self.X[1] - self.X[0]
        self.dy = self.Y[1] - self.Y[0]
        self.dz = self.Z[1] - self.Z[0]
        self.period = self.conf['Simulation']['params']['array_period']
        self.layers = get_layers(self)
        self.height = self.get_height()

    def _get_data_manager(self):
        """
        Factory function that instantiates the correct data manager object
        depending on the file type specified in the config
        """

        ftype = self.conf['General']['save_as'].lower()
        if ftype == 'hdf5':
            return HDF5DataManager(self.conf, self.log)
        else:
            raise ValueError('Invalid file type in config')

    def write_data(self, blacklist=('normE', 'normEsquared', 'genRate')):
        """
        Writes the data. This is a simple wrapper around the write_data()
        method of the DataManager object, with some code to compute the time it
        took to perform the write operation
        """

        start = time.time()
        self.data.write_data(blacklist=blacklist)
        end = time.time()
        self.log.info('Write time: %.2f seconds', end - start)

    def get_height(self):
        """Returns the total height of the device"""
        height = 0
        for layer, ldata in self.conf['Layers'].items():
            layer_t = ldata['params']['thickness']
            height += layer_t
        return height

    def get_scalar_quantity(self, quantity):
        """
        Retrieves the entire 3D matrix for some scalar quantity

        :param str quantity: The quantity you would like to retrive (ex: 'Ex'
                             or 'normE')
        :return: A 3D numpy array shape (zsamples, xsamples, ysamples)
                 containing the specified quantity
        :rtype: np.ndarray
        :raises KeyError: If the specified quantity does not exist in the data
                          dict
        """

        self.log.debug('Retrieving scalar quantity %s', str(quantity))
        try:
            return self.data[quantity]
        except KeyError:
            self.log.error('You attempted to retrieve a quantity that does not'
                           ' exist in the data dict. Attempting to calculate')
        try:
            result = getattr(self, quantity)()
            self.extend_data(quantity, result)
            return self.data[quantity]
        except:
            self.log.error('Unable to calculate following quantity: %s',
                           quantity)
            raise

    def clear_data(self):
        """Clears all the data attributes to free up memory"""
        if isinstance(self.data, DataManager):
            self.data._update_keys(clear=True)
        else:
            self.data = {}

    def extend_data(self, quantity, new_data):
        """
        Adds a new key, value pair to the DataManager object
        """
        if quantity in self.data:
            self.log.debug("Quantity %s exists in matrix, updating", quantity)
            self.data[quantity] = new_data
        else:
            self.log.debug('Adding %s to data dict', str(quantity))
            self.data[quantity] = new_data

    def get_line(self, quantity, line_dir, c1, c2):
        """
        Gets data along a line through the 3D data array for the given quantity
        along a given direction

        :param str line_dir: Any of 'x', 'y', or 'z'. Determines the direction
                             along which the line cut is taken, the other two
                             coordinates remain fixed and are specified by c1
                             and c2.
        :param int c1: The integer index for the first fixed coordinate.
                       Indexes are in x,y, z order so if line_dir='z' then c1
                       corresponds to x
        :param int c2: The integer index for the second coordinate.
        :param str quantity: The quantity whose data array you wish to take a
                             line cut through
        """

        scalar = self.get_scalar_quantity(quantity)
        if line_dir == 'x':
            # z along rows, y along columns
            return scalar[c2, :, c1]
        elif line_dir == 'y':
            # x along columns, z along rows
            return scalar[c2, c1, :]
        elif line_dir == 'z':
            # x along rows, y along columns
            return scalar[:, c1, c2]

    def get_plane(self, quantity, plane, pval):
        """
        Gets data along a 2D plane/slice through the 3D data array for a given
        quantity

        :param str plane: Any of 'xy', 'yz', or 'xz'. Determines the plane
                          along which the slice is taken
        :param int pval: The index along the final unspecified direction. If
                         plane='xy' then index would index along the z
                         direction.
        :param str quantity: The quantity whose data array you wish to take a
                             line cut through
        """

        self.log.info('Retrieving plane for %s', quantity)
        scalar = self.get_scalar_quantity(quantity)
        if plane == 'yz' or plane == 'zy':
            # z along rows, y along columns
            return scalar[:, pval, :]
        elif plane == 'xz' or plane == 'zx':
            # x along columns, z along rows
            return scalar[:, :, pval]
        elif plane == 'xy' or plane == 'yx':
            # x along rows, y along columns
            return scalar[pval, :, :]

    def normE(self):
        """
        Calculates the norm of E. Adds it to the data dict for the simulation
        and also returns a 3D array
        :return: A 3D numpy array containing :math: |E|
        """

        # Get the magnitude of E and add it to our data
        E_mag = np.zeros_like(self.data['Ex'], dtype=np.float64)
        for comp in ('Ex', 'Ey', 'Ez'):
            E_mag += np.absolute(self.data[comp])**2
        self.extend_data('normE', np.sqrt(E_mag))
        return np.sqrt(E_mag)

    def normEsquared(self):
        """
        Calculates and returns normE squared. Adds it to the data dict for
        the simulation and also returns a 3D array
        :return: A 3D numpy array containing :math: |E|^2
        """

        # Get the magnitude of E and add it to our data
        E_magsq = np.zeros_like(self.data['Ex'], dtype=np.float64)
        for comp in ('Ex', 'Ey', 'Ez'):
            E_magsq += np.absolute(self.data[comp])**2
            # E_magsq += self.data[comp].real**2
        self.extend_data('normEsquared', E_magsq)
        return E_magsq

    def normH(self):
        """Calculate and returns :math: |H|"""

        H_mag = np.zeros_like(self.data['Hx'], dtype=np.float64)
        for comp in ('Hx', 'Hy', 'Hz'):
            H_mag += np.absolute(self.data[comp])
        self.extend_data('normH', H_mag)
        return H_mag

    def normHsquared(self):
        """Calculates and returns :math: |H|^2"""

        H_magsq = np.zeros_like(self.data['Hx'], dtype=np.float64)
        for comp in ('Hx', 'Hy', 'Hz'):
            H_magsq += np.absolute(self.data[comp])**2
        self.extend_data('normHsquared', H_magsq)
        return H_magsq

    def genRate(self):
        """
        Computes and returns the 3D matrix containing the generation rate.
        Returns in units of cm^-3
        """

        # We need to compute normEsquared before we can compute the generation
        # rate
        normEsq = self.get_scalar_quantity('normEsquared')
        # Prefactor for generation rate. Note we gotta convert from m^3 to
        # cm^3, hence 1e6 factor
        fact = consts.epsilon_0 / (consts.hbar * 1e6)
        gvec = np.zeros_like(normEsq)
        # Main loop to compute generation in each layer
        freq = self.conf[('Simulation', 'params', 'frequency')]
        for name, layer in self.layers.items():
            self.log.debug('LAYER: %s', name)
            self.log.debug('LAYER T: %f', layer.thickness)
            self.log.debug('START: %f', layer.start)
            self.log.debug('END: %f', layer.end)
            # Use the layer object to get the nk matrix with correct material
            # geometry
            nmat, kmat = layer.get_nk_matrix(freq, self.X, self.Y)
            gvec[layer.get_slice(self.Z)] = fact * nmat * kmat * normEsq[layer.get_slice(self.Z)]
            # gvec[layer.get_slice()] = nmat * kmat * normEsq[layer.get_slice(self.Z)]
        self.extend_data('genRate', gvec)
        return gvec

    def angularAvg(self, quantity):
        """
        Perform an angular average of some quantity for either the E or H field
        """

        try:
            quant = self.get_scalar_quantity(quantity)
        except KeyError:
            getattr(self, quantity)()
            quant = self.get_scalar_quantity(quantity)
            # Make sure we don't compute it twice
            try:
                self.conf['Postprocessing']['Cruncher'][
                    quantity]['compute'] = False
            except KeyError:
                pass
        # Get spatial discretization
        rsamp = self.conf['General']['r_samples']
        thsamp = self.conf['General']['theta_samples']
        period = self.conf['Simulation']['params']['array_period']
        # Maximum r value such that circle and square unit cell have equal area
        rmax = period / np.sqrt(np.pi)
        # Diff between rmax and unit cell boundary at point of maximum
        # difference
        delta = rmax - period / 2.0
        # Extra indices we need to expand layers by
        x_inds = int(np.ceil(delta / self.dx))
        y_inds = int(np.ceil(delta / self.dy))
        # Use periodic BCs to extend the data in the x-y plane
        ext_vals = np.zeros((quant.shape[0], quant.shape[1] +
                             2 * x_inds, quant.shape[2] + 2 * y_inds),
                            dtype=quant.dtype)
        # Left-Right extensions. This indexing madness extracts the slice we
        # want, flips it along the correct dimension then sticks in the correct
        # spot in the extended array
        ext_vals[:, x_inds:-x_inds, 0:y_inds] = quant[:, :, 0:y_inds][:, :, ::-1]
        ext_vals[:, x_inds:-x_inds, -
                 y_inds:] = quant[:, :, -y_inds:][:, :, ::-1]
        # Top-Bottom extensions
        ext_vals[:, 0:x_inds, y_inds:-
                 y_inds] = quant[:, 0:x_inds, :][:, ::-1, :]
        ext_vals[:, -x_inds:, y_inds:-
                 y_inds] = quant[:, -x_inds:, :][:, ::-1, :]
        # Corners, slightly trickier
        # Top left
        ext_vals[:, 0:x_inds, 0:y_inds] = ext_vals[
            :, x_inds:2 * x_inds, 0:y_inds][:, ::-1, :]
        # Bottom left
        ext_vals[:, -x_inds:, 0:y_inds] = ext_vals[:, -
                                                   2 * x_inds:-x_inds, 0:y_inds][:, ::-1, :]
        # Top right
        ext_vals[:, 0:x_inds, -y_inds:] = ext_vals[:,
                                                   0:x_inds, -2 * y_inds:-y_inds][:, :, ::-1]
        # Bottom right
        ext_vals[:, -x_inds:, -y_inds:] = ext_vals[:, -
                                                   x_inds:, -2 * y_inds:-y_inds][:, :, ::-1]
        # Now the center
        ext_vals[:, x_inds:-x_inds, y_inds:-y_inds] = quant[:, :, :]
        # Extend the points arrays to include these new regions
        x = np.concatenate((np.array([self.dx * i for i in
                                      range(-x_inds, 0)]), self.X,
                            np.array([self.X[-1] + self.dx * i for i in range(1, x_inds + 1)])))
        y = np.concatenate((np.array([self.dy * i for i in
                                      range(-y_inds, 0)]), self.Y,
                            np.array([self.Y[-1] + self.dy * i for i in range(1, y_inds + 1)])))
        # The points on which we have data
        points = (x, y)
        # The points corresponding to "rings" in cylindrical coordinates. Note
        # we construct these rings around the origin so we have to shift them
        # to actually correspond to the center of the nanowire
        rvec = np.linspace(0, rmax, rsamp)
        thvec = np.linspace(0, 2 * np.pi, thsamp)
        cyl_coords = np.zeros((len(rvec) * len(thvec), 2))
        start = 0
        for r in rvec:
            xring = r * np.cos(thvec)
            yring = r * np.sin(thvec)
            cyl_coords[start:start + len(thvec), 0] = xring
            cyl_coords[start:start + len(thvec), 1] = yring
            start += len(thvec)
        cyl_coords += period / 2.0
        # For every z layer in the 3D matrix of our quantity
        avgs = np.zeros((ext_vals.shape[0], len(rvec)))
        i = 0
        for layer in ext_vals:
            interp_vals = interpolate.interpn(
                points, layer, cyl_coords, method='linear')
            rings = interp_vals.reshape((len(rvec), len(thvec)))
            avg = np.average(rings, axis=1)
            avgs[i, :] = avg
            i += 1
        avgs = avgs[:, ::-1]
        # Save to avgs dict for this sim
        key = quantity + '_angularAvg'
        self.data[key] = avgs
        return avgs

    def integrate_nanowire(self, zvals, nkEsq=None):
        """
        Use natural neighbor interpolation to integrate nk|E|^2 inside the
        nanowire in shell in polar coordinates for optimum accuracy
        """

        # if len(self.X) % 2 == 0 or len(self.Y) % 2 == 0:
        #     raise ValueError("Need and odd number of x-y samples to use this "
        #                      " function")
        self.log.info("CALLING INTEGRATE_NANOWIRE")
        nw_layer = self.layers['NW_AlShell']
        core_rad = self.conf['Layers']['NW_AlShell']['params']['core_radius']
        shell_rad = self.conf['Layers']['NW_AlShell']['params']['shell_radius']
        period = self.conf['Simulation']['params']['array_period']
        # shell_rad = self.conf['Layers']['NW_AlShell']['params']['shell_radius']
        # Extract nkEsq data in layer
        if nkEsq is None:
            nkEsq = self.data['nknormEsq'][nw_layer.get_slice(self.Z)]
        # Get masks for each region
        core_mask = get_mask_by_material(nw_layer, 'GaAs', self.X, self.Y)
        shell_mask = get_mask_by_material(nw_layer, 'AlInP', self.X, self.Y)
        cyc_mask = np.logical_not(np.logical_or(core_mask, shell_mask))
        core_mask3d = np.broadcast_to(core_mask,
                                      (nkEsq.shape[0],
                                       core_mask.shape[0],
                                       core_mask.shape[1]))
        shell_mask3d = np.broadcast_to(shell_mask,
                                       (nkEsq.shape[0],
                                        shell_mask.shape[0],
                                        shell_mask.shape[1]))
        cyc_mask3d = np.broadcast_to(cyc_mask,
                                     (nkEsq.shape[0],
                                      cyc_mask.shape[0],
                                      cyc_mask.shape[1]))
        # Finally the cyclotene
        cyc_vals = nkEsq*cyc_mask3d
        cyc_result = integrate3d(cyc_vals, zvals, self.X, self.Y,
                                 meth=intg.simps)
        # Extract vals in each region from nkEsq array
        core_inds = np.where(core_mask3d)
        shell_inds = np.where(shell_mask3d)
        core_vals = nkEsq[core_inds]
        shell_vals = nkEsq[shell_inds]
        pts = cartesian_product((zvals, self.X, self.Y))
        # Shift x and y values so origin is at center of nanowire
        # core_pts_inds = np.where((core_pts[:, 1] - period/2)**2 + (core_pts[:, 2] - period/2)**2 <= core_rad**2)
        p2 = period/2.0
        pts[:, 2] -= p2
        pts[:, 1] -= p2
        core_pts_inds = np.where(pts[:, 1]**2 + pts[:, 2]**2 <= core_rad**2)
        shell_pts_inds = np.where((core_rad**2 < pts[:, 1]**2 + pts[:, 2]**2)
                                  &
                                  (pts[:, 1]**2 + pts[:, 2]**2 <= shell_rad**2))
        # core_pts_inds = np.where((pts[:, 1]-p2)**2 + (pts[:, 2]-p2)**2 <= core_rad**2)
        # shell_pts_inds = np.where((core_rad**2 <= (pts[:, 1]-p2)**2 + (pts[:, 2]-p2)**2)
        #                           &
        #                           ((pts[:, 1]-p2)**2 + (pts[:, 2]-p2)**2 <= shell_rad**2))
        core_pts = pts[core_pts_inds[0], :]
        shell_pts = pts[shell_pts_inds[0], :]
        # core_pts = np.column_stack((xx[core_inds[1], core_inds[2], core_inds[0]],
        #                        yy[core_inds[1], core_inds[2], core_inds[0]],
        #                        zz[core_inds[1], core_inds[2], core_inds[0]]))
        # Transform cartesian points into polar coordinates.
        # polar_pts[r, theta, z]
        core_polar_pts = np.zeros_like(core_pts)
        core_polar_pts[:, 0] = np.sqrt(core_pts[:, 2]**2 + core_pts[:, 1]**2)
        # This returns angles on [-pi, pi], so shift them
        core_polar_pts[:, 1] = np.arctan2(core_pts[:, 2], core_pts[:, 1])
        # core_polar_pts[:, 1][core_polar_pts[:, 1] < 0] += 2*np.pi
        core_polar_pts[:, 2] = core_pts[:, 0]
        # Same for the shell
        shell_polar_pts = np.zeros_like(shell_pts)
        shell_polar_pts[:, 0] = np.sqrt(shell_pts[:, 2]**2 + shell_pts[:, 1]**2)
        # This returns angles on [-pi, pi], so shift them
        shell_polar_pts[:, 1] = np.arctan2(shell_pts[:, 2], shell_pts[:, 1])
        # shell_polar_pts[:, 1][shell_polar_pts[:, 1] < 0] += 2*np.pi
        shell_polar_pts[:, 2] = shell_pts[:, 0]
        ###########
        # Insert S4 data at r = 0 here and all theta values.
        # Odd numbers of points guarantee an S4 point at the center of the unit
        # cell. Use an odd number of points, and take the center value and
        # replicate it across all thetas for r = 0
        # 1) Does the weird start in the center go away
        # 2) Compare integral by material to plain old integral and flux method
        ###########
        # Get function value at r = 0
        # center_inds = np.where((pts[:, 1] == .125) & (pts[:, 2] == .125))
        center_inds = np.where(core_polar_pts[:, 0] == 0)
        rzero_core_vals = core_vals[center_inds[0]]
        extra_theta = 180
        extra_pts = cartesian_product((np.array([0]),
                                       np.linspace(-np.pi, np.pi, extra_theta),
                                       core_polar_pts[center_inds[0], 2]))
        repeated_zero_vals = np.concatenate([rzero_core_vals for i in range(extra_theta)])
        core_polar_pts = np.concatenate((core_polar_pts, extra_pts))
        core_vals = np.concatenate((core_vals, repeated_zero_vals))

        # Extract interpolated points on a polar grid
        rstart = 0
        core_numr = 180
        shell_numr = 60
        numtheta = 360
        numz = 200
        # If the last element of each range is complex, the ranges behave like
        # np.linspace
        ranges = [[rstart, core_rad, 1j*core_numr], [-np.pi, np.pi, 1j*numtheta],
                  [nw_layer.start, nw_layer.end, 1j*numz]]
        # ranges = [[rstart, core_rad, 1j*core_numr], [0, 2*np.pi, 1j*numtheta],
        #           [nw_layer.start, nw_layer.end, 1j*numz]]
        core_interp = nn.griddata(core_polar_pts, core_vals, ranges)
        # ranges = [[core_rad, shell_rad, 1j*shell_numr], [0, 2*np.pi, 1j*numtheta],
        #           [nw_layer.start, nw_layer.end, 1j*numz]]
        ranges = [[core_rad, shell_rad, 1j*shell_numr], [-np.pi, np.pi, 1j*numtheta],
                  [nw_layer.start, nw_layer.end, 1j*numz]]
        shell_interp = nn.griddata(shell_polar_pts, shell_vals, ranges)
        # Multiply by area factor in polar coords to get integrand
        core_rvals = np.linspace(rstart, core_rad, core_numr)
        thetavals = np.linspace(-np.pi, np.pi, numtheta)
        intzvals = np.linspace(nw_layer.start, nw_layer.end, numz)
        rr, tt = np.meshgrid(core_rvals, thetavals, indexing='ij')
        # xx = rr*np.cos(tt)
        # yy = rr*np.sin(tt)
        # __import__('pdb').set_trace()
        integrand = core_interp*rr[:, :, None]
        core_result = integrate3d(integrand, thetavals, intzvals, core_rvals,
                                  meth=intg.simps)
        # Shell integral
        shell_rvals = np.linspace(core_rad, shell_rad, shell_numr)
        rr, tt = np.meshgrid(shell_rvals, thetavals, indexing='ij')
        integrand = shell_interp*rr[:, :, None]
        # integrand = core_interp*rr[:, :, None]
        shell_result = integrate3d(integrand, thetavals, intzvals, shell_rvals,
                                   meth=intg.simps)
        #plt.matshow(shell_mask3d[1, :, :])
        #plt.show()
        #plt.matshow(shell_mask)
        #plt.show()
        self.log.info("Core Integral Result = {}".format(core_result))
        self.log.info("Shell Integral Result = {}".format(shell_result))
        self.log.info("Cyc Integral Result = {}".format(cyc_result))
        return sum((core_result, shell_result, cyc_result))
        # return (core_rvals, shell_rvals, thetavals, zvals, core_interp,
        #         shell_interp, core_result, shell_result, cyc_result)

    def integrate_layer(self, lname, layer):
        freq = self.conf[('Simulation', 'params', 'frequency')]
        n_mat, k_mat = layer.get_nk_matrix(freq, self.X, self.Y)
        try:
            Esq = self.data['normEsquared']
        except KeyError:
            Esq = self.normEsquared()
        nkEsq = n_mat*k_mat*Esq[layer.get_slice(self.Z)]
        results = {}
        for mat in layer.materials.keys():
            mask = get_mask_by_material(layer, mat, self.X, self.Y)
            values = nkEsq*mask
            points = (self.Z[layer.get_slice(self.Z)], self.X, self.Y)
            rgi = interpolate.RegularGridInterpolator(points, values,
                                                      method='linear',
                                                      bounds_error=True)
            z = self.Z[layer.get_slice(self.Z)]
            # x = self.X
            # y = self.Y
            z = np.linspace(self.Z[layer.get_slice(self.Z)][0],
                            self.Z[layer.get_slice(self.Z)][-1], len(z)*2)
            x = np.linspace(self.X[0], self.X[-1], len(self.X)*2)
            y = np.linspace(self.Y[0], self.Y[-1], len(self.Y)*2)
            pts = cartesian_product((z, x, y))
            vals = rgi(pts).reshape((len(z), len(x), len(y)))
            z_integral = intg.trapz(vals, x=z, axis=0)
            x_integral = intg.trapz(z_integral, x=x, axis=0)
            y_integral = intg.trapz(x_integral, x=y, axis=0)
            results[mat] = y_integral
        result = sum(results.values())
        return result

    def absorption_per_layer(self, per_area=True, order=(0, 1, 2)):
        """
        Computes the absorption in each layer of the device using two methods.
        The first method takes the difference between the areal power fluxes
        entering a layer and leaving a layer.

        :math:`P_{abs} = P_{in} - P_{out}`

        The second method uses the raw fields and computes a volume integral of
        the norm squared of the electric field

        .. math::

           P_{abs} &= \\frac{\\omega}{2} \\int |E|^2
                      \\mathrm{Im}[\\epsilon] dV \\\\
                   &= \\frac{\\omega \\epsilon_0}{2}
                      \\int |E|^2 \\mathrm{Im}[\\epsilon_r] dV

        Recall :math:`\mathrm{Im}[\epsilon_r] = 2nk` so we finally arrive at

        .. math::

           P_{abs} = \omega \int |E|^2 n k dV

        This provides a metric for quantifying how converged the real space
        reconstructions of the fields are.

        :param per_area: Compute the absorption in units of power per unit area
                         or not. If True, in units of Watts/micrometer^2
        :type per_area: bool
        """

        fluxes = self.data['fluxes']
        base_unit = self.conf['General']['base_unit']
        Zo = consts.physical_constants['characteristic impedance of vacuum'][0]
        # try:
        #     Esq = self.data['normEsquared']
        # except KeyError:
        #     Esq = self.normEsquared()
        freq = self.conf[('Simulation', 'params', 'frequency')]
        dt = [('layer', 'S25'), ('flux_method', 'c16'), ('int_method', 'c16'),
              ('int_method_polar', 'c16'), ('difference', 'f8'),
              ('difference_polar', 'f8')]
        num_rows = len(list(self.layers.keys()))
        absorb_arr = np.recarray((num_rows,), dtype=dt)
        counter = 0
        sdict = self.conf['General']['sample_dict']
        for layer_name, layer_obj in self.layers.items():
            # Method 1: Go through power flux
            blayer_name = layer_name.encode('utf-8')
            bottom = layer_name+'_bottom'
            bbottom = bottom.encode('utf-8')
            port, forw_top, back_top = fluxes[fluxes.layer == blayer_name][0]
            port, forw_bot, back_bot = fluxes[fluxes.layer == bbottom][0]
            # Minus sign because S4 stick a minus in front of all backward
            # components
            Pin = forw_top - back_bot
            Pout = forw_bot - back_top
            Plost = Pin - Pout
            # S4 returns \int |E|^2 / Area, so we need to multiply by the area
            # here. Factor of one over vacuum impedance to get the units into
            # power.
            Pabs_flux = .5*Plost/Zo
            if not per_area:
                Pabs_flux *= self.period**2
            self.log.info("Layer: {}".format(layer_name))
            self.log.info("Flux Method Absorbed: {}".format(Pabs_flux))
            # Method 2: Go through integral of field intensity
            # if layer_name == 'Air':
            #     continue
            n_mat, k_mat = layer_obj.get_nk_matrix(freq, self.X, self.Y)
            keys = ['{}_{}'.format(layer_name, f) for f in ("Ex", "Ey", "Ez")]
            fields = {k[-2:]: self.data[k] for k in keys}
            Esq = np.abs(fields['Ex'])**2 + np.abs(fields['Ey'])**2 + np.abs(fields['Ez'])**2
            # z = np.linspace(layer_obj.start, layer_obj.end, Esq.shape[0])
            if isinstance(sdict[layer_name], int):
                z = np.linspace(layer_obj.start, layer_obj.end,
                                sdict[layer_name])
            else:
                args = [layer_obj.start, layer_obj.end, *sdict[layer_name]]
                z = arithmetic_arange(*args)
            if layer_name == "NW_AlShell":
                # y_integral_polar = self.integrate_nanowire(z, nkEsq=n_mat*k_mat*Esq)
                y_integral_polar = 0
                y_integral = integrate3d(n_mat*k_mat*Esq, z, self.X, self.Y,
                                         meth=intg.simps, order=order)
            else:
                y_integral_polar = 0
                y_integral = integrate3d(n_mat*k_mat*Esq, z, self.X, self.Y,
                                         meth=intg.simps, order=order)
            # abs_dict[lname] = result
            # nkEsq = n_mat*k_mat*Esq
            # y_integral = 0
            # for material in layer_obj.materials:
            #     mask = get_mask_by_material(layer_obj, material, self.X,
            #                                 self.Y)
            #     y_integral += self.integrate_quantity(nkEsq, layer=layer_name,
            #                                           mask=mask)

            # n and k could be functions of space, so we need to multiply the
            # fields by n and k before integrating
            # arr_slice = Esq[layer_obj.get_slice(self.Z)]*n_mat*k_mat
            # z_vals = self.Z[layer_obj.get_slice(self.Z)]
            # z_integral = intg.trapz(arr_slice, x=z_vals, axis=0)
            # x_integral = intg.trapz(z_integral, x=self.X, axis=0)
            # y_integral = intg.trapz(x_integral, x=self.Y, axis=0)
            # y_integral = self.integrate_layer(layer_name, layer_obj)

            # print('Arr slice shape: {}'.format(arr_slice.shape))
            # x_integral = intg.trapz(arr_slice, x=self.X, axis=1)
            # print('X Integral shape: {}'.format(x_integral.shape))
            # y_integral = intg.trapz(x_integral, x=self.Y, axis=1)
            # print('Y Integral shape: {}'.format(y_integral.shape))
            # z_integral = intg.trapz(y_integral, x=z_vals, axis=0)
            # 2\pi for conversion to angular frequency
            # epsilon_0 comes out of dielectric constant
            # Factor of base unit because we need to convert from our reference
            # lengths in base units to SI reference length unit (meters)
            # Factor of 1/2 for time averaging gets canceled by imaginary part
            # of dielectric constant (2*n*k)
            self.log.info("Pabs Pure Integral Result: {}".format(y_integral))
            Pabs_integ_polar = 2*np.pi*freq*consts.epsilon_0*base_unit*y_integral_polar
            Pabs_integ = 2*np.pi*freq*consts.epsilon_0*base_unit*y_integral
            # Pabs_integ = 2*np.pi*freq*consts.epsilon_0*base_unit*z_integral
            if per_area:
                Pabs_integ /= self.period**2
                Pabs_integ_polar /= self.period**2
            self.log.info("Integrated Absorbed: {}".format(Pabs_integ))
            diff = np.abs(Pabs_flux - Pabs_integ)/Pabs_flux
            if layer_name == "NW_AlShell":
                diff_polar = np.abs(Pabs_flux - Pabs_integ_polar)/Pabs_flux
            else:
                diff_polar = 0
            absorb_arr[counter] = (layer_name, Pabs_flux, Pabs_integ,
                                   Pabs_integ_polar, diff, diff_polar)
            counter += 1
        self.log.info("Integration Order: %s, Layer absorption arr: %s",
                      str(order), str(absorb_arr))
        fout = osp.join(self.dir, 'abs_per_layer.dat')
        with open(fout, 'w') as f:
            absorb_arr.tofile(f, sep=', ')
        self.data['abs_per_layer'] = absorb_arr
        return absorb_arr

    def transmissionData(self, port='Substrate_bottom'):
        """
        Computes reflection, transmission, and absorbance

        :param str port: Name of the location at which you would like to place
                         the transmission port (i.e where you would like to
                         compute transmission). This must correspond to one of
                         the keys placed in the fluxes dict located at
                         self.data['fluxes']
        """

        data = self.data['fluxes']
        # sorted_layers is an OrderedDict, and thus has the popitem method
        sorted_layers = self.conf.sorted_dict(self.conf['Layers'])
        # self.log.info('SORTED LAYERS: %s', str(sorted_layers))
        first_layer = sorted_layers.popitem(last=False)
        self.log.info('FIRST LAYER: %s', str(first_layer))
        # An ordered dict is actually just a list of tuples so we can access
        # the key directly like so
        first_name = first_layer[0]
        bfirst_name = first_name.encode('utf-8')
        self.log.info('FIRST LAYER NAME: %s', str(first_name))
        bport = port.encode('utf-8')
        # p_inc = data[first_name][0]
        # p_ref = np.abs(data[first_name][1])
        # p_trans = data[last_name][0]
        p_inc = np.absolute(data[data.layer == bfirst_name][0][1])
        p_ref = np.absolute(data[data.layer == bfirst_name][0][2])
        p_trans = np.absolute(data[data.layer == bport][0][1])
        reflectance = p_ref / p_inc
        transmission = p_trans / p_inc
        absorbance = 1 - reflectance - transmission
        tot = reflectance + transmission + absorbance
        delta = np.abs(tot - 1)
        self.log.info('Reflectance %f' % reflectance)
        self.log.info('Transmission %f' % transmission)
        self.log.info('Absorbance %f' % absorbance)
        self.log.info('Total = %f' % tot)
        assert(reflectance >= 0)
        assert(transmission >= 0)
        assert(absorbance >= 0)
        # assert(delta < .00001)
        # Try to get a prexisting array if it exists
        try:
            # If array has been computed before get it
            arr = self.data['transmission_data']
            # If we already have data at this port update that row, otherwise
            # resize the array to accomodate the new row and insert the data
            ind = np.where(arr.port == port.encode('utf-8'))[0]
            if len(ind) > 0:
                arr[ind[0]] = (port, reflectance, transmission, absorbance)
            else:
                arr = np.resize(arr, arr.shape[0]+1)
                arr[-1] = (port, reflectance, transmission, absorbance)
        except:
            # Otherwise we need to make a new one
            dt = [('port', 'S25'), ('reflection', 'f8'),
                  ('transmission', 'f8'), ('absorption', 'f8')]
            arr = np.recarray((1,), dtype=dt)
            arr[-1] = (port, reflectance, transmission, absorbance)
        self.data['transmission_data'] = arr
        return reflectance, transmission, absorbance

    def integrate_quantity(self, q, mask=None, layer=None):
        """
        Compute a 3D integral of a specified quantity

        :param q: A key in the self.data dict that specifies the quantity you
                  wish to integrate
        :type q: str or np.array
        :param mask: A numpy array of ones and zeros, which can be 2D or 3D. If
                     a 3D array is provided, it will multiply the 3D array of
                     the quantity elementwise before integration. The
                     z-direction is along the first axis, i.e mask[z, x, y].
                     If a 2D array is provided, it will be extended along the
                     z-direction and then will multiply the 3D array of the
                     quantity elementwise before integration. This can be
                     useful if you want to integrate only over a specific
                     region of the device. You would supply a 3D mask that is
                     1 inside that region, and zero outside that region.
                     Combining with the layer arg is supported.
        :type mask: np.ndarray
        :param layer: The name of the layer you wish to integrate over.
                      Providing this will extract a 3D slice of data that is in
                      the specified layer, and only integrate over that slice.
                      Use this if you do not want to include all layers in the
                      integration. Combining with the mask arg is supported.
        :type layer: str
        :return: Result of :math:`\\int quantity(x, y, z) dV`
        :rtype: float
        """
        if type(q) == str:
            qarr = self.get_scalar_quantity(q)
        else:
            qarr = q
        if layer is not None:
            l = self.layers[layer]
            z_vals = self.Z[l.get_slice(self.Z)]
            qarr = qarr[l.get_slice(self.Z)]
            if mask is not None and len(mask.shape) == 3:
                mask = mask[l.get_slice(self.Z)]
        else:
            z_vals = self.Z
        if mask is not None:
            qarr = qarr*mask
        z_integral = intg.trapz(qarr, x=z_vals, axis=0)
        x_integral = intg.trapz(z_integral, x=self.X, axis=0)
        y_integral = intg.trapz(x_integral, x=self.Y, axis=0)
        return y_integral

    def jsc_contrib(self, port='Substrate_bottom'):
        self.log.info('Computing Jsc contrib')
        wv_fact = consts.e / (consts.c * consts.h * 10)
        # Unpack data for the port we passed in as an argument
        try:
            arr = self.data['transmission_data']
            port, ref, trans, absorb = arr[arr.port == port.encode('utf-8')][0]
        except KeyError:
            ref, trans, absorb = self.transmissionData(port=port)
        freq = self.conf['Simulation']['params']['frequency']
        wvlgth = consts.c / freq
        wvlgth_nm = wvlgth * 1e9
        # Get solar power from chosen spectrum
        sun_pow = get_incident_amplitude(self)
        # This is our integrand
        val = absorb * wvlgth * sun_pow
        # val = absorb * wvlgth
        # test = absorb * sun_pow * wvlgth_nm * wv_fact * delta_wv
        # self.log.info('Sim %s Jsc Integrand: %f', self.ID, test)
        # Use Trapezoid rule to perform the integration. Note all the
        # necessary factors of the wavelength have already been included
        # above
        Jsc = wv_fact * val
        outf = osp.join(self.dir, 'jsc_contrib.dat')
        with open(outf, 'w') as out:
            out.write('%f\n' % Jsc)
        self.log.info('Jsc contrib = %f', Jsc)
        return Jsc

    def jsc_integrated_contrib(self):
        self.log.info('Computing integrated Jsc contribution')
        try:
            genRate = self.data['genRate']
        except KeyError:
            genRate = self.genRate()
        # Gen rate in cm^-3. Gotta convert lengths here from um to cm
        z_integral = intg.trapz(genRate, x=self.Z, axis=0)
        x_integral = intg.trapz(z_integral, x=self.X, axis=0)
        y_integral = intg.trapz(x_integral, x=self.Y, axis=0)
        # Convert period to cm and current to mA
        sun_pow = get_incident_amplitude(self)
        self.log.info('Sun power = %f', sun_pow)
        self.log.info('Integral = %f', y_integral)
        Jsc = 1000*(consts.e/(self.period*1e-4)**2)*y_integral
        outf = osp.join(self.dir, 'jsc_integrated_contrib.dat')
        with open(outf, 'w') as out:
            out.write('%f\n' % Jsc)
        self.log.info('Jsc_integrated contrib = %f', Jsc)
        return Jsc

    def integrated_absorbtion(self):
        """
        Computes the absorption of a layer by using the volume integral of
        the product of the imaginary part of the relative permittivity and the
        norm squared of the E field
        """

        raise NotImplementedError('There are some bugs in S4 and other reasons'
                                  ' that this function doesnt work yet')
        base = self.conf['General']['sim_dir']
        path = osp.join(base, 'integrated_absorption.dat')
        inpath = osp.join(base, 'energy_densities.dat')
        freq = self.conf['Simulation']['params']['frequency']
        # TODO: Assuming incident amplitude and therefore incident power is
        # just 1 for now
        fact = -.5 * freq * consts.epsilon_0
        with open(inpath, 'r') as inf:
            lines = inf.readlines()
            # Remove header line
            lines.pop(0)
            # Dict where key is layer name and value is list of length 2 containing real and
            # imaginary parts of energy density integral
            data = {line.strip().split(',')[0]: line.strip().split(',')[
                1:] for line in lines}
        self.log.info('Energy densities: %s' % str(data))
        with open(path, 'w') as outf:
            outf.write('# Layer, Absorption\n')
            for layer, vals in data.items():
                absorb = fact * float(vals[1])
                outf.write('%s,%s\n' % (layer, absorb))

    def plot_q_values(self):
        """
        Create a 2D scatter plot containing the propagation constants q. RCWA
        assumes all z dependence is contained in a factor :math:`e^{i q_n z}`
        for a set of eigenvalues :math:`q_n` whose length is equal to the
        number of basis terms used in the simulations. The y axis plots the
        imaginary part of q, which quantifies the exponential decay rate of the
        waves as they propagate along z. The x axis plots the real part of q,
        which determines the oscillation frequency of the waves.
        """

        sim_freq = self.conf['Simulation']['params']['frequency']
        sim_wvlgth = 1e9*consts.c / sim_freq
        leg_str = ''
        for mat, matpath in self.conf['Materials'].items():
            n, k = get_nk(matpath, sim_freq)
            mat_wv = 1e-3*sim_wvlgth / n
            mat_q = 2*np.pi/mat_wv
            leg_str += '{}: {:.2f} [rads/$\mu$m]\n'.format(mat, mat_q)
        leg_str = leg_str[0:-1]
        for lname, l_obj in self.layers.items():
            qarr = self.data['{}_qvals'.format(lname)]
            max_pos_freq = np.amax(qarr.real)
            max_neg_freq = np.amin(qarr.real)
            min_pos_wv = 1e3*2*np.pi/max_pos_freq
            if max_neg_freq == 0:
                min_neg_wv = 0
            else:
                min_neg_wv = 1e3*2*np.pi/max_neg_freq
            plt.figure()
            inc_q = 2*np.pi/(1e-3*sim_wvlgth)
            title = 'Layer: {}, Incident q: {:.2f} [rads/$\mu$m]'.format(lname, inc_q)
            # title += 'Min Positive $\\lambda$: {:.2f} nm, '
            # title += 'Min Negative $\\lambda$: {:.2f} nm'
            # title = title.format(lname, sim_wvlgth, min_pos_wv, min_neg_wv)
            # title = title.format(lname, sim_wvlgth)
            plt.title(title)
            # plt.scatter(1e3*2*np.pi/qarr.real, 1e4*qarr.imag/2*np.pi, c='b', s=.5,
            #             marker='o', label=leg_str)
            plt.scatter(qarr.real, qarr.imag/(2*np.pi), c='b', s=.75,
                        marker='o', label=leg_str)
            # pt = (qarr[0].real, qarr[0].imag)
            # theta = np.linspace(0, 1.48, 200)
            # plt.plot(pt[0]*np.cos(theta), pt[1]/np.cos(theta), 'r--')
            plt.legend(loc='best')
            # plt.annotate(leg_str, xy=(.95,.95), xycoords='axes fraction',
            #              size=14, ha='right', va='top',
            #              bbox=dict(boxstyle='round', fc='w'))
            plt.xlabel('Re(q) [radians/micron]')
            plt.ylabel('Im(q) [1/microns]')
            plot_path = osp.join(self.dir, '{}_qvals.png'.format(lname))
            plt.grid(True)
            plt.savefig(plot_path)
            plt.close()

    def _draw_layer_circle(self, layer, shape, material, plane, pval, ax_hand):
        """Draws the circle within a layer"""
        center = shape.center
        cx = float(shape.center.x)
        cy = float(shape.center.y)
        radius = float(shape.radius)
        if plane == 'xy':
            circle = mpatches.Circle((center.x, center.y), radius=radius,
                                     fill=False)
            ax_hand.add_artist(circle)
        if plane in ["xz", "zx", "yz", "zy"]:
            plane_x = pval*self.dx
            plane_to_center = np.abs(cx - plane_x)
            self.log.debug('DIST: {}'.format(plane_to_center))
            # Only draw if the observation plane actually intersects with the
            # circle
            if not plane_to_center >= radius:
                # Check if the plane intersects with the center of the circle
                if plane_to_center > 0:
                    intersect_angle = np.arccos(plane_to_center/radius)
                    self.log.debug('ANGLE: {}'.format(intersect_angle))
                    half_width = plane_to_center*np.tan(intersect_angle)
                else:
                    half_width = radius
                self.log.debug('HALF WIDTH: {}'.format(half_width))
                # Vertical lines should span height of the layer
                # z = [self.height - layer.start + .5, self.height - layer.end +.5]
                # z = [layer.start+.5, layer.end+.5]
                z = [layer.start, layer.end]
                # The upper edge
                x = [cy + half_width, cy + half_width]
                line = mlines.Line2D(x, z, linestyle='solid', linewidth=2.0,
                                     color='grey')
                ax_hand.add_line(line)
                # Now the lower edge
                x = [cy - half_width, cy - half_width]
                line = mlines.Line2D(x, z, linestyle='solid', linewidth=2.0,
                                     color='grey')
                ax_hand.add_line(line)
        return ax_hand

    def _draw_layer_geometry(self, layer, plane, pval, ax_hand):
        """Given a dictionary with the data containing the geometry for a
        layer, draw the internal geometry of the layer for a given plane type
        and plane value"""
        for sname, (shape, material) in layer.shapes.items():
            if isinstance(shape, Circle):
                ax = self._draw_layer_circle(layer, shape, material, plane, pval, ax_hand)
            else:
                self.log.warning('Drawing of shape {} not '
                                 'supported'.format(shape))
        return ax

    def draw_geometry_2d(self, plane, pval, ax_hand, skip_list=[]):
        """This function draws the layer boundaries and in-plane geometry on 2D
        heatmaps"""
        period = self.conf['Simulation']['params']['array_period']
        for lname, layer in self.layers.items():
            if plane in ["xz", "zx", "yz", "zy"]:
                # Get boundaries between layers and their starting and ending
                # indices
                if layer.thickness > 0:
                    if layer not in skip_list:
                        x = [0, period]
                        y = [layer.end, layer.end]
                        # label_y = y[0] + 3*self.dz
                        # label_x = x[-1] - .01
                        # ax_hand.text(label_x, label_y, layer, ha='right',
                        #              family='sans-serif', size=16, color='grey')
                        line = mlines.Line2D(x, y, linestyle='solid',
                                             linewidth=2.0, color='grey')
                        ax_hand.add_line(line)
            # If we have some internal geometry for this layer, draw it
            if layer.shapes:
                ax_hand = self._draw_layer_geometry(layer, plane, pval, ax_hand)
        return ax_hand

    def heatmap2d(self, x, y, cs, labels, ptype, pval, save_path=None,
                  show=False, draw=False, fixed=None, colorsMap='jet'):
        """A general utility method for plotting a 2D heat map"""
        # cs = np.flipud(cs)
        cm = plt.get_cmap(colorsMap)
        if np.iscomplexobj(cs):
            self.log.warning('Plotting only real part of %s in heatmap',
                             labels[2])
            cs = cs.real
        if fixed:
            if 'dielectric_profile' in save_path:
                cNorm = matplotlib.colors.Normalize(
                    vmin=np.amin(0), vmax=np.amax(16))
            else:
                pass
                cNorm = matplotlib.colors.Normalize(
                    vmin=np.amin(cs), vmax=np.amax(cs))
                # cNorm = matplotlib.colors.Normalize(
                #     vmin=np.amin(0), vmax=np.amax(2.5))
        else:
            cNorm = matplotlib.colors.Normalize(
                vmin=np.amin(cs), vmax=np.amax(cs))
            # cNorm = matplotlib.colors.LogNorm(vmin=np.amin(cs)+.001, vmax=np.amax(cs))
            # cNorm = matplotlib.colors.LogNorm(vmin=1e13, vmax=np.amax(cs))
        scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cm)
        fig = plt.figure(figsize=(10, 8))
        ax = fig.add_subplot(111)
        # ax.imshow(cs,cmap=cm,norm=cNorm,extent=[x.min(),x.max(),y.min(),y.max()],aspect='auto')
        ax.invert_yaxis()
        ax.pcolormesh(x, y, cs, cmap=cm, norm=cNorm)
                      # extent=[x.min(),x.max(),y.min(),y.max()],aspect='auto')
        ax.grid(False)
        scalarMap.set_array(cs)
        # div = make_axes_locatable(ax)
        # zoom_ax = div.append_axes("right",size='100%', pad=.5)
        # zoom_ax.imshow(cs[75:100,:], extent=[x.min(), x.max(), .8, 1.4])
        # zoom_ax.grid(False)
        # cax = div.append_axes("right",size="100%",pad=.05)
        cb = fig.colorbar(scalarMap)
        cb.set_label(labels[2])
        ax.set_xlabel(labels[0])
        ax.set_ylabel(labels[1])
        if draw:
            self.log.info('Beginning geometry drawing routines ...')
            ax = self.draw_geometry_2d(ptype, pval, ax)
        if save_path:
            fig.savefig(save_path, bbox_inches='tight')
        if show:
            plt.show()
        plt.close(fig)

    def plane_2d(self, quantity, plane, pval, draw=False, fixed=None):
        """Plots a heatmap of a fixed 2D plane"""
        self.log.info('Plotting plane')
        pval = int(pval)
        # x = np.arange(0, self.period, self.dx)
        # y = np.arange(0, self.period, self.dy)
        # z = np.arange(0, self.height + self.dz, self.dz)
        x = self.X
        y = self.Y
        z = self.Z
        # Get the scalar values
        freq = self.conf['Simulation']['params']['frequency']
        wvlgth = (consts.c / freq) * 1E9
        title = 'Frequency = {:.4E} Hz, Wavelength = {:.2f} nm'.format(
            freq, wvlgth)
        # Get the plane we wish to plot
        cs = self.get_plane(quantity, plane, pval)
        self.log.info('DATA SHAPE: %s' % str(cs.shape))
        show = self.conf['General']['show_plots']
        p = False
        sim_dir = osp.expandvars(self.conf['General']['sim_dir'])
        if plane == 'yz' or plane == 'zy':
            labels = ('y [um]', 'z [um]', quantity, title)
            if self.conf['General']['save_plots']:
                p = osp.join(sim_dir,
                                 '%s_plane_2d_yz_pval%s.png' % (quantity,
                                                               str(pval)))
            self.heatmap2d(y, z, cs, labels, plane, pval,
                           save_path=p, show=show, draw=draw, fixed=fixed)
        elif plane == 'xz' or plane == 'zx':
            labels = ('x [um]', 'z [um]', quantity, title)
            if self.conf['General']['save_plots']:
                p = osp.join(sim_dir,
                                 '%s_plane_2d_xz_pval%s.png' % (quantity,
                                                               str(pval)))
            self.heatmap2d(x, z, cs, labels, plane, pval,
                           save_path=p, show=show, draw=draw, fixed=fixed)
        elif plane == 'xy' or plane == 'yx':
            labels = ('y [um]', 'x [um]', quantity, title)
            if self.conf['General']['save_plots']:
                p = osp.join(sim_dir,
                                 '%s_plane_2d_xy_pval%s.png' % (quantity,
                                                               str(pval)))
            self.heatmap2d(x, y, cs, labels, plane, pval,
                           save_path=p, show=show, draw=draw, fixed=fixed)

    def scatter3d(self, x, y, z, cs, labels, ptype, colorsMap='jet'):
        """A general utility method for scatter plots in 3D"""
        cm = plt.get_cmap(colorsMap)
        cNorm = matplotlib.colors.Normalize(vmin=min(cs), vmax=max(cs))
        scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cm)
        fig = plt.figure(figsize=(9, 7))

        ax = fig.add_subplot(111, projection='3d')
        ax.scatter(x, y, z, c=scalarMap.to_rgba(cs), edgecolor='none')
        scalarMap.set_array(cs)
        cb = fig.colorbar(scalarMap)
        cb.set_label(labels[3])
        ax.set_xlabel(labels[0])
        ax.set_ylabel(labels[1])
        ax.set_zlabel(labels[2])
        fig.suptitle(osp.basename(self.conf['General']['sim_dir']))
        if self.conf['General']['save_plots']:
            name = labels[-1] + '_' + ptype + '.png'
            path = osp.join(self.conf['General']['sim_dir'], name)
            fig.savefig(path)
        if self.conf['General']['show_plots']:
            plt.show()
        plt.close(fig)

    def full_3d(self, quantity):
        """Generates a full 3D plot of a specified scalar quantity"""
        # The data just tells you what integer grid point you are on. Not what actual x,y coordinate you
        # are at
        x = np.arange(0, self.period, self.dx)
        y = np.arange(0, self.period, self.dy)
        z = np.arange(0, self.height + self.dz, self.dz)
        points = np.array(list(itertools.product(z, x, y)))
        # Get the scalar
        scalar = self.get_scalar_quantity(quantity)
        labels = ('X [um]', 'Y [um]', 'Z [um]', quantity)
        # Now plot!
        self.scatter3d(points[:, 1], points[:, 2], points[
                       :, 0], scalar.flatten(), labels, 'full_3d')

    def planes_3d(self, quantity, xplane, yplane):
        """Plots some scalar quantity in 3D but only along specified x-z and y-z planes"""
        xplane = int(xplane)
        yplane = int(yplane)
        # Get the scalar values
        # Get the data on the plane with a fixed x value. These means we'll
        # have changing (y, z) points
        xdata = self.get_plane(quantity, 'yz', xplane)
        # z first cuz we want y to be changing before z to correspond with the
        # way numpy flattens arrays. Note this means y points will be in the
        # 2nd column
        xplanepoints = np.array(list(itertools.product(self.Z, self.Y)))
        xdata = xdata.flatten()
        xplanexval = np.array(list(itertools.repeat(x[xplane], len(xdata))))
        xplanedata = np.zeros((xplanepoints.shape[0], 4))
        xplanedata[:, 0] = xplanexval
        xplanedata[:, 1] = xplanepoints[:, 1]
        xplanedata[:, 2] = xplanepoints[:, 0]
        xplanedata[:, 3] = xdata
        # Same procedure for fixed y plane
        ydata = self.get_plane(quantity, 'xz', yplane)
        yplanepoints = np.array(list(itertools.product(z, x)))
        ydata = ydata.flatten()
        yplaneyval = np.array(list(itertools.repeat(y[yplane], len(ydata))))
        yplanedata = np.zeros((yplanepoints.shape[0], 4))
        yplanedata[:, 0] = yplanepoints[:, 1]
        yplanedata[:, 1] = yplaneyval
        yplanedata[:, 2] = yplanepoints[:, 0]
        yplanedata[:, 3] = ydata
        labels = ('X [um]', 'Y [um]', 'Z [um]', quantity)
        # Now stack them vertically and plot!
        all_data = np.vstack((xplanedata, yplanedata))
        self.scatter3d(all_data[:, 0], all_data[:, 1], all_data[:, 2],
                       all_data[:, 3], labels, 'planes_3d')

    def line_plot(self, x, y, labels, ax=None):
        """Make a simple line plot and return the figure and axes handles"""
        if ax is None:
            fig, ax = plt.subplots()
        else:
            fig = None
        ax.plot(x, y, '--o', label=labels[0])
        ax.set_xlabel(labels[1])
        ax.set_ylabel(labels[2])
        ax.set_title(labels[3])
        return fig, ax

    def fixed_line(self, quantity, direction, coord1, coord2):
        """
        Plot a scalar quantity on a line along a given direction at some pair
        of coordinates in the plane perpendicular to that direction. The
        remaining coordinates are specified in x, y, z order. So for example if
        direction='z' then coord1 corresponds to x and coord2 corresponds to y.
        If direction='y' then coord1 corresponds to x and coord2 corresponds to
        z.
        :param str direction: The direction along which to plot the line. Must
        be one of 'x', 'y', or 'z'.
        :param str direction: The direction along which you wish to plot a
        line. Must be one of 'x', 'y', or 'z'. The other two coordinates remain
        fixed and are specified by coord1 and coord2.
        :param int coord1: The integer index for the first fixed coordinate.
        Indexes are in x,y, z order so if line_dir='z' then c1 corresponds to x
        :param int coord2: The integer index for the second coordinate.
        :param str quantity: The quantity whose data array you wish to take a
        line cut through
        """

        coord1 = int(coord1)
        coord2 = int(coord2)
        # Get the scalar values
        # Filter out any undesired data that isn't on the planes
        data = self.get_line(quantity, direction, coord1, coord2)
        if direction == 'x':
            # z along rows, y along columns
            pos_data = self.X
        elif direction == 'y':
            # x along columns, z along rows
            pos_data = self.Y
        elif direction == 'z':
            # x along rows, y along columns
            pos_data = self.Z
        freq = self.conf['Simulation']['params']['frequency']
        wvlgth = (consts.c / freq) * 1E9
        title = 'Frequency = {:.4E} Hz, Wavelength = {:.2f} nm'.format(
            freq, wvlgth)
        ptype = "%s_line_plot_%i_%i" % (direction, coord1, coord2)
        if np.iscomplexobj(data):
            labels = ('Real Part', 'Z [um]', quantity, title)
            fig, ax = self.line_plot(pos_data, data.real, labels)
            labels = ('Imag Part', 'Z [um]', quantity, title)
            _, ax = self.line_plot(pos_data, data.imag, labels, ax=ax)
        else:
            labels = (None, 'Z [um]', quantity, title)
            fig, ax = self.line_plot(pos_data, data, labels)
        ax.legend()
        if self.conf['General']['save_plots']:
            name = labels[2] + '_' + ptype + '.png'
            sim_dir = osp.expandvars(self.conf['General']['sim_dir'])
            path = osp.join(sim_dir, name)
            fig.savefig(path)
        if self.conf['General']['show_plots']:
            plt.show()
        plt.close(fig)

class SimulationGroup:

    """
    A group of simulations. Takes a list of Simulation objects as an argument.
    This class is not responsible for grouping Simulation objects by some
    criteria, it just expects a list of already grouped Simulation objects.

    Provides methods for calculating things on a group of simulations. The
    results of these methods might not be sensible for some groupings. For
    example, calculating the convergence of a group that is group against
    frequency doesn't make any sense. Similiarly, calculating Jsc of a group
    that is grouped against number of basis terms also doesn't make sense.
    """

    def __init__(self, sims, grouped_against=None, grouped_by=None):
        if grouped_by is None and grouped_against is None:
            raise ValueError("Must know how this SimulationGroup is grouped")
        self.grouped_by = grouped_by
        self.grouped_against = grouped_against
        self.sims = sims
        # for sim in self.sims:
        #     sim.get_layers()
        self.log = logging.getLogger(__name__)
        self.num_sims = len(sims)


    def get_plane(self, scalar, plane, pval):
        """
        Gets data along a 2D plane/slice through the 3D data array for a given
        quantity

        :param str plane: Any of 'xy', 'yz', or 'xz'. Determines the plane
        along which the slice is taken
        :param int pval: The index along the final unspecified direction. If
        plane='xy' then index would index along the z direction.
        :param str quantity: The quantity whose data array you wish to take a
        line cut through
        """

        if plane == 'yz' or plane == 'zy':
            # z along rows, y along columns
            return scalar[:, pval, :]
        elif plane == 'xz' or plane == 'zx':
            # x along columns, z along rows
            return scalar[:, :, pval]
        elif plane == 'xy' or plane == 'yx':
            # x along rows, y along columns
            return scalar[pval, :, :]

    def diff_sq(self, x, y):
        """Returns the magnitude of the difference vector squared between two vector fields at each
        point in space"""
        # Calculate the magnitude of the difference vector SQUARED at each point in space
        # This is mag(vec(x) - vec(y))^2 at each point in space. This should be a 1D array
        # with # of elements = # sampling points
        mag_diff_vec = sum([np.absolute(v1 - v2)**2 for v1, v2 in zip(x, y)])
        return mag_diff_vec

    def get_slice(self, sim):
        """Returns indices for data that strip out air and substrate regions"""
        # TODO: This function is definitely not general. We need to get a list
        # of layers to exclude from the user. For now, just assume we want to
        # exclude the top and bottom regions
        # sorted_layers is an OrderedDict, and thus has the popitem method
        sorted_layers = sim.conf.sorted_dict(sim.conf['Layers'])
        first_layer = sorted_layers.popitem(last=False)
        last_layer = sorted_layers.popitem()
        # We can get the starting and ending planes from their heights
        start_plane = int(round(first_layer[1]['params'][
                          'thickness'] / sim.dz))
        end_plane = int(round(last_layer[1]['params'][
                        'thickness'] / sim.dz))
        return start_plane, end_plane

    def get_comp_vec(self, sim, field, start, end):
        """Returns the comparison vector"""
        # Compare all other sims to our best estimate, which is sim with highest number of
        # basis terms (last in list cuz sorting)

        # Get the proper file extension depending on the field.
        norm = 'norm'+field
        # Get the comparison vector
        vecs = [sim.data[field+comp][start:end] for comp in ('x', 'y', 'z')]
        normvec = sim.get_scalar_quantity('normE')
        normvec = normvec[start:end]**2
        return vecs, normvec

    def difference_squared(self, field, layer_name, sim1, sim2):
        """
        Computes the difference squared between the given field in the given
        layer of two simulations at each point in space, for example normE or
        genRate. If any vector components are passed in, this is also handled
        appropriately (for example Ex, Ey, or Ez)

        :param field: The field or field component you wish to compare
        :type field: str
        :param layer_name: The name of the layer in which you wish to make the
        comparison
        :type layer_name: str
        :return: An array containing the difference squared at each point in
        space.
        :rtype: np.ndarray
        """

        if field in ('Ex', 'Ey', 'Ez'):
            field = '{}_{}'.format(layer_name, field)
        self.log.info('Running the difference squared computation for quantity %s', field)
        layer_obj = self.layers[layer_name]
        # Set the reference sim
        # Get the comparison vector
        ref_arr = sim1.data[field][layer_obj.get_slice()]
        # For all other sims in the groups, compare to best estimate
        # and write to error file
        comp_arr = sim2.data[field][layer_obj.get_slice()]
        # Get the array containing the magnitude of the difference vector
        # at each point in space
        diff_sq = np.absolute(ref_arr - comp_arr)**2
        return diff_sq

    def global_error(self, field, exclude=False):
        """Computes the global error between the vector fields of two simulations. This is the sum
        of the magnitude squared of the difference vectors divided by the sum of the magnitude
        squared of the comparison efield vector over the desired section of the simulation cell"""

        self.log.info('Running the global error computation for quantity %s', field)
        # If we need to exclude calculate the indices
        if exclude:
            start, end = self.get_slice(self.sims[0])
            excluded = '_excluded'
        else:
            start = 0
            end = None
            excluded = ''
        # base = self.sims[0].conf['General']['base_dir']
        base = self.sims[0].conf['General']['results_dir']
        errpath = osp.join(base, 'globalerror_%s%s.dat' % (field, excluded))
        with open(errpath, 'w') as errfile:
            self.log.info('Computing global error for sweep %s', base)
            # Set reference sim
            ref_sim = self.sims[-1]
            # Get the comparison vector
            vecs1, normvec = self.get_comp_vec(ref_sim, field, start, end)
            # For all other sims in the groups, compare to best estimate
            # and write to error file
            for i in range(0, self.num_sims - 1):
                sim2 = self.sims[i]
                vecs2, normvec2 = self.get_comp_vec(sim2, field, start, end)
                self.log.info("Computing global error between numbasis %i and numbasis %i",
                              ref_sim.conf['Simulation'][ 'params']['numbasis'],
                              sim2.conf['Simulation']['params']['numbasis'])
                # Get the array containing the magnitude of the difference vector at each point
                # in space
                mag_diff_vec = self.diff_sq(vecs1, vecs2)
                # Check for equal lengths between norm array and diff mag
                # array
                if len(mag_diff_vec) != len(normvec):
                    self.log.error( "The normalization vector has an incorrect number of elements!!!")
                    raise ValueError
                # Error as a percentage should be the square root of the ratio of sum of mag diff vec
                # squared to mag efield squared
                error = np.sqrt(np.sum(mag_diff_vec) / np.sum(normvec))
                errfile.write('%i,%f\n' % (sim2.conf['Simulation']['params']['numbasis'], error))
                sim2.clear_data()
            ref_sim.clear_data()

    def adjacent_error(self, field, exclude=False):
        """Computes the global error between the vector fields of two simulations. This is the sum
        of the magnitude squared of the difference vectors divided by the sum of the magnitude
        squared of the comparison efield vector over the desired section of the simulation cell.
        This computes error between adjacent sims in a sweep through basis terms."""

        self.log.info('Running the adjacent error computation for quantity %s', field)
        # If we need to exclude calculate the indices
        if exclude:
            start, end = self.get_slice(self.sims[0])
            excluded = '_excluded'
        else:
            start = 0
            end = None
            excluded = ''
        base = self.sims[0].conf['General']['results_dir']
        errpath = osp.join(base, 'adjacenterror_%s%s.dat' % (field, excluded))
        with open(errpath, 'w') as errfile:
            self.log.info('Computing adjacent error for sweep %s', base)
            # For all other sims in the groups, compare to best estimate
            # and write to error file
            for i in range(1, self.num_sims):
                # Set reference sim
                ref_sim = self.sims[i]
                # Get the comparison vector
                vecs1, normvec = self.get_comp_vec(ref_sim, field, start, end)
                sim2 = self.sims[i - 1]
                vecs2, normvec2 = self.get_comp_vec(sim2, field, start, end)
                self.log.info("Computing adjacent error between numbasis %i and numbasis %i",
                              ref_sim.conf['Simulation'][ 'params']['numbasis'],
                              sim2.conf['Simulation']['params']['numbasis'])
                # Get the array containing the magnitude of the difference vector at each point
                # in space
                mag_diff_vec = self.diff_sq(vecs1, vecs2)
                # Check for equal lengths between norm array and diff mag
                # array
                if len(mag_diff_vec) != len(normvec):
                    self.log.error("The normalization vector has an incorrect number of elements!!!")
                    raise ValueError
                # Error as a percentage should be thkkk square root of the ratio of sum of mag diff vec
                # squared to mag efield squared
                error = np.sqrt(np.sum(mag_diff_vec) / np.sum(normvec))
                # self.log.info(str(error))
                errfile.write('%i,%f\n' % (sim2.conf['Simulation']['params']['numbasis'], error))
                sim2.clear_data()
                ref_sim.clear_data()

    def scalar_reduce(self, quantity, avg=False):
        """
        Combine a scalar quantity across all simulations in each group. If
        avg=False then a direct sum is computed, otherwise an average is
        computed
        """

        base = self.sims[0].conf['General']['results_dir']
        self.log.info('Performing scalar reduction for group at %s' % base)
        self.log.debug('QUANTITY: %s'%quantity)
        group_comb = self.sims[0].get_scalar_quantity(quantity)
        self.log.debug(group_comb.dtype)
        self.sims[0].clear_data()
        # This approach is more memory efficient then building a 2D array
        # of all the data from each group and summing along an axis
        for sim in self.sims[1:]:
            self.log.debug(sim.id)
            quant = sim.get_scalar_quantity(quantity)
            self.log.debug(quant.dtype)
            group_comb += quant
            sim.clear_data()
        if avg:
            group_comb = group_comb / self.num_sims
            fname = 'scalar_reduce_avg_%s' % quantity
        else:
            fname = 'scalar_reduce_%s' % quantity
            path = osp.join(base, fname)
        ftype = self.sims[0].conf['General']['save_as']
        if ftype == 'npz':
            np.save(path, group_comb)
        elif ftype == 'hdf5':
            self.log.warning('FIX HDF5 SCALAR REDUCE SAVING')
            np.save(path, group_comb)
        else:
            raise ValueError('Invalid file type in config')

    def fractional_absorbtion(self, port='Substrate'):
        """
        Computes the fraction of the incident spectrum that is absorbed by
        the device. This is a unitless number, and its interpretation somewhat
        depends on the units you express the incident spectrum in. If you
        expressed your incident spectrum in photon number, this can be
        interpreted as the fraction of incident photons that were absorbed. If
        you expressed your incident spectrum in terms of power per unit area,
        then this can be interpreted as the fraction of incident power per unit
        area that gets absorbed. In summary, its the fraction of whatever you
        put in that is being absorbed by the device.
        """

        base = self.sims[0].conf['General']['results_dir']
        self.log.info('Computing fractional absorbtion for group at %s' % base)
        vals = np.zeros(self.num_sims)
        freqs = np.zeros(self.num_sims)
        wvlgths = np.zeros(self.num_sims)
        spectra = np.zeros(self.num_sims)
        path = self.sims[0].conf['Simulation']['input_power_wv']
        wv_vec, p_vec = np.loadtxt(path, usecols=(0, 2), unpack=True, delimiter=',')
        p_wv = interpolate.interp1d(wv_vec, p_vec, kind='linear',
                                    bounds_error=False, fill_value='extrapolate')
        # Assuming the sims have been grouped by frequency, sum over all of
        # them
        for i, sim in enumerate(self.sims):
            # Unpack data for the port we passed in as an argument
            ref, trans, absorb = sim.data['transmission_data'][port]
            freq = sim.conf['Simulation']['params']['frequency']
            wvlgth = consts.c / freq
            wvlgth_nm = wvlgth * 1e9
            freqs[i] = freq
            wvlgths[i] = wvlgth
            # Get solar power from chosen spectrum
            # Get p at wvlength by interpolation
            sun_pow = p_wv(wvlgth_nm)
            spectra[i] = sun_pow * wvlgth_nm
            vals[i] = absorb * sun_pow * wvlgth_nm
            sim.clear_data()
        # Use Trapezoid rule to perform the integration. Note all the
        # necessary factors of the wavelength have already been included
        # above
        wvlgths = wvlgths[::-1]
        vals = vals[::-1]
        spectra = spectra[::-1]
        integrated_absorbtion = intg.trapz(vals, x=wvlgths * 1e9)
        power = intg.trapz(spectra, x=wvlgths * 1e9)
        # factor of 1/10 to convert A*m^-2 to mA*cm^-2
        #wv_fact = consts.e/(consts.c*consts.h*10)
        #wv_fact = .1
        #Jsc = (Jsc*wv_fact)/power
        frac_absorb = integrated_absorbtion / power
        outf = osp.join(base, 'fractional_absorbtion.dat')
        with open(outf, 'w') as out:
            out.write('%f\n' % frac_absorb)
        self.log.info('Fractional Absorbtion = %f' % frac_absorb)
        return frac_absorb

    def photocurrent_density(self, port='Substrate_bottom', method='flux'):
        """
        Computes photocurrent density assuming perfect carrier collection,
        meaning every absorbed photon gets converted to 1 collected electron.

        The incident power is computed using
        :py:func:`get_incident_power`.  See that function for
        details about how the incident power is computed.

        Given some number of frequency values N (and simulations at those
        frequencies), the photocurrent density can be computed using

        .. math:: J_{ph} = \\int q \\sum_i^N \\frac{P(f_i)A(f_i)}{E_{photon}}
           :label: Jph

        where

        * :math:`P(f_i)` is the incident power at frequency i
        * :math:`A(f_i)` is the fraction of the incident power that is absorbed
        * :math:`E_{photon} = h f_i` is the energy of the incident photons at
          frequency i.
        * :math:`q` is the fundamental charge

        :param port: The location at which to set the transmission port
        :type port: str
        :param method: The method used to compute the absorbed power. One of
                       either 'flux' or 'integral'
        :type method: str
        :return: The photocurrent density, see :eq:`Jph`
        :rtype: float
        :raises ValueError: if the ``method`` kwarg is not 'flux' or
                            'integral'.  This is a bit of a hack at the moment
                            for testing purposes.
        """

        if method not in ('flux', 'integral'):
            msg = 'Invalid method {} for computing photocurrent density'.format(method)
            raise ValueError(msg)

        print(self.grouped_against)
        if not tuple(self.grouped_against) == ('Simulation', 'params', 'frequency'):
            raise ValueError('Can only compute photocurrent density when '
                             'grouped against frequency')


        base = osp.expandvars(self.sims[0].conf['General']['results_dir'])
        period = self.sims[0].conf['Simulation']['params']['array_period']
        self.log.info('Computing photocurrent density for group at %s', base)
        jph_vals = np.zeros(self.num_sims)
        freqs = np.zeros(self.num_sims)
        for i, sim in enumerate(self.sims):
            freq = sim.conf['Simulation']['params']['frequency']
            freqs[i] = freq
        # Assuming the sims have been grouped by frequency, sum over all of
        # them
        for i, sim in enumerate(self.sims):
            # freq = sim.conf['Simulation']['params']['frequency']
            # freqs[i] = freq
            E_photon = consts.h * freq
            if method == 'flux':
                # arr = sim.data['transmission_data']
                # _, ref, trans, absorb = arr[arr.port == port.encode('utf-8')][0]
                # incident_power = get_incident_power(sim)
                # jph_vals[i] = incident_power * absorb / E_photon
                try:
                    abs_arr = sim.data['abs_per_layer']
                except KeyError:
                    abs_arr = sim.absorption_per_layer()
                absorbed_power = np.sum(abs_arr['flux_method'])
                jph_vals[i] = absorbed_power / (E_photon*period**2)
            else:
                try:
                    abs_arr = sim.data['abs_per_layer']
                except KeyError:
                    abs_arr = sim.absorption_per_layer()
                absorbed_power = np.sum(abs_arr['int_method'])
                jph_vals[i] = absorbed_power / (E_photon*period**2)
            sim.clear_data()
        # factor of 1/10 to convert A*m^-2 to mA*cm^-2
        Jph = .1 * consts.e * np.sum(jph_vals)
        outf = osp.join(base, 'jph_{}.dat'.format(method))
        with open(outf, 'w') as out:
            out.write('%f\n' % Jph)
        self.log.info('Jph = %f', Jph)
        return Jph

    def Jsc_integrated_persim(self):
        for i, sim in enumerate(self.sims):
            try:
                genRate = sim.data['genRate']
            except FileNotFoundError:
                genRate = sim.genRate()
            # Gen rate in cm^-3. Gotta convert lengths here from um to cm
            z_vals = self.sims[0].Z
            x_vals = self.sims[0].X
            y_vals = self.sims[0].Y
            z_integral = intg.trapz(genRate, x=z_vals, axis=0)
            x_integral = intg.trapz(z_integral, x=x_vals, axis=0)
            y_integral = intg.trapz(x_integral, x=y_vals, axis=0)
            # z_integral = intg.simps(genRate, x=z_vals, axis=0)
            # x_integral = intg.simps(z_integral, x=x_vals, axis=0)
            # y_integral = intg.simps(x_integral, x=y_vals, axis=0)
            # Convert period to cm and current to mA
            Jsc = 1000*(consts.e/(sim.period*1e-4)**2)*y_integral
            self.log.info('Sim %s Jsc Integrate Value: %f', sim.id, Jsc)


    def Jsc_integrated(self):
        """
        Compute te photocurrent density by performing a volume integral of the
        generation rate
        """
        fname = 'scalar_reduce_genRate.npy'
        base = self.sims[0].conf['General']['results_dir']
        self.log.info('Computing integrated Jsc for group at %s', base)
        path = osp.join(base, fname)
        try:
            genRate = np.load(path)
        except FileNotFoundError:
            self.scalar_reduce('genRate')
            genRate = np.load(path)
        # Gen rate in cm^-3. Gotta convert lengths here from um to cm
        z_vals = self.sims[0].Z
        x_vals = self.sims[0].X
        y_vals = self.sims[0].Y
        z_integral = intg.trapz(genRate, x=z_vals, axis=0)
        x_integral = intg.trapz(z_integral, x=x_vals, axis=0)
        y_integral = intg.trapz(x_integral, x=y_vals, axis=0)
        # z_integral = intg.simps(genRate, x=z_vals, axis=0)
        # x_integral = intg.simps(z_integral, x=x_vals, axis=0)
        # y_integral = intg.simps(x_integral, x=y_vals, axis=0)
        # Convert period to cm and current to mA
        Jsc = 1000*(consts.e/(self.sims[0].period*1e-4)**2)*y_integral
        outf = osp.join(base, 'jsc_integrated.dat')
        with open(outf, 'w') as out:
            out.write('%f\n' % Jsc)
        self.log.info('Jsc_integrated = %f', Jsc)
        return Jsc

    def weighted_transmissionData(self, port='Substrate'):
        """Computes spectrally weighted absorption,transmission, and reflection"""

        base = self.sims[0].conf['General']['results_dir']
        self.log.info('Computing spectrally weighted transmission data for group at %s' % base)
        abs_vals = np.zeros(self.num_sims)
        ref_vals = np.zeros(self.num_sims)
        trans_vals = np.zeros(self.num_sims)
        freqs = np.zeros(self.num_sims)
        wvlgths = np.zeros(self.num_sims)
        spectra = np.zeros(self.num_sims)
        # Get solar power from chosen spectrum
        path = self.sims[0].conf['Simulation']['input_power_wv']
        wv_vec, p_vec = np.loadtxt(path, usecols=(0, 2), unpack=True, delimiter=',')
        # Get interpolating function for power
        p_wv = interpolate.interp1d(wv_vec, p_vec, kind='linear',
                                    bounds_error=False, fill_value='extrapolate')
        # Assuming the leaves contain frequency values, sum over all of them
        for i, sim in enumerate(self.sims):
            ref, trans, absorb = sim.data['transmission_data'][port]
            freq = sim.conf['Simulation']['params']['frequency']
            wvlgth = consts.c / freq
            wvlgth_nm = wvlgth * 1e9
            freqs[i] = freq
            wvlgths[i] = wvlgth
            sun_pow = p_wv(wvlgth_nm)
            spectra[i] = sun_pow * wvlgth_nm
            abs_vals[i] = sun_pow * absorb * wvlgth_nm
            ref_vals[i] = sun_pow * ref * wvlgth_nm
            trans_vals[i] = sun_pow * trans * wvlgth_nm
        # Now integrate all the weighted spectra and divide by the power of
        # the spectra
        wvlgths = wvlgths[::-1]
        abs_vals = abs_vals[::-1]
        ref_vals = ref_vals[::-1]
        trans_vals = trans_vals[::-1]
        spectra = spectra[::-1]
        power = intg.trapz(spectra, x=wvlgths * 1e9)
        wght_ref = intg.trapz(ref_vals, x=wvlgths * 1e9) / power
        wght_abs = intg.trapz(abs_vals, x=wvlgths * 1e9) / power
        wght_trans = intg.trapz(trans_vals, x=wvlgths) / power
        out = osp.join(base, 'weighted_transmission_data.dat')
        with open(out, 'w') as outf:
            outf.write('# Reflection, Transmission, Absorbtion\n')
            outf.write('%f,%f,%f' % (wght_ref, wght_trans, wght_abs))
        return wght_ref, wght_trans, wght_abs

    def plot_absorption_per_layer(self, input_ax=None, plot_layer=None, quant=None,
                                  sim_slice=(0, None), marker='--o', xlabel='',
                                  ylabel='', title='', leg_label=''):
        """
        Plots absorption per layer and error
        """

        if self.grouped_against is None:
            raise ValueError("Can only call plot_absorption_per_layer when "
                             "sims are grouped against a parameter")
        results_dir = self.sims[0].conf['General']['results_dir']
        results_dict = {}
        xvals = np.zeros(len(self.sims[sim_slice[0]:sim_slice[1]]))
        for i, sim in enumerate(self.sims[sim_slice[0]:sim_slice[1]]):
            xvals[i] = sim.conf[self.grouped_against]
            abs_arr = sim.data['abs_per_layer']
            flux_total = 0
            integ_total = 0
            integ_polar_total = 0
            for (layer, flux, integ, integ_polar, diff, diff_polar) in abs_arr:
                layer = layer.decode('utf-8')
                if layer in results_dict:
                    results_dict[layer][0].append(flux.real)
                    results_dict[layer][1].append(integ.real)
                    results_dict[layer][2].append(diff.real)
                    results_dict[layer][3].append(integ_polar.real)
                    results_dict[layer][4].append(diff_polar.real)
                else:
                    results_dict[layer] = [[flux.real], [integ.real],
                                           [diff.real], [integ_polar.real],
                                           [diff_polar.real]]
                flux_total += flux.real
                integ_total += integ.real
                if layer == "NW_AlShell":
                    integ_polar_total += integ_polar.real
                else:
                    integ_polar_total += integ.real
            total_diff = np.abs(flux_total.real - integ_total.real)/np.abs(flux_total)
            total_diff_polar = np.abs(flux_total.real - integ_polar_total.real)/np.abs(flux_total)
            # total_diff = np.real(flux_total.real - integ_total.real)/flux_total.real
            # total_diff_polar = np.real(flux_total.real - integ_polar_total.real)/flux_total.real
            if 'total' in results_dict:
                results_dict['total'][0].append(flux_total.real)
                results_dict['total'][1].append(integ_total.real)
                results_dict['total'][2].append(total_diff.real)
                results_dict['total'][3].append(integ_polar_total.real)
                results_dict['total'][4].append(total_diff_polar.real)
            else:
                results_dict['total'] = [[flux_total.real], [integ_total.real],
                                         [total_diff.real], [integ_polar_total.real],
                                         [total_diff_polar.real]]
        layers = results_dict.keys()
        if plot_layer is not None:
            layers = [l for l in layers if l == plot_layer]

        quants = {'fluxmethod_absorption': 0,
                  'integralmethod_absorption': 1,
                  'relativediff': 2,
                  'integralmethod_absorption_polar': 3,
                  'relativediff_polar': 4}
        if quant is not None:
            quants = {q: ind for q, ind in quants.items() if q in quant}
        xvals = 1e9*consts.c/xvals
        for layer in layers:
            results = results_dict[layer]
            for name, ind in quants.items():
                # diff = abs(results[ind][-1] - results[ind][0])/abs(results[ind][-1])
                diff = abs(results[ind][-1] - results[ind][0])/abs(results[ind][-1])
                if input_ax is None:
                    fig, ax = plt.subplots()
                else:
                    ax = input_ax
                pfile = osp.join(results_dir,
                                     '{}_{}.png'.format(layer, name))
                print(np.amin(results[ind]))
                ax.plot(xvals, results[ind], marker, label=leg_label)
                ylims = ax.get_ylim()
                ax.set_ylim([0, ylims[-1]])
                ax.set_xlabel(self.grouped_against[-1])
                if ylabel:
                    ax.set_ylabel(ylabel)
                if xlabel:
                    ax.set_xlabel(xlabel)
                if title:
                    ax.set_title(title)
                if input_ax is None:
                    plt.savefig(pfile)
        return ax, diff

    def convergence(self, quantity, err_type='global', scale='linear'):
        """Plots the convergence of a field across all available simulations"""

        self.log.info('Plotting convergence')
        base = self.sims[0].conf['General']['base_dir']
        if err_type == 'local':
            fglob = osp.join(base, 'localerror_%s*.dat' % quantity)
        elif err_type == 'global':
            fglob = osp.join(base, 'globalerror_%s*.dat' % quantity)
        elif err_type == 'adjacent':
            fglob = osp.join(base, 'adjacenterror_%s*.dat' % quantity)
        else:
            self.log.error('Attempting to plot an unsupported error type')
            raise ValueError
        paths = glob.glob(fglob)
        for path in paths:
            labels = []
            errors = []
            with open(path, 'r') as datf:
                for line in datf.readlines():
                    lab, err = line.split(',')
                    labels.append(lab)
                    errors.append(err)
            fig = plt.figure(figsize=(9, 7))
            plt.ylabel('M.S.E of %s' % quantity)
            plt.xlabel('Number of Fourier Terms')
            plt.plot(labels, errors)
            plt.yscale(scale)
            # plt.xticks(x,labels,rotation='vertical')
            plt.tight_layout()
            plt.title(osp.basename(base))
            if self.gconf['General']['save_plots']:
                if '_excluded' in path:
                    excluded = '_excluded'
                else:
                    excluded = ''
                name = '%s_%sconvergence_%s%s.png' % (
                    osp.basename(base), err_type, quantity, excluded)
                path = osp.join(base, name)
                fig.savefig(path)
            if self.gconf['General']['show_plots']:
                plt.show()
            plt.close(fig)

    def plot_scalar_reduce(self, quantity, plane, pval, draw=False, fixed=None):
        """Plot the result of a particular scalar reduction for each group"""

        sim = self.sims[0]
        base = osp.expandvars(sim.conf['General']['results_dir'])
        self.log.info('Plotting scalar reduction of %s for quantity %s' % (base, quantity))
        cm = plt.get_cmap('jet')
        max_depth = sim.conf['General']['max_depth']
        period = sim.conf['Simulation']['params']['array_period']
        x = np.arange(0, period, sim.dx)
        y = np.arange(0, period, sim.dy)
        z = np.arange(0, max_depth + sim.dz, sim.dz)
        ftype = sim.conf['General']['save_as']
        if ftype == 'npz':
            globstr = osp.join(base, 'scalar_reduce*_%s.npy' % quantity)
            files = glob.glob(globstr)
        elif ftype == 'hdf5':
            self.log.warning('FIX LOAD IN GLOBAL SCALAR REDUCE')
            globstr = osp.join(base, 'scalar_reduce*_%s.npy' % quantity)
            files = glob.glob(globstr)
        else:
            raise ValueError('Incorrect file type in config')
        title = 'Reduction of %s' % quantity
        for datfile in files:
            p = False
            if ftype == 'npz':
                scalar = np.load(datfile)
            elif ftype == 'hdf5':
                self.log.warning('FIX LOAD IN GLOBAL SCALAR REDUCE')
                scalar = np.load(datfile)
            else:
                raise ValueError('Incorrect file type in config')
            cs = self.get_plane(scalar, plane, pval)
            if plane == 'yz' or plane == 'zy':
                labels = ('y [um]', 'z [um]', quantity, title)
                if sim.conf['General']['save_plots']:
                    fname = 'scalar_reduce_%s_plane_2d_yz.png' % quantity
                    p = osp.join(base, fname)
                show = sim.conf['General']['show_plots']
                self.sims[0].heatmap2d(y, z, cs, labels, plane, pval,
                               save_path=p, show=show, draw=draw, fixed=fixed)
            elif plane == 'xz' or plane == 'zx':
                labels = ('x [um]', 'z [um]', quantity, title)
                if sim.conf['General']['save_plots']:
                    fname = 'scalar_reduce_%s_plane_2d_xz.png' % quantity
                    p = osp.join(base, fname)
                show = sim.conf['General']['show_plots']
                self.sims[0].heatmap2d(sim, x, z, cs, labels, plane, pval,
                               save_path=p, show=show, draw=draw, fixed=fixed)
            elif plane == 'xy' or plane == 'yx':
                labels = ('y [um]', 'x [um]', quantity, title)
                if sim.conf['General']['save_plots']:
                    fname = 'scalar_reduce_%s_plane_2d_xy.png' % quantity
                    p = osp.join(base, fname)
                self.sims[0].heatmap2d(sim, x, y, cs, labels, plane, pval,
                               save_path=p, show=show, draw=draw, fixed=fixed)

    def transmission_data(self, absorbance, reflectance, transmission, port='Substrate'):
        """Plot transmissions, absorption, and reflectance assuming leaves are frequency"""

        base = self.sims[0].conf['General']['results_dir']
        self.log.info('Plotting transmission data for group at %s' % base)
        # Assuming the leaves contain frequency values, sum over all of them
        freqs = np.zeros(self.num_sims)
        refl_l = np.zeros(self.num_sims)
        trans_l = np.zeros(self.num_sims)
        absorb_l = np.zeros(self.num_sims)
        bport = port.encode('utf-8')
        for i, sim in enumerate(self.sims):
            # Unpack data for the port we passed in as an argument
            tdata = sim.data['transmission_data']
            pdata = tdata[tdata.port == bport][0]
            ref = pdata[1]
            trans = pdata[2]
            absorb = pdata[3]
            freq = sim.conf['Simulation']['params']['frequency']
            freqs[i] = freq
            trans_l[i] = trans
            refl_l[i] = ref
            absorb_l[i] = absorb
        freqs = (consts.c / freqs[::-1]) * 1e9
        refl_l = refl_l[::-1]
        absorb_l = absorb_l[::-1]
        trans_l = trans_l[::-1]
        plt.figure()
        if absorbance:
            self.log.info('Plotting absorbance')
            plt.plot(freqs, absorb_l, '-o', label='Absorption')
        if reflectance:
            plt.plot(freqs, refl_l, '-o', label='Reflection')
        if transmission:
            plt.plot(freqs, trans_l, '-o', label='Transmission')
        plt.legend(loc='best')
        figp = osp.join(base, 'transmission_plots_port%s.png'%port)
        plt.xlabel('Wavelength (nm)')
        plt.ylim((0, 1.0))
        plt.savefig(figp)
        plt.close()

    def plot_q_values(self):
        """
        Plot the q values for different numbers of basis terms on the same axis
        """
        if 'numbasis' not in self.grouped_against:
            raise ValueError("""Simulations must be grouped against number of
            basis terms to plot q values on the same axis""")

        layers = self.sims[0].layers
        base = self.sims[0].conf['General']['results_dir']
        freq = self.sims[0].conf[('Simulation', 'params', 'frequency')]
        wvlgth = 1e9*consts.c/freq
        for lname, l_obj in layers.items():
            plt.figure()
            title = 'Layer: {}, Freq = {:.3E}, Wavelength = {:.2f} nm'
            plt.title(title.format(lname, freq, wvlgth))
            for sim in self.sims:
                qarr = sim.data['{}_qvals'.format(lname)]
                label = 'Numbasis: {}'
                label = label.format(sim.conf[('Simulation','params','numbasis')])
                plt.scatter(qarr.real, qarr.imag/(2*np.pi), s=.75, label=label)
                # pt = (qarr[0].real, qarr[0].imag)
                # theta = np.linspace(0, 1.48, 200)
                # plt.plot(pt[0]*np.cos(theta), pt[1]/np.cos(theta), 'r--')
                # plt.annotate(leg_str, xy=(.95,.95), xycoords='axes fraction',
                #              size=14, ha='right', va='top',
                #              bbox=dict(boxstyle='round', fc='w'))
            plt.xlabel('Re(q) [radians/micron]')
            plt.ylabel('Im(q) [1/microns]')
            plot_path = osp.join(base, '{}_qvals.png'.format(lname))
            plt.grid(True)
            plt.legend(loc='best')
            plt.savefig(plot_path)
            plt.close()

def scatter3d(x, y, z, cs=None, colorsMap='jet'):
    fig = plt.figure()
    ax = Axes3D(fig)
    if cs is not None:
        cm = plt.get_cmap(colorsMap)
        cNorm = matplotlib.colors.Normalize(vmin=min(cs), vmax=max(cs))
        scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cm)
        ax.scatter(x, y, z, c=scalarMap.to_rgba(cs))
        scalarMap.set_array(cs)
        fig.colorbar(scalarMap)
    else:
        ax.scatter(x, y, z)
    return fig, ax


def integrate1d(arr, xvals, meth=intg.trapz):
    x_integral = meth(arr, x=xvals, axis=0)
    return x_integral


def integrate2d(arr, xvals, yvals, meth=intg.trapz):
    x_integral = meth(arr, x=xvals, axis=0)
    y_integral = meth(x_integral, x=yvals, axis=0)
    return y_integral


def integrate3d(arr, ax0, ax1, ax2, order=(0, 1, 2), meth=intg.trapz):
    """
    Perform numerical integration on a 3D array `arr` whose spatial coordinates
    along each axis correspond to the arrays `ax0`, `ax1`, and `ax2`.

    The `order` kwarg specifies the integration order of the axes. For example,
    if order=(1, 0, 2) the array is integrated along axis 1, followed by axis
    0, then axis 2. This is useful for determining if the integration order
    matters, which it sometimes can numerically.

    :param arr: The 3D numpy array to be integrated. Must have shape (len(ax0),
    len(ax1), len(ax2))
    :param ax0, ax1, ax2: 1D numpy arrays specifying the spatial coordinates of
    each axis
    :param meth: The integration method to use.
    :param order: A length 3 tuple specifying the order in which to integrate.
    :type meth: function
    """

    coords = (ax0, ax1, ax2)
    result = arr
    remaining_axes = [0, 1, 2]
    for ax in order:
        int_ax = remaining_axes.index(ax)
        result = meth(result, x=coords[ax], axis=int_ax)
        remaining_axes.pop(int_ax)
    return result

def counted(fn):
    def wrapper(self):
        wrapper.called += 1
        return fn(self)
    wrapper.called = 0
    wrapper.__name__ = fn.__name__
    return wrapper


def _call_func(quantity, obj, args):
    """
    Calls an instance method of an object with args
    """

    log = logging.getLogger(__name__)
    try:
        result = getattr(obj, quantity)(*args)
    except AttributeError:
        log.error("Object %s has no method: %s", str(obj), quantity)
        raise
    except:
        log.error("Error while calling method %s of object %s", quantity,
                  str(obj))
        raise
    return result


def execute_plan(conf, plan, grouped_against=None, grouped_by=None):
    """
    Executes the given plan for the given Config object or list/tuple of Config
    objects. If conf is a single Config object, a Simulation object will
    created and the provided plan will be executed for it. If a list or tuple
    of Config objects is given, a SimulationGroup object will be created and
    the provided plan will be executed for it.

    `plan` must be a dictionary with the following structure::

        {'crunch':
            'funcA':
                compute: True
                args: [['list', 'of', 'args'], ['another', 'set']]
            'funcB':
                compute: False
                args: [['this function is skipped']]
         'plot': 'same structure as crunch'
        }

    For each of the keys under the `crunch` and `plot` sections, a method of
    the created Simulation/SimulationGroup object with the same name will be
    called for each set of provided arguments. No attempts are made to validate
    the given plan before raising exceptions
    """

    log = logging.getLogger(__name__)
    try:
        if isinstance(conf, list) or isinstance(conf, tuple):
            sample_conf = conf[0]
            log.info("Executing SimulationGroup plan")
            if not sample_conf['General']['save_as']:
                obj = SimulationGroup([Simulation(simulator=Simulator(c)) for c
                                       in conf],
                                      grouped_against=grouped_against,
                                      grouped_by=grouped_by)
            else:
                obj = SimulationGroup([Simulation(conf=c) for c in conf],
                                      grouped_against=grouped_against,
                                      grouped_by=grouped_by)
            ID = str(obj)
        else:
            log.info("Executing Simulation plan")
            if not conf['General']['save_as']:
                obj = Simulation(simulator=Simulator(conf))
            else:
                obj = Simulation(conf=conf)
                # Concatenate the field components in each layer into a single 3D
                # array, but add to blacklist so the concatenated arrays don't get
                # written to disk
                # FIXME: This is grossly memory-inefficient
                for f in ('Ex', 'Ey', 'Ez'):
                    ks = ['{}_{}'.format(lname, f) for lname in obj.layers.keys()]
                    obj.data[f] = np.concatenate([obj.data[k] for k in ks])
                    obj.data.add_to_blacklist(f)
            ID = obj.id[0:10]

        for task_name in ('crunch', 'plot'):
            if task_name not in plan:
                continue
            task = plan[task_name]
            log.info("Beginning %s for obj %s", task_name, ID)
            for func, data in task.items():
                if not data['compute']:
                    continue
                else:
                    argsets = data['args']
                if argsets and isinstance(argsets[0], list):
                    for argset in argsets:
                        if argset:
                            _call_func(func, obj, argset)
                        else:
                            _call_func(func, obj, [])
                else:
                    if argsets:
                        _call_func(func, obj, argsets)
                    else:
                        _call_func(func, obj, [])
            log.info("Completed %s for obj %s", task_name, ID)
        log.info("Plan execution for obj %s complete", ID)
        if isinstance(obj, Simulation):
            log.info("Saving and clearing data for Simulation %s", ID)
            if obj.conf['General']['sample_dict']:
                obj.write_data(blacklist=('normE', 'normEsquared', 'Ex', 'Ey',
                                          'Ez'))
            else:
                obj.write_data()
            obj.clear_data()
        else:
            log.info("Clearing data for SimulationGroup %s", ID)
            for sim in obj.sims:
                sim.clear_data()
    except Exception as e:
        if isinstance(conf, list) or isinstance(conf, tuple):
            log.error('Group raised exception')
        else:
            log.error('Conf %s raised exception', conf['General']['sim_dir'])
        raise


class Processor(object):
    """
    Generic class for automating the processing of Simulations and
    SimulationGroups
    """

    def __init__(self, path=None, conf=None, sims=[], sim_groups=[], failed_sims=[]):
        if path is not None:
            if osp.isfile(path):
                self.gconf = Config(osp.abspath(path))
            else:
                raise ValueError("Path {} to conf file doesn't "
                                 " exist".format(path))
        elif conf is not None:
            self.gconf = conf
        else:
            raise ValueError("Must pass in either path of conf kwargs")
        # self.gconf.expand_vars()
        self.log = logging.getLogger(__name__)
        self.log.debug("Processor base init")
        self.sims = sims
        self.sim_groups = sim_groups
        # A place to store any failed sims (i.e sims that are missing their
        # data file)
        self.failed_sims = failed_sims
        self.grouped_against = None
        self.grouped_by = None

    def load_confs(self, *args, **kwargs):
        """
        Load configs from disk

        Collect all the simulations contained in the HDF5 database file located
        at filesystem path `db` satisfying the query `query`.

        Parameters
        ----------
        db : str
            A path to an HDF5 file containing the database of simulation
            configs
        base_dir : str, optional
            The base directory of the directory tree that all simulations will
            dump their output data into. If not specified, defaults to the
            directory of the database file.
        query : str, optional
            A query string conforming to PyTables table.where syntax specifying
            which configs should be loaded from the HDF5 database. If not
            specified, all configs in the database are loaded
        table_path : str, optional
            String specifying path inside the HDF5 database to the group
            containing the HDF5 simulations table. Default: '/'
        table_path : str, optional
            String specifying the name of the HDF5 simulations table.
            Default: 'simulations'

        Returns
        -------
        list
            A list of :py:class:`nanowire.preprocess.config.Config` objects
        """
        confs, t_sweeps = load_confs(*args, **kwargs)
        self.t_sweeps = t_sweeps
        self.sim_confs = confs
        return confs, t_sweeps

    def make_sims(self):
        """
        Build out the self.sims list by constructing Simulations objects from
        the collected confs
        """
        self.sims = [Simulation(conf=c) for c in self.sim_confs]

    def get_param_vals(self, parseq):
        """
        Return all possible values of the provided parameter for this sweep
        """

        vals = []
        for conf in self.sim_confs:
            val = conf[parseq]
            if val not in vals:
                vals.append(val)
        return vals

    def filter_by_param(self, pars):
        """Accepts a dict where the keys are parameter names and the values are
        a list of possible values for that parameter. Any simulation whose
        parameter does not match any of the provided values is removed from the
        sims and sim_groups attribute"""

        assert(type(pars) == dict)
        for par, vals in pars.items():
            vals = [type(self.sim_confs[0][par])(v) for v in vals]
            self.sim_confs = [conf for conf in self.sim_confs if conf[par] in vals]
            groups = []
            for group in self.sim_groups:
                filt_group = [conf for conf in group if conf[par] in vals]
                groups.append(filt_group)
            self.sim_groups = groups
        assert(len(self.sim_confs) >= 1)
        return self.sim_confs, self.sim_groups

    def call_func(self, quantity, obj, args):
        """
        Calls an instance method of an object with args
        """

        try:
            result = getattr(obj, quantity)(*args)
        except KeyError:
            self.log.error("Unable to call the following function: %s",
                           quantity, exc_info=True, stack_info=True)
            raise
        return result

    def process(self, crunch=True, plot=True, gcrunch=True, gplot=True,
                grouped_against=None, grouped_by=None):
        # Create pool if processing in parallel
        if self.gconf['General']['post_parallel']:
            num_procs = self.gconf['General']['num_cores']
            pool = mp.Pool(processes=num_procs)
        # First process all the sims
        self.log.info('Beginning processing for all sims ...')
        plan = copy.deepcopy(self.gconf['Postprocessing']['Single'])
        if not crunch:
            del plan['crunch']
        if not plot:
            del plan['plot']
        if 'crunch' in plan or 'plot' in plan:
            if not self.gconf['General']['post_parallel']:
                for conf in self.sim_confs:
                    execute_plan(conf, plan, grouped_against=grouped_against,
                                 grouped_by=grouped_by)
            else:
                self.log.info('Plotting sims in parallel using %s cores ...',
                              str(num_procs))
                args_list = list(zip(self.sim_confs, repeat(plan),
                                     repeat(grouped_against),
                                     repeat(grouped_by)))
                pool.starmap(execute_plan, args_list)
        # Process groups
        plan = copy.deepcopy(self.gconf['Postprocessing']['Group'])
        if not gcrunch:
            del plan['crunch']
        if not gplot:
            del plan['plot']
        if 'crunch' in plan or 'plot' in plan:
            if not self.gconf['General']['post_parallel']:
                for group in self.sim_groups:
                    execute_plan(group, plan, grouped_against=grouped_against,
                                 grouped_by=grouped_by)
            else:
                self.log.info('Plotting sims in parallel using %s cores ...',
                              str(num_procs))
                args_list = list(zip(self.sim_groups, repeat(plan),
                                     repeat(grouped_against),
                                     repeat(grouped_by)))
                pool.starmap(execute_plan, args_list)
        if self.gconf['General']['post_parallel']:
            pool.close()
            pool.join()
