import os
import os.path as osp
import traceback
import glob
import sys
import time
import copy
import traceback
import logging
import pint
import hashlib
import unqlite
import tables as tb
import multiprocessing as mp
import matplotlib
try:
    os.environ['DISPLAY']
except KeyError:
    matplotlib.use('Agg')
import matplotlib.pyplot as plt
# plt.style.use(['paper'])
import matplotlib.cm as cmx
import matplotlib.lines as mlines
import matplotlib.patches as mpatches
import scipy.constants as consts
import scipy.integrate as intg
import numpy as np
# import naturalneighbor as nn
from mpl_toolkits.mplot3d import Axes3D
from itertools import repeat, product
from scipy import interpolate
from sympy import Circle
from nanowire.optics.simulate import Simulator
from nanowire.preprocess.preprocessor import (
    UNSAFE_OPERATORS,
    fn_pint_quantity,
    fn_pint_magnitude,
)
from nanowire.preprocess import Parser
from nanowire.optics.utils.utils import (
    get_nk,
    get_incident_power,
    get_incident_amplitude,
)
from nanowire.utils.config import (
    Config,
    load_confs,
    group_against as _group_against,
    group_by as _group_by,
)
from nanowire.utils.utils import (
    cartesian_product,
    pairwise,
    arithmetic_arange,
    open_pytables_file,
    get_record,
    numpy_arr_to_dict,
    ureg,
    Q_,
)
from nanowire.utils.logging import (
    IdFilter,
)
from nanowire.utils.data_manager import (
    DataManager,
    HDF5DataManager,
    create_power_rescaling_hook,
    create_field_rescaling_hook,
)
from nanowire.optics.utils.geometry import (
    Layer,
    get_mask_by_shape,
    get_mask_by_material,
    get_layers
)

# Configure logging for this module
# Get numeric level safely
logfile = 'logs/postprocess.log'
debug = getattr(logging, 'debug'.upper(), None)
info = getattr(logging, 'info'.upper(), None)
warn = getattr(logging, 'warn'.upper(), None)
# Set formatting
formatter = logging.Formatter('%(asctime)s [%(name)s:%(levelname)s]'
                              ' - %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')
# Create logger
logger = logging.getLogger(__name__)
logger.setLevel(info)
log_dir, logfile = osp.split(osp.expandvars(logfile))
# Set up file handler
try:
    os.makedirs(log_dir)
except OSError:
    # Log dir already exists
    pass
output_file = osp.join(log_dir, logfile)
fhandler = logging.FileHandler(output_file)
fhandler.setFormatter(formatter)
fhandler.setLevel(debug)
# We dont want log records generated by Simulation instances making it to the
# global simulate.log file
fhandler.addFilter(IdFilter(reject=True))
logger.addHandler(fhandler)
# Logging to console
ch = logging.StreamHandler()
ch.setLevel(debug)
ch.setFormatter(formatter)
ch.addFilter(IdFilter(reject=True))
logger.addHandler(ch)


# This will log any uncaught exceptions
def handle_exception(exc_type, exc_value, exc_traceback):
    if issubclass(exc_type, KeyboardInterrupt):
        sys.__excepthook__(exc_type, exc_value, exc_traceback)
        return

    logger.critical("Uncaught exception", exc_info=(exc_type, exc_value,
                                                    exc_traceback))


sys.excepthook = handle_exception


class Simulation:
    """
    An object that represents a completed simulation and it used for
    postprocessing the data associated with said Simulation. It stores a
    DataManager object for managing the reading and writing of all data in the
    ``self.data`` attribute. It also stores a Config object, which is a
    dict-like object representing the configuration for the simulation, in the
    ``self.conf`` attribute.

    Parameters
    ----------

    outdir : str
        The directory this simulation will output all plots and postprocessed
        data files to
    bandwith : float, :py:class:`Quantity`
        Either a floating point number or a pint Quantity representing the
        spectral bandwidth for this simulation. This is necessary for
        determining the input power given a spectrum and scaling the electric
        and magnetic fields accordingly. If a float is received, it is assumed
        to be in units of Hertz and will immediately be converted to a pint
        Quantity with units of Hertz. If a Quantity is received, it will be
        converted to Hertz if possible, and raise a Dimensionality error if
        not. If unspecified, and attempt will be made to read the bandwidth
        from the Postprocessing section of the Config, and raises a value error
        if it cannot be found.
    """

    def __init__(self, outdir, bandwidth, spectrum='am1.5g', conf=None,
                 simulator=None, skip_power_rescale_hook=False,
                 rescale_method=False):
        """
        :param :class:`~utils.config.Config`: Config object for this simulation
        """
        self.spectrum = spectrum
        self.skip_power_rescale_hook = skip_power_rescale_hook
        self.use_rescale_method = rescale_method
        if conf is None and simulator is None:
            raise ValueError('Must pass in either a Config object or a'
                             ' Simulator object')
        if isinstance(conf, str):
            self.conf = Config.fromFile(conf)
        elif isinstance(conf, Config):
            self.conf = conf
        else:
            self.conf = copy.deepcopy(simulator.conf)
        # if bandwidth is None:
        #     try:
        #         self.bandwidth = self.conf['Postprocessing/bandwidth']
        #     except KeyError as e:
        #         msg = 'Need to specify bandwidth either as a kwarg or in ' \
        #               'the Postprocessing config at Postprocessing/bandwidth'
        #         e.args = (msg,)
        #         raise
        # else:
        if bandwidth is None:
            self.bandwidth = None
            self.skip_power_rescale_hook = True
        elif isinstance(bandwidth, pint.quantity._Quantity):
            self.bandwidth = bandwidth.to('hertz', 'spectroscopy')
        elif isinstance(bandwidth, (float, int)):
            self.bandwidth = Q_(bandwidth, 'hertz')
        elif isinstance(bandwidth, tuple):
            if not all(isinstance(el, pint.quantity._Quantity) for el in
                       bandwidth):
                msg = 'Invalid bandwidth argument: {}'.format(bandwidth)
                raise ValueError(msg)
            else:
                self.bandwidth = bandwidth
        else:
            msg = 'Invalid bandwidth argument: {}'.format(bandwidth)
            raise ValueError(msg)
        self.ID = self.conf.ID
        self.path = osp.abspath(osp.normpath(osp.expandvars(outdir)))
        self.base, self.dir = osp.split(self.path)
        self.fhandler = logging.FileHandler(osp.join(self.path, 'postprocess.log'))
        self.fhandler.addFilter(IdFilter(ID=self.ID))
        formatter = logging.Formatter('%(asctime)s [%(name)s:%(levelname)s] - %(message)s',datefmt='%m/%d/%Y %I:%M:%S %p')
        self.fhandler.setFormatter(formatter)
        log = logging.getLogger(__name__)
        log.addHandler(self.fhandler)
        # Store the logger adapter to the module level logger as an attribute.
        # We use this to log in any methods, and the sim_id of this instance
        # will get stored in the log record
        self.log = logging.LoggerAdapter(log, {'ID': self.ID})
        self.log.debug('Logger initialized')
        if not self.conf['General']['save_as']:
            self.data = {}
            simulator.setup()
            comps = {'Ex', 'Ey', 'Ez', 'Hx', 'Hy', 'Hz'}
            dkeys = set(simulator.data.keys())
            compute = any(comp not in dkeys for comp in comps)
            if compute:
                Ex, Ey, Ez, Hx, Hy, Hz = simulator.compute_fields()
                self.data.update({'Ex': Ex, 'Ey': Ey, 'Ez': Ez,
                                  'Hx': Hx, 'Hy': Hy, 'Hz': Hz})
            if 'fluxes' not in dkeys:
                flux_arr = simulator.compute_fluxes()
                self.data.update({'fluxes': flux_arr})
            # self.data.update(simulator.data)
        else:
            self.data = self._get_data_manager()
        # Compute and store dx, dy, dz at attributes
        self.X = self.data['xcoords']
        self.Y = self.data['ycoords']
        self.Z = self.data['zcoords']
        self.dx = self.X[1] - self.X[0]
        self.dy = self.Y[1] - self.Y[0]
        self.dz = self.Z[1] - self.Z[0]
        self.xsamps = len(self.X)
        self.ysamps = len(self.Y)
        self.zsamps = len(self.Z)
        self.period = self.conf['Simulation/array_period']
        self.layers = get_layers(self)
        self.height = self.get_height()
        if self.use_rescale_method:
            # A set of keys that are affected by the rescaled fields. They will
            # be written and read by appending '_rescaled' to their name when
            # using get_quantity and extend_data
            self.rescaled_keys = {'power_absorbed', 'normE', 'normEsquared',
                                  'genRate', 'jph_integral_method'}
            self.log.info("GET HOOKS: ", self.data.get_hooks)
            try:
                factors = self.get_quantity('rescaling_factors')[0]
            except KeyError:
                factors = self.rescaling_factors()
                self.write_data()
            else:
                factors = numpy_arr_to_dict(factors)
            hook = create_field_rescaling_hook(factors, self.layers)
            self.data.add_get_hook(hook)
            self.clear_data()
            # We clear the data so we don't consume a bunch of memory, but when
            # we do that we lose the rescaled fields and fluxes, and the
            # rescaling cache prevents us from doing it again. So, fix the
            # cache
            for k in ('Ex', 'Ey', 'Ez', 'fluxes'):
                self.data.power_rescaled[k] = False

    def _get_data_manager(self):
        """
        Factory function that instantiates the correct data manager object
        depending on the file type specified in the config
        """

        ftype = self.conf['General']['save_as'].lower()
        if self.skip_power_rescale_hook:
            self.log.warning("Rescaling to unit spectrum!!!")
            power = Q_(1.0, ureg.watt / ureg.meter**2)
            amplitude = Q_(1.0, ureg.volts / ureg.meter)
        else:
            amplitude = get_incident_amplitude(
                self.spectrum,
                self.conf['Simulation/frequency'],
                self.conf['Simulation/polar_angle'],
                self.bandwidth,
                logger=self.log
            )
            power = get_incident_power(
                self.spectrum,
                self.conf['Simulation/frequency'],
                self.conf['Simulation/polar_angle'],
                self.bandwidth,
                logger=self.log
            )
        hook = create_power_rescaling_hook(power, amplitude, self.log)
        # Create the rescaling hook for the data manager that rescales all
        # powers and fields using the power/amplitude of the incident spectrum
        if ftype == 'hdf5':
            file_path = os.path.join(self.path, 'sim.hdf5')
            gpath = '/sim_{}'.format(self.conf.ID)
            # Add a get_hook to the data manager that rescales the field
            # components using the incident amplitude
            manager = HDF5DataManager(file_path, group_path=gpath, mode='a',
                                      logger=self.log)
            manager.add_get_hook(hook)
            for k in ('Ex', 'Ey', 'Ez', 'fluxes'):
                manager.add_to_blacklist(k)
            return manager
        else:
            raise ValueError('Invalid file type in config')

    def write_data(self, blacklist=()):
        """
        Writes the data. This is a simple wrapper around the write_data()
        method of the DataManager object, with some code to compute the time it
        took to perform the write operation
        """

        start = time.time()
        self.data.write_data(blacklist=blacklist)
        end = time.time()
        self.log.info('Write time: %.2f seconds', end - start)

    def get_height(self):
        """Returns the total height of the device"""
        return sum(layer.thickness for layer in self.layers.values())

    def get_quantity(self, quantity):
        """
        Retrieves the entire 3D matrix for some scalar quantity.

        If the quantity does not exist in the data manager, and attempt is made
        to calculate it by calling an instance method of the same name. If that
        fails, an AttributeError is raised

        Parameters
        ----------

        quantity : str
            The quantity you would like to retrieve (ex: 'Ex' or 'normE')

        Returns
        -------

        numpy.ndarray
            A 3D numpy array shape (zsamples, xsamples, ysamples) containing
            the specified quantity

        Raises
        ------

        AttributeError
            If you tried to retrieve a quantity that does not exist in the data
            dict and cannot be calculated via a method of the same name in this
            class
        """

        if self.use_rescale_method and quantity in self.rescaled_keys:
            key = quantity + '_rescaled'
        else:
            key = quantity
        self.log.debug('Retrieving scalar quantity %s', str(key))
        try:
            return self.data[key]
        except KeyError:
            self.log.info("Key {} does not exist! Computing ...".format(key))
        try:
            result = getattr(self, quantity)()
            self.extend_data(key, result)
            return result
        except:
            self.log.error('Unable to calculate following quantity: %s',
                           quantity)
            raise

    def extend_data(self, quantity, new_data):
        """
        Adds a new key, value pair to the DataManager object
        """

        if self.use_rescale_method and quantity in self.rescaled_keys:
            quantity += '_rescaled'
        if quantity in self.data:
            self.log.debug("Quantity %s exists in matrix, updating", quantity)
            self.data[quantity] = new_data
        else:
            self.log.debug('Adding %s to data dict', str(quantity))
            self.data[quantity] = new_data

    def clear_data(self):
        """
        Clears all the data key the self.data DataManager object to free up memory
        """
        if isinstance(self.data, DataManager):
            self.data.clear_data()
        else:
            self.data = {}

    def close(self):
        if isinstance(self.data, DataManager):
            self.data.close()

    def normE(self):
        """
        Calculates the norm of E. Adds it to the data dict for the simulation
        and also returns a 3D array

        Returns
        -------
        np.ndarray
            A 3D numpy array containing :math:`|E|`
        """

        # Get the magnitude of E and add it to our data
        E_mag = Q_(np.zeros_like(self.data['Ex'], dtype=np.float64),
                   ureg.volt ** 2 / ureg.meter ** 2)
        for comp in ('Ex', 'Ey', 'Ez'):
            d = self.get_quantity(comp)
            E_mag += np.absolute(d)**2
        E_mag = np.sqrt(E_mag)
        self.extend_data('normE', E_mag)
        return np.sqrt(E_mag)

    def normEsquared(self):
        """
        Calculates and returns normE squared. Adds it to the data dict for
        the simulation and also returns a 3D array

        Returns
        -------
        np.ndarray
            A 3D numpy array containing :math: |E|^2
        """

        # Get the magnitude of E and add it to our data
        E_magsq = Q_(np.zeros_like(self.data['Ex'], dtype=np.float64),
                     ureg.volt ** 2 / ureg.meter ** 2)
        for comp in ('Ex', 'Ey', 'Ez'):
            d = self.get_quantity(comp)
            E_magsq += np.absolute(d)**2
        self.extend_data('normEsquared', E_magsq)
        return E_magsq

    def normH(self):
        """Calculate and returns :math: |H|"""

        H_mag = np.zeros_like(self.data['Hx'], dtype=np.float64)
        for comp in ('Hx', 'Hy', 'Hz'):
            H_mag += np.absolute(self.data[comp])
        self.extend_data('normH', H_mag)
        return H_mag

    def normHsquared(self):
        """Calculates and returns :math: |H|^2"""

        H_magsq = np.zeros_like(self.data['Hx'], dtype=np.float64)
        for comp in ('Hx', 'Hy', 'Hz'):
            H_magsq += np.absolute(self.data[comp])**2
        self.extend_data('normHsquared', H_magsq)
        return H_magsq

    def genRate(self, units='1 / cm**3 / second'):
        """
        Computes and returns the 3D matrix containing the generation rate.

        units : str
            The units to return the generation rate in.
            Default: 1 / cm**3 / second
        """

        # We need to compute normEsquared before we can compute the generation
        # rate
        normEsq = self.get_quantity('normEsquared')
        # Prefactor for generation rate. Note we gotta convert from m^3 to
        # cm^3, hence 1e6 factor
        fact = 1.0 * ureg.vacuum_permittivity / ureg.hbar
        gvec = Q_(np.zeros_like(normEsq), normEsq.units)
        # Main loop to compute generation in each layer
        freq = self.conf['Simulation/frequency']
        for name, layer in self.layers.items():
            self.log.debug('LAYER: %s', name)
            self.log.debug('LAYER T: %f', layer.thickness)
            self.log.debug('START: %f', layer.start)
            self.log.debug('END: %f', layer.end)
            # Use the layer object to get the nk matrix with correct material
            # geometry
            nmat, kmat = layer.get_nk_matrix(freq, self.X, self.Y)
            start, end = layer.get_inds(self.Z)
            gvec[start:end, :, :] = nmat * kmat * normEsq[start:end, :, :]
        gvec *= fact
        gvec.ito(units)
        self.extend_data('genRate', gvec)
        return gvec

    def angularAvg(self, quantity):
        """
        Perform an angular average of some quantity

        The purpose of this function is primarily to take the generation rate
        in 3D and average about the azimuthal angle to get a 2D generation
        rate. This 2D generation rate is then fed into an electrical model.
        """

        try:
            quant = self.get_quantity(quantity)
        except KeyError:
            getattr(self, quantity)()
            quant = self.get_quantity(quantity)
            # Make sure we don't compute it twice
            try:
                self.conf['Postprocessing']['Cruncher'][
                    quantity]['compute'] = False
            except KeyError:
                pass
        dx_mag = self.dx.magnitude
        dx_units = self.dx.units
        dy_mag = self.dy.magnitude
        dy_units = self.dy.units
        # Get spatial discretization
        rsamp = self.conf['General/r_samples']
        thsamp = self.conf['General/theta_samples']
        period = self.conf['Simulation/array_period']
        # Maximum r value such that circle and square unit cell have equal area
        rmax = period / np.sqrt(np.pi)
        # Diff between rmax and unit cell boundary at point of maximum
        # difference
        delta = rmax - period / 2.0
        # Extra indices we need to expand layers by
        x_inds = int(np.ceil(delta / self.dx))
        y_inds = int(np.ceil(delta / self.dy))
        # Use periodic BCs to extend the data in the x-y plane
        ext_vals = np.zeros((quant.shape[0], quant.shape[1] +
                             2 * x_inds, quant.shape[2] + 2 * y_inds),
                            dtype=quant.dtype)
        # Left-Right extensions. This indexing madness extracts the slice we
        # want, flips it along the correct dimension then sticks in the correct
        # spot in the extended array
        ext_vals[:, x_inds:-x_inds, 0:y_inds] = quant[:, :, -y_inds:]
        ext_vals[:, x_inds:-x_inds, -y_inds:] = quant[:, :, 0:y_inds]
        # Top-Bottom extensions
        ext_vals[:, 0:x_inds, y_inds:-y_inds] = quant[:, -x_inds:, :]
        ext_vals[:, -x_inds:, y_inds:- y_inds] = quant[:, 0:x_inds:, :]
        # Corners, slightly trickier
        # Top left
        ext_vals[:, 0:x_inds, 0:y_inds] = quant[:, -x_inds:, -y_inds:]
        # Bottom left
        ext_vals[:, -x_inds:, 0:y_inds] = quant[:, 0:x_inds, -y_inds:]
        # Top right
        ext_vals[:, 0:x_inds, -y_inds:] = quant[:, -x_inds:, -y_inds:]
        # Bottom right
        ext_vals[:, -x_inds:, -y_inds:] = ext_vals[:, 0:x_inds, 0:y_inds]
        # Now the center
        ext_vals[:, x_inds:-x_inds, y_inds:-y_inds] = quant[:, :, :]
        # Anna's method
        midpoint = (int(ext_vals.shape[1]/2), int(ext_vals.shape[2]/2))
        # Average the left and right slices
        right_edge_slice = ext_vals[:, midpoint[0]:, midpoint[1]]
        left_edge_slice = np.fliplr(ext_vals[:, 0:midpoint[0], midpoint[1]])
        edge_slice = (left_edge_slice + right_edge_slice)/2
        full_diag = np.diagonal(ext_vals, axis1=1, axis2=2)
        right_corner_slice = full_diag[:, midpoint[0]:]
        left_corner_slice = np.fliplr(full_diag[:, 0:midpoint[0]])
        corner_slice = (right_corner_slice + left_corner_slice)/2
        # Corner slice has different radial coordinates than the edge slice.
        # Need to interpolate onto the edge coordinates
        edge_rvec = Q_(np.linspace(0, rmax.magnitude, edge_slice.shape[1]),
                       rmax.units)
        rtot = (period / np.sqrt(2)) + (np.sqrt(2) * delta)
        corner_rvec = Q_(np.linspace(0, rtot.magnitude, corner_slice.shape[1]),
                         rmax.units)
        interp_coords = cartesian_product((self.Z, edge_rvec))
        corner_vals = interpolate.interpn((self.Z, corner_rvec), corner_slice,
                                          interp_coords).reshape(edge_slice.shape)
        avg = (edge_slice + corner_vals)/2
        self.extend_data('radial_coords', edge_rvec)

        # # Extend the points arrays to include these new regions
        # pts_left = Q_(np.array([dx_mag * i for i in range(-x_inds, 0)]),
        #               dx_units)
        # pts_right = Q_(np.array([self.X[-1].magnitude + dx_mag * i for i in range(1, x_inds + 1)]),
        #                dx_units)
        # x = np.concatenate((pts_left, self.X, pts_right))
        # pts_bot = Q_(np.array([dy_mag * i for i in range(-y_inds, 0)]),
        #              dy_units)
        # pts_top = Q_(np.array([self.Y[-1].magnitude + dy_mag * i for i in range(1, y_inds + 1)]),
        #              dy_units)
        # y = np.concatenate((pts_bot, self.Y, pts_top))
        # # The points on which we have data
        # points = (x, y)
        # # The points corresponding to "rings" in cylindrical coordinates. Note
        # # we construct these rings around the origin so we have to shift them
        # # to actually correspond to the center of the nanowire
        # rvec = Q_(np.linspace(0, rmax.magnitude, rsamp), rmax.units)
        # thvec = Q_(np.linspace(0, 2 * np.pi, thsamp), 'radians')
        # self.extend_data('radial_coords', rvec)
        # self.extend_data('angle_coords', thvec)
        # cyl_coords = Q_(np.zeros((len(rvec) * len(thvec), 2)), rmax.units)
        # start = 0
        # for r in rvec:
        #     xring = r * np.cos(thvec)
        #     yring = r * np.sin(thvec)
        #     cyl_coords[start:start + len(thvec), 0] = xring
        #     cyl_coords[start:start + len(thvec), 1] = yring
        #     start += len(thvec)
        # cyl_coords += period / 2.0
        # # For every z layer in the 3D matrix of our quantity
        # avgs = np.zeros((ext_vals.shape[0], len(rvec)))
        # i = 0
        # for layer in ext_vals:
        #     interp_vals = interpolate.interpn(
        #         points, layer, cyl_coords, method='linear')
        #     rings = interp_vals.reshape((len(rvec), len(thvec)))
        #     avg = np.average(rings, axis=1)
        #     avgs[i, :] = avg
        #     i += 1
        # avgs = avgs[:, ::-1]
        # Save to avgs dict for this sim
        key = quantity + '_angularAvg'
        self.extend_data(key, avg)
        return avg

    def power_absorbed(self, per_area=True, order=(0, 1, 2)):
        """
        Computes the absorption in each layer of the device as well as the
        total absorption using two methods.  The first method takes the
        difference between the areal power fluxes entering a layer and leaving
        a layer.

        :math:`P_{abs} = P_{in} - P_{out}`

        The second method uses the raw fields and computes a volume integral of
        the norm squared of the electric field

        .. math::

           P_{abs} &= \\frac{\\omega}{2} \\int |E|^2
                      \\mathrm{Im}[\\epsilon] dV \\\\
                   &= \\frac{\\omega \\epsilon_0}{2}
                      \\int |E|^2 \\mathrm{Im}[\\epsilon_r] dV

        Recall :math:`\mathrm{Im}[\epsilon_r] = 2nk` so we finally arrive at

        .. math::

           P_{abs} = \omega \int |E|^2 n k dV

        This provides a metric for quantifying how converged the real space
        reconstructions of the fields are.

        :param per_area: Compute the absorption in units of power per unit area
                         or not. If True, in units of Watts/micrometer^2
        :type per_area: bool
        """

        fluxes = self.get_quantity('fluxes')
        Esq = self.get_quantity('normEsquared')
        freq = self.conf['Simulation/frequency']
        # object type for pint Quantities
        dt = [('layer', 'S25'), ('flux_method', 'O'), ('int_method', 'O'),
              ('difference', 'O')]
        num_rows = len(self.layers)
        absorb_arr = np.recarray((num_rows,), dtype=dt)
        counter = 0
        for layer_name, layer_obj in self.layers.items():
            if layer_name == 'Air':
                absorb_arr[counter] = (layer_name,
                                       Q_(0.0, ureg.watts / ureg.meter**2),
                                       Q_(0.0, ureg.watts / ureg.meter**2),
                                       Q_(0.0, ureg.dimensionless))
                counter += 1
                continue
            # Method 1: Go through power flux
            bottom = layer_name+'_bottom'
            layers = [el.decode('utf-8') for el in fluxes['layer']]
            top_ind = layers.index(layer_name)
            bottom_ind = layers.index(bottom)
            forw_top = fluxes[top_ind]['forward']
            back_top = fluxes[top_ind]['backward']
            forw_bot = fluxes[bottom_ind]['forward']
            back_bot = fluxes[bottom_ind]['backward']
            # Minus sign because S4 stick a minus in front of all backward
            # components
            Pin = forw_top - back_bot
            Pout = forw_bot - back_top
            Pabs_flux = Pin - Pout
            if not np.isclose(Pabs_flux.imag, 0):
                raise ValueError('Imaginary part of absorbed power not zero')
            else:
                Pabs_flux = np.abs(Pabs_flux)
            # S4 returns \int |E|^2 / Area, so we need to multiply by the area
            # here. Factor of one over vacuum impedance to get the units into
            # power.
            if not per_area:
                Pabs_flux *= self.period**2
                Pabs_flux.ito(ureg.watt)
            else:
                Pabs_flux.ito(ureg.watt / ureg.meter**2)
            self.log.info("Layer: {}".format(layer_name))
            self.log.info("Flux Method Absorbed: {}".format(Pabs_flux))
            # Method 2: Go through integral of field intensity
            # if layer_name == 'Air':
            #     continue
            n_mat, k_mat = layer_obj.get_nk_matrix(freq, self.X, self.Y)
            start, end = layer_obj.get_inds(self.Z)
            integrand = n_mat*k_mat*Esq[start:end, :, :]
            z = self.Z[start:end]
            y_integral = integrate3d(integrand, z, self.X, self.Y)
            # 2\pi for conversion to angular frequency
            # epsilon_0 comes out of dielectric constant
            self.log.info("Pabs Pure Integral Result: {}".format(y_integral))
            Pabs_integ = Q_(2*np.pi, ureg.radians)*freq*ureg.vacuum_permittivity*y_integral
            if per_area:
                Pabs_integ /= self.period**2
                Pabs_integ.ito(ureg.watts / ureg.meter**2)
            else:
                Pabs_integ.ito(ureg.watts)
            self.log.info("Integrated Absorbed: {}".format(Pabs_integ))
            diff = np.abs((Pabs_flux - Pabs_integ)/Pabs_flux)
            absorb_arr[counter] = (layer_name, Pabs_flux, Pabs_integ, diff)
            counter += 1
        total_flux = np.sum(absorb_arr['flux_method'])
        total_integ = np.sum(absorb_arr['int_method'])
        total_diff = np.abs((total_flux - total_integ)/total_flux)
        new_rec = np.array(('total', total_flux, total_integ, total_diff),
                           dtype=absorb_arr.dtype)
        absorb_arr = np.append(absorb_arr, new_rec)
        self.log.info("Integration Order: %s, Layer absorption arr: %s",
                      str(order), str(absorb_arr))
        self.extend_data('power_absorbed', absorb_arr)
        return absorb_arr

    def transmissionData(self, port='Substrate_bottom'):
        """
        Computes reflection, transmission, and absorbance

        :param str port: Name of the location at which you would like to place
                         the transmission port (i.e where you would like to
                         compute transmission). This must correspond to one of
                         the keys placed in the fluxes dict located at
                         self.data['fluxes']
        """

        data = self.get_quantity('fluxes')
        # self.log.info('SORTED LAYERS: %s', str(sorted_layers))
        first_layer = list(self.layers.items())[0]
        self.log.info('FIRST LAYER: %s', str(first_layer))
        # An ordered dict is actually just a list of tuples so we can access
        # the key directly like so
        first_name = first_layer[0]
        self.log.info('FIRST LAYER NAME: %s', str(first_name))
        layers = [el.decode('utf-8') for el in data['layer']]
        first_ind = layers.index(first_name)
        port_ind = layers.index(port)
        top_data = data[first_ind]
        bot_data = data[port_ind]
        # p_inc = data[first_name][0]
        # p_ref = np.abs(data[first_name][1])
        # p_trans = data[last_name][0]
        p_inc = np.absolute(top_data['forward'])
        p_ref = np.absolute(top_data['backward'])
        p_trans = np.absolute(bot_data['forward'])
        reflectance = p_ref / p_inc
        transmission = p_trans / p_inc
        absorbance = 1 - reflectance - transmission
        self.log.info('Reflectance %f' % reflectance)
        self.log.info('Transmission %f' % transmission)
        self.log.info('Absorbance %f' % absorbance)
        assert(reflectance >= 0)
        assert(transmission >= 0)
        assert(absorbance >= 0)
        # Try to get a prexisting array if it exists
        try:
            # If array has been computed before get it
            arr = self.get_quantity('transmission_data')
            # If we already have data at this port update that row, otherwise
            # resize the array to accomodate the new row and insert the data
            ind = np.where(arr.port == port.encode('utf-8'))[0]
            if len(ind) > 0:
                arr[ind[0]] = (port, reflectance, transmission, absorbance)
            else:
                arr = np.resize(arr, arr.shape[0]+1)
                arr[-1] = (port, reflectance, transmission, absorbance)
        except:
            # Otherwise we need to make a new one
            dt = [('port', 'S25'), ('reflection', 'O'),
                  ('transmission', 'O'), ('absorption', 'O')]
            arr = np.recarray((1,), dtype=dt)
            arr[-1] = (port, reflectance, transmission, absorbance)
        self.extend_data('transmission_data', arr)
        return reflectance, transmission, absorbance

    def rescaling_factors(self):
        data = self.power_absorbed()
        dt = [(layer.decode(), np.float64) for layer in data['layer'] if layer
              != b'total']
        ratios = np.recarray((1,), dtype=dt)
        for record in data:
            layer = record['layer'].decode('utf-8')
            if layer == 'total':
                continue
            flux = record['flux_method'].magnitude
            integral = record['int_method'].magnitude
            try:
                if np.isfinite(flux) and np.isfinite(integral):
                    with np.errstate(all='raise'):
                        ratio = np.abs(flux/integral)
                        ratio = ratio
                else:
                    self.log.critical("FOUND NANS!!!")
                    print("FOUND NANS!!!")
                    ratio = 1
            except (ZeroDivisionError, FloatingPointError):
                self.log.warning("Integral method is zero, setting rescaling factor to 1")
                ratio = 1
            ratios[layer] = ratio
        self.log.info("Rescaling factors: {}".format(ratios))
        self.extend_data('rescaling_factors', ratios)
        return ratios

    def integrate_quantity(self, q, layer=None, mask=None, **kwargs):
        """
        Compute a 3D integral of a specified quantity


        Parameters
        ----------

        q : str
            A key in the self.data dict that specifies the quantity you wish to
            integrate
        mask : np.ndarray
            A numpy array of ones and zeros, which can be 2D or 3D. If a 3D
            array is provided, it will multiply the 3D array of the quantity
            elementwise before integration. The z-direction is along the first
            axis, i.e mask[z, x, y].  If a 2D array is provided, it will be
            extended along the z-direction and then will multiply the 3D array
            of the quantity elementwise before integration. This can be useful
            if you want to integrate only over a specific region of the device.
            You would supply a 3D mask that is 1 inside that region, and zero
            outside that region.  Combining with the layer arg is supported.
        layer : str
            The name of the layer you wish to integrate over.  Providing this
            will extract a 3D slice of data that is in the specified layer, and
            only integrate over that slice.  Use this if you do not want to
            include all layers in the integration. Combining with the mask arg
            is supported.


        Returns
        -------

        float
            Result of :math:`\\int quantity(x, y, z) dV`
        """

        if type(q) == str:
            qarr = self.get_quantity(q)
        else:
            qarr = q
        if layer is not None:
            l = self.layers[layer]
            z_vals = self.Z[l.get_slice(self.Z)]
            qarr = qarr[l.get_slice(self.Z)]
            if mask is not None and len(mask.shape) == 3:
                mask = mask[l.get_slice(self.Z)]
        else:
            z_vals = self.Z
        result = integrate3d(q, z_vals, self.X, self.Y, mask=mask, **kwargs)
        return result

    def photocurrent_density(self):
        """
        Computes this Simulation's contribution to the total photocurrent
        density.

        This simulation was run at a single frequency centered within a
        frequency bin of some finite, non-zero bandwidth. All powers and fields
        were scaled by the power/amplitude contained within this bin (i.e the
        integral of the chosen spectrum over the bin). The power absorbed by
        the device (i.e rate of energy absorption) can be converted to a rate
        of photon absorption (i.e number of photons absorbed per unit time) by
        dividing the absorbed power by the energy of incident photons.  If we
        assume that for every absorbed photon we extract exactly one electron,
        the rate of photon absorption can be converted to a photocurrent
        density by multiplying by the fundamental charge.

        The energy of the incident photons is

        ..math::

            E_{photon} = h f = \\hbar \\omega

             dividing the absorbed power

        where :math:`h` is Planck's constant, :math:`f` is the temporal
        frequency of the incident photon with units of Hertz, :math:`\\hbar =
        h/2\\pi`, and the angular frequency is :math:`\\omega = 2 \\pi f` which
        has units of radians/second. The input frequencies are temporal
        frequencies with units of Hertz.
        """

        self.log.info('Computing contribution to photocurrent density')
        freq = self.conf['Simulation/frequency']
        E_photon = ureg.planck_constant * freq
        # Unpack data for the port we passed in as an argument
        try:
            arr = self.get_quantity('power_absorbed')
            # port, ref, trans, absorb = arr[arr.port == port.encode('utf-8')][0]
        except KeyError:
            arr = self.power_absorbed()
        layers = [el['layer'].decode() for el in arr]
        total_ind = layers.index('total')
        totals = arr[total_ind]
        power_abs_flux = totals['flux_method']
        power_abs_integ = totals['int_method']
        # check if powers are per area or not. If not per area, there will be a
        # length squared in the dimensions
        per_area = '[length]' not in power_abs_flux.dimensionality
        flux_jph = ureg.elementary_charge * power_abs_flux / E_photon
        integ_jph = ureg.elementary_charge * power_abs_integ / E_photon
        if per_area:
            flux_jph.ito(ureg.milliampere / ureg.centimeter**2)
            integ_jph.ito(ureg.milliampere / ureg.centimeter**2)
        else:
            flux_jph.ito(ureg.milliampere)
            integ_jph.ito(ureg.milliampere)
        self.log.info('Flux Method Jph = {}'.format(flux_jph))
        self.log.info('Integral Method Jph = {}'.format(integ_jph))
        # Prevent numpy scalars from being stored inside the Quantity object
        flux_jph = Q_(float(flux_jph.magnitude), flux_jph.units)
        integ_jph = Q_(float(integ_jph.magnitude), integ_jph.units)
        self.extend_data('jph_flux_method', flux_jph)
        self.extend_data('jph_integral_method', integ_jph)
        return flux_jph, integ_jph

    def plot_q_values(self):
        """
        Create a 2D scatter plot containing the propagation constants q. RCWA
        assumes all z dependence is contained in a factor :math:`e^{i q_n z}`
        for a set of eigenvalues :math:`q_n` whose length is equal to the
        number of basis terms used in the simulations. The y axis plots the
        imaginary part of q, which quantifies the exponential decay rate of the
        waves as they propagate along z. The x axis plots the real part of q,
        which determines the oscillation frequency of the waves.
        """

        sim_freq = self.conf['Simulation/frequency']
        sim_wvlgth = 1e9*consts.c / sim_freq
        leg_str = ''
        for mat, matpath in self.conf['Materials'].items():
            n, k = get_nk(matpath, sim_freq)
            mat_wv = 1e-3*sim_wvlgth / n
            mat_q = 2*np.pi/mat_wv
            leg_str += '{}: {:.2f} [rads/$\mu$m]\n'.format(mat, mat_q)
        leg_str = leg_str[0:-1]
        for lname, l_obj in self.layers.items():
            qarr = self.get_quantity('{}_qvals'.format(lname))
            max_pos_freq = np.amax(qarr.real)
            max_neg_freq = np.amin(qarr.real)
            min_pos_wv = 1e3*2*np.pi/max_pos_freq
            if max_neg_freq == 0:
                min_neg_wv = 0
            else:
                min_neg_wv = 1e3*2*np.pi/max_neg_freq
            plt.figure()
            inc_q = 2*np.pi/(1e-3*sim_wvlgth)
            title = 'Layer: {}, Incident q: {:.2f} [rads/$\mu$m]'.format(lname, inc_q)
            # title += 'Min Positive $\\lambda$: {:.2f} nm, '
            # title += 'Min Negative $\\lambda$: {:.2f} nm'
            # title = title.format(lname, sim_wvlgth, min_pos_wv, min_neg_wv)
            # title = title.format(lname, sim_wvlgth)
            plt.title(title)
            # plt.scatter(1e3*2*np.pi/qarr.real, 1e4*qarr.imag/2*np.pi, c='b', s=.5,
            #             marker='o', label=leg_str)
            plt.scatter(qarr.real, qarr.imag/(2*np.pi), c='b', s=.75,
                        marker='o', label=leg_str)
            # pt = (qarr[0].real, qarr[0].imag)
            # theta = np.linspace(0, 1.48, 200)
            # plt.plot(pt[0]*np.cos(theta), pt[1]/np.cos(theta), 'r--')
            plt.legend(loc='best')
            # plt.annotate(leg_str, xy=(.95,.95), xycoords='axes fraction',
            #              size=14, ha='right', va='top',
            #              bbox=dict(boxstyle='round', fc='w'))
            plt.xlabel('Re(q) [radians/micron]')
            plt.ylabel('Im(q) [1/microns]')
            plot_path = osp.join(self.path, '{}_qvals.png'.format(lname))
            plt.grid(True)
            plt.savefig(plot_path)
            plt.close()

    def _draw_layer_circle(self, layer, shape, material, plane, pval, ax_hand):
        """Draws the circle within a layer"""
        center = shape.center
        cx = float(shape.center.x)
        cy = float(shape.center.y)
        radius = float(shape.radius)
        if plane == 'xy':
            circle = mpatches.Circle((center.x, center.y), radius=radius,
                                     fill=False)
            ax_hand.add_artist(circle)
        if plane in ["xz", "zx", "yz", "zy"]:
            plane_x = pval*self.dx.magnitude
            plane_to_center = np.abs(cx - plane_x)
            self.log.debug('DIST: {}'.format(plane_to_center))
            # Only draw if the observation plane actually intersects with the
            # circle
            if not plane_to_center >= radius:
                # Check if the plane intersects with the center of the circle
                if plane_to_center > plane_to_center:
                    intersect_angle = np.arccos(plane_to_center/radius)
                    self.log.debug('ANGLE: {}'.format(intersect_angle))
                    half_width = plane_to_center*np.tan(intersect_angle)
                else:
                    half_width = radius
                self.log.debug('HALF WIDTH: {}'.format(half_width))
                # Vertical lines should span height of the layer
                # z = [self.height - layer.start + .5, self.height - layer.end +.5]
                # z = [layer.start+.5, layer.end+.5]
                z = [layer.start.magnitude, layer.end.magnitude]
                # The upper edge
                x = [cy + half_width, cy + half_width]
                line = mlines.Line2D(x, z, linestyle='solid', linewidth=2.0,
                                     color='grey')
                ax_hand.add_line(line)
                # Now the lower edge
                x = [cy - half_width, cy - half_width]
                line = mlines.Line2D(x, z, linestyle='solid', linewidth=2.0,
                                     color='grey')
                ax_hand.add_line(line)
        return ax_hand

    def _draw_layer_geometry(self, layer, plane, pval, ax_hand):
        """Given a dictionary with the data containing the geometry for a
        layer, draw the internal geometry of the layer for a given plane type
        and plane value"""
        for sname, (shape, material) in layer.shapes.items():
            if isinstance(shape, Circle):
                ax = self._draw_layer_circle(layer, shape, material, plane,
                                             pval, ax_hand)
            else:
                self.log.warning('Drawing of shape {} not '
                                 'supported'.format(shape))
        return ax

    def draw_geometry_2d(self, plane, pval, ax_hand, skip_list=[]):
        """This function draws the layer boundaries and in-plane geometry on 2D
        heatmaps"""
        period = self.conf['Simulation/array_period']
        for lname, layer in self.layers.items():
            if plane in ["xz", "zx", "yz", "zy"]:
                # Get boundaries between layers and their starting and ending
                # indices
                if layer.thickness > Q_(0.0, layer.thickness.units):
                    if layer not in skip_list:
                        x = [0, period.magnitude]
                        y = [layer.end.magnitude, layer.end.magnitude]
                        # label_y = y[0] + 3*self.dz
                        # label_x = x[-1] - .01
                        # ax_hand.text(label_x, label_y, layer, ha='right',
                        #              family='sans-serif', size=16, color='grey')
                        line = mlines.Line2D(x, y, linestyle='solid',
                                             linewidth=2.0, color='grey')
                        ax_hand.add_line(line)
            # If we have some internal geometry for this layer, draw it
            if layer.shapes:
                ax_hand = self._draw_layer_geometry(layer, plane, pval, ax_hand)
        return ax_hand

    def heatmap2d(self, x, y, cs, labels, ptype, pval, save_path=None,
                  show=False, draw=False, fixed=None, colorsMap='viridis'):
        """A general utility method for plotting a 2D heat map"""
        # cs = np.flipud(cs)
        cm = plt.get_cmap(colorsMap)
        if np.iscomplexobj(cs):
            self.log.warning('Plotting only real part of %s in heatmap',
                             labels[2])
            cs = cs.real
        if fixed:
            cNorm = matplotlib.colors.Normalize(
                vmin=fixed[0], vmax=fixed[1])
        else:
            cNorm = matplotlib.colors.Normalize(
                vmin=np.amin(cs), vmax=np.amax(cs))
            # cNorm = matplotlib.colors.LogNorm(vmin=np.amin(cs)+.001, vmax=np.amax(cs))
            # cNorm = matplotlib.colors.LogNorm(vmin=1e13, vmax=np.amax(cs))
        scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cm)
        fig = plt.figure(figsize=(10, 8))
        ax = fig.add_subplot(111)
        # ax.imshow(cs,cmap=cm,norm=cNorm,extent=[x.min(),x.max(),y.min(),y.max()],aspect='auto')
        ax.invert_yaxis()
        ax.pcolormesh(x, y, cs, cmap=cm, norm=cNorm)
                      # extent=[x.min(),x.max(),y.min(),y.max()],aspect='auto')
        ax.grid(False)
        scalarMap.set_array(cs)
        # div = make_axes_locatable(ax)
        # zoom_ax = div.append_axes("right",size='100%', pad=.5)
        # zoom_ax.imshow(cs[75:100,:], extent=[x.min(), x.max(), .8, 1.4])
        # zoom_ax.grid(False)
        # cax = div.append_axes("right",size="100%",pad=.05)
        cb = fig.colorbar(scalarMap)
        cb.set_label(labels[2])
        ax.set_xlabel(labels[0])
        ax.set_ylabel(labels[1])
        if draw:
            self.log.info('Beginning geometry drawing routines ...')
            ax = self.draw_geometry_2d(ptype, pval, ax)
        if save_path:
            fig.savefig(save_path, bbox_inches='tight')
        if show:
            plt.show()
        plt.close(fig)

    def plane_2d(self, quantity, plane, pval, draw=False, fixed=None,
                 name_mod=''):
        """Plots a heatmap of a fixed 2D plane"""
        self.log.info('Plotting plane')
        pval = int(pval)
        # x = np.arange(0, self.period, self.dx)
        # y = np.arange(0, self.period, self.dy)
        # z = np.arange(0, self.height + self.dz, self.dz)
        x = self.X.magnitude
        y = self.Y.magnitude
        z = self.Z.magnitude
        # Get the scalar values
        freq = self.conf['Simulation/frequency']
        wvlgth = (consts.c / freq) * 1E9
        title = 'Frequency = {:.4E} Hz, Wavelength = {:.2f} nm'.format(
            freq, wvlgth)
        # Get the plane we wish to plot
        quant_arr = self.get_quantity(quantity)
        cs = get_plane(quant_arr, plane, pval).magnitude
        self.log.info('DATA SHAPE: %s' % str(cs.shape))
        show = self.conf['General']['show_plots']
        p = False
        if plane == 'yz' or plane == 'zy':
            labels = (r'$y [{:~L}]$'.format(self.Y.units),
                      r'$z [{:~L}]$'.format(self.Z.units),
                      quantity,
                      title)
            if self.conf['General']['save_plots']:
                fname = '%s_plane_2d_yz_pval%s' % (quantity, str(pval))
                fname += '{}.png'.format(name_mod)
                p = osp.join(self.path, fname)
            self.heatmap2d(y, z, cs, labels, plane, pval,
                           save_path=p, show=show, draw=draw, fixed=fixed)
        elif plane == 'xz' or plane == 'zx':
            labels = (r'$x [{:~L}]$'.format(self.X.units),
                      r'$z [{:~L}]$'.format(self.Z.units),
                      quantity,
                      title)
            if self.conf['General']['save_plots']:
                fname = '%s_plane_2d_xz_pval%s' % (quantity, str(pval))
                fname += '{}.png'.format(name_mod)
                p = osp.join(self.path, fname)
            self.heatmap2d(x, z, cs, labels, plane, pval,
                           save_path=p, show=show, draw=draw, fixed=fixed)
        elif plane == 'xy' or plane == 'yx':
            labels = (r'$y [{:~L}]$'.format(self.Y.units),
                      r'$x [{:~L}]$'.format(self.X.units),
                      quantity,
                      title)
            if self.conf['General']['save_plots']:
                fname = '%s_plane_2d_xy_pval%s' % (quantity, str(pval))
                fname += '{}.png'.format(name_mod)
                p = osp.join(self.path, fname)
                p = osp.join(self.path, fname)
            self.heatmap2d(x, y, cs, labels, plane, pval,
                           save_path=p, show=show, draw=draw, fixed=fixed)

    def scatter3d(self, x, y, z, cs, labels, ptype, colorsMap='jet'):
        """A general utility method for scatter plots in 3D"""
        cm = plt.get_cmap(colorsMap)
        cNorm = matplotlib.colors.Normalize(vmin=min(cs), vmax=max(cs))
        scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cm)
        fig = plt.figure(figsize=(9, 7))

        ax = fig.add_subplot(111, projection='3d')
        ax.scatter(x, y, z, c=scalarMap.to_rgba(cs), edgecolor='none')
        scalarMap.set_array(cs)
        cb = fig.colorbar(scalarMap)
        cb.set_label(labels[3])
        ax.set_xlabel(labels[0])
        ax.set_ylabel(labels[1])
        ax.set_zlabel(labels[2])
        fig.suptitle(osp.basename(self.conf['General']['sim_dir']))
        if self.conf['General']['save_plots']:
            name = labels[-1] + '_' + ptype + '.png'
            path = osp.join(self.conf['General']['sim_dir'], name)
            fig.savefig(path)
        if self.conf['General']['show_plots']:
            plt.show()
        plt.close(fig)

    def full_3d(self, quantity):
        """Generates a full 3D plot of a specified scalar quantity"""
        # The data just tells you what integer grid point you are on. Not what actual x,y coordinate you
        # are at
        x = np.arange(0, self.period, self.dx)
        y = np.arange(0, self.period, self.dy)
        z = np.arange(0, self.height + self.dz, self.dz)
        points = np.array(list(product(z, x, y)))
        # Get the scalar
        scalar = self.get_quantity(quantity)
        labels = ('X [um]', 'Y [um]', 'Z [um]', quantity)
        # Now plot!
        self.scatter3d(points[:, 1], points[:, 2], points[
                       :, 0], scalar.flatten(), labels, 'full_3d')

    def planes_3d(self, quantity, xplane, yplane):
        """Plots some scalar quantity in 3D but only along specified x-z and y-z planes"""
        xplane = int(xplane)
        yplane = int(yplane)
        # Get the scalar values
        # Get the data on the plane with a fixed x value. These means we'll
        # have changing (y, z) points
        quant_arr = self.get_quantity(quantity)
        xdata = get_plane(quant_arr, 'yz', xplane)
        # z first cuz we want y to be changing before z to correspond with the
        # way numpy flattens arrays. Note this means y points will be in the
        # 2nd column
        xplanepoints = np.array(list(product(self.Z, self.Y)))
        xdata = xdata.flatten()
        xplanexval = np.array(list(repeat(x[xplane], len(xdata))))
        xplanedata = np.zeros((xplanepoints.shape[0], 4))
        xplanedata[:, 0] = xplanexval
        xplanedata[:, 1] = xplanepoints[:, 1]
        xplanedata[:, 2] = xplanepoints[:, 0]
        xplanedata[:, 3] = xdata
        # Same procedure for fixed y plane
        quant_arr = self.get_quantity(quantity)
        ydata = get_plane(quant_arr, 'xz', yplane)
        yplanepoints = np.array(list(product(z, x)))
        ydata = ydata.flatten()
        yplaneyval = np.array(list(repeat(y[yplane], len(ydata))))
        yplanedata = np.zeros((yplanepoints.shape[0], 4))
        yplanedata[:, 0] = yplanepoints[:, 1]
        yplanedata[:, 1] = yplaneyval
        yplanedata[:, 2] = yplanepoints[:, 0]
        yplanedata[:, 3] = ydata
        labels = ('X [um]', 'Y [um]', 'Z [um]', quantity)
        # Now stack them vertically and plot!
        all_data = np.vstack((xplanedata, yplanedata))
        self.scatter3d(all_data[:, 0], all_data[:, 1], all_data[:, 2],
                       all_data[:, 3], labels, 'planes_3d')

    def line_plot(self, x, y, labels, ax=None):
        """Make a simple line plot and return the figure and axes handles"""
        if ax is None:
            fig, ax = plt.subplots()
        else:
            fig = None
        ax.plot(x, y, '--o', label=labels[0])
        ax.set_xlabel(labels[1])
        ax.set_ylabel(labels[2])
        ax.set_title(labels[3])
        return fig, ax

    def fixed_line(self, quantity, direction, coord1, coord2):
        """
        Plot a scalar quantity on a line along a given direction at some pair
        of coordinates in the plane perpendicular to that direction. The
        remaining coordinates are specified in x, y, z order. So for example if
        direction='z' then coord1 corresponds to x and coord2 corresponds to y.
        If direction='y' then coord1 corresponds to x and coord2 corresponds to
        z.
        :param str direction: The direction along which to plot the line. Must
        be one of 'x', 'y', or 'z'.
        :param str direction: The direction along which you wish to plot a
        line. Must be one of 'x', 'y', or 'z'. The other two coordinates remain
        fixed and are specified by coord1 and coord2.
        :param int coord1: The integer index for the first fixed coordinate.
        Indexes are in x,y, z order so if line_dir='z' then c1 corresponds to x
        :param int coord2: The integer index for the second coordinate.
        :param str quantity: The quantity whose data array you wish to take a
        line cut through
        """

        coord1 = int(coord1)
        coord2 = int(coord2)
        # Get the scalar values
        # Filter out any undesired data that isn't on the planes
        quant_arr = self.get_quantity(quantity)
        data = get_line(quant_arr, direction, coord1, coord2)
        if direction == 'x':
            # z along rows, y along columns
            pos_data = self.X
        elif direction == 'y':
            # x along columns, z along rows
            pos_data = self.Y
        elif direction == 'z':
            # x along rows, y along columns
            pos_data = self.Z
        freq = self.conf['Simulation/frequency']
        wvlgth = (consts.c / freq) * 1E9
        title = 'Frequency = {:.4E} Hz, Wavelength = {:.2f} nm'.format(
            freq, wvlgth)
        ptype = "%s_line_plot_%i_%i" % (direction, coord1, coord2)
        if np.iscomplexobj(data):
            labels = ('Real Part', 'Z [um]', quantity, title)
            fig, ax = self.line_plot(pos_data, data.real, labels)
            labels = ('Imag Part', 'Z [um]', quantity, title)
            _, ax = self.line_plot(pos_data, data.imag, labels, ax=ax)
        else:
            labels = (None, 'Z [um]', quantity, title)
            fig, ax = self.line_plot(pos_data, data, labels)
        ax.legend()
        if self.conf['General']['save_plots']:
            name = labels[2] + '_' + ptype + '.png'
            path = osp.join(self.path, name)
            fig.savefig(path)
        if self.conf['General']['show_plots']:
            plt.show()
        plt.close(fig)

class SimulationGroup:

    """
    A group of simulations. Takes a list of Simulation objects as an argument.
    This class is not responsible for grouping Simulation objects by some
    criteria, it just expects a list of already grouped Simulation objects.

    Provides methods for calculating things on a group of simulations. The
    results of these methods might not be sensible for some groupings. For
    example, calculating the convergence of a group that is group against
    frequency doesn't make any sense. Similiarly, calculating Jsc of a group
    that is grouped against number of basis terms also doesn't make sense.
    """

    def __init__(self, base_dir, bandwidths, sims=None, sim_confs_and_dirs=None,
                 grouped_against=None, grouped_by=None):
        if sims is None and sim_confs_and_dirs is None:
            raise ValueError("Must provide a list of Simulation objects or a "
                             "list of tuples containing (Config, sim_dir)")
        if grouped_by is None and grouped_against is None:
            raise ValueError("Must know how this SimulationGroup is grouped")
        self.log = logging.getLogger(__name__)
        self.base_dir = base_dir
        self.grouped_by = grouped_by
        self.grouped_against = grouped_against
        if sims is not None:
            self.sims = sims
            self.sim_confs = [sim.conf for sim in self.sims]
        else:
            self.sim_confs = [tup[0] for tup in sim_confs_and_dirs]
            self.sims = [Simulation(outdir, bandwidths[conf.ID], conf=conf) for
                         conf, outdir in sim_confs_and_dirs]
        self.ID, self.results_dir = self.get_group_info()
        self.num_sims = len(self.sims)
        fpath = osp.join(self.results_dir, 'data.hdf5')
        self.data = HDF5DataManager(fpath, group_path='/', mode='a', logger=self.log)

    def get_group_info(self):
        all_ids = ''.join(conf.ID for conf in self.sim_confs)
        hasher = hashlib.md5()
        hasher.update(all_ids.encode('utf-8'))
        GID = hasher.hexdigest()
        if self.grouped_by is not None:
            self.log.info('Simulations grouped by: %s', self.grouped_by)
            dname = self.grouped_by.replace('/', '_')
            results_dir = osp.join(self.base_dir, 'grouped_by', dname, GID[0:10])
        elif self.grouped_against is not None:
            self.log.info('Simulations grouped against: %s',
                          self.grouped_against)
            dname = self.grouped_against.replace('/', '_')
            results_dir = osp.join(self.base_dir, 'grouped_against', dname, GID[0:10])
        self.log.info("Group results directory: %s", results_dir)
        if not os.path.isdir(results_dir):
            os.makedirs(results_dir)
        return GID, results_dir

    def save_group_info(self):
        fname = osp.join(self.results_dir, 'sims_in_group.txt')
        # TODO: Add list saving to HDF5DataManager so this works
        # sim_ids = []
        with open(fname, 'w') as f:
            for sim in self.sims:
                f.write('{}\n'.format(sim.ID))
                # sim_ids.append(sim.ID)
        # self.data['sim_ids'] = sim_ids

    def write_data(self, blacklist=(), write_sims=False):
        """
        Writes the data. This is a simple wrapper around the write_data()
        method of the DataManager object, with some code to compute the time it
        took to perform the write operation
        """
        self.data.write_data(blacklist=blacklist)
        if write_sims:
            for sim in self.sims:
                self.log.info("Writing data for Simulation %s", sim.ID)
                sim.write_data()

    def clear_data(self, clear_sims=True):
        """
        Clears all the data from the self.data DataManager object to free up memory
        """
        self.log.info("Clearing data for SimulationGroup %s", self.ID)
        if isinstance(self.data, DataManager):
            self.data.clear_data()
        else:
            self.data = {}
        if clear_sims:
            for sim in self.sims:
                self.log.info("Clearing data for Simulation %s", sim.ID)
                sim.clear_data()

    def close(self):
        self.log.info("Closing data for SimulationGroup %s", self.ID)
        self.data.close()
        for sim in self.sims:
            self.log.info("Closing data for Simulation %s", sim.ID)
            sim.close()

    def scalar_reduce(self, quantity, avg=False, force=False):
        """
        Combine a scalar quantity across all simulations in each group. If
        avg=False then a direct sum is computed, otherwise an average is
        computed
        """

        self.log.info('Performing scalar reduction for group at %s',
                      self.results_dir)
        print("REDUCING {}".format(quantity))
        if self.sims[0].use_rescale_method:
            key = 'scalar_reduce_{}_rescale'.format(quantity)
        else:
            key = 'scalar_reduce_{}'.format(quantity)
        self.log.debug('Quantity to reduce: %s', quantity)
        try:
            if force:
                raise KeyError
            else:
                return self.data[key]
        except KeyError:
            print("KEY NOT PRESENT IN SCALAR REDUCE")
            group_comb = self.sims[0].get_quantity(quantity)
            # if np.isnan(group_comb).any():
            #     print("Sim {} has NANs!".format(self.sims[0].ID))
            # self.sims[0].clear_data()
            for sim in self.sims[1:]:
                print("-"*25)
                print("Sim ID: {}".format(sim.ID))
                self.log.debug(sim.ID)
                quant = sim.get_quantity(quantity)
                self.log.debug(quant.dtype)
                if np.isnan(quant).any():
                    print("Sim {} has NANs!".format(sim.ID))
                group_comb += quant
                sim.clear_data()
            if avg:
                group_comb = group_comb / self.num_sims
                fname = 'scalar_reduce_avg_%s' % quantity
            else:
                fname = 'scalar_reduce_%s' % quantity
            if self.sims[0].use_rescale_method:
                fname += '_rescaled'
            ftype = self.sims[0].conf['General']['save_as']
            if ftype == 'hdf5':
                self.data[fname] = group_comb
                self.data['xcoords'] = self.sims[0].data['xcoords']
                self.data['ycoords'] = self.sims[0].data['ycoords']
                self.data['zcoords'] = self.sims[0].data['zcoords']
                if 'radial_coords' in self.sims[0].data:
                    self.data['radial_coords'] = self.sims[0].data['radial_coords']
                if 'angle_coords' in self.sims[0].data:
                    self.data['angle_coords'] = self.sims[0].data['angle_coords']
            else:
                raise ValueError('Invalid file type in config')
            return group_comb

    def photocurrent_density(self, method='flux', units='mA/cm^2'):
        """
        Computes photocurrent density assuming perfect carrier collection,
        meaning every absorbed photon gets converted to 1 collected electron.

        The incident power is computed using
        :py:func:`get_incident_power`.  See that function for
        details about how the incident power is computed.

        Given some number of frequency values N (and simulations at those
        frequencies), the photocurrent density can be computed using

        .. math:: J_{ph} = \\int q \\sum_i^N \\frac{P(f_i)A(f_i)}{E_{photon}}
           :label: Jph

        where

        * :math:`P(f_i)` is the incident power at frequency i
        * :math:`A(f_i)` is the fraction of the incident power that is absorbed
        * :math:`E_{photon} = h f_i` is the energy of the incident photons at
          frequency i.
        * :math:`q` is the fundamental charge

        :param port: The location at which to set the transmission port
        :type port: str
        :param method: The method used to compute the absorbed power. One of
                       either 'flux' or 'integral'
        :type method: str
        :return: The photocurrent density, see :eq:`Jph`
        :rtype: float
        :raises ValueError: if the ``method`` kwarg is not 'flux' or
                            'integral'.  This is a bit of a hack at the moment
                            for testing purposes.
        """

        if method not in ('flux', 'integral'):
            msg = 'Invalid method {} for computing photocurrent density'.format(method)
            raise ValueError(msg)

        if not self.grouped_against == 'Simulation/frequency':
            raise ValueError('Can only compute photocurrent density when '
                             'grouped against frequency')

        base = self.results_dir
        self.log.info('Computing photocurrent density for group at %s', base)
        jph_vals = Q_(np.zeros(self.num_sims), units)
        freqs = Q_(np.zeros(self.num_sims), 'hertz')
        # for i, sim in enumerate(self.sims):
        #     freqs[i] = freq
        # Assuming the sims have been grouped by frequency, sum over all of
        # them
        for i, sim in enumerate(self.sims):
            # freq = sim.conf['Simulation']['params']['frequency']
            freq = sim.conf['Simulation/frequency']
            freqs[i] = freq
            E_photon = ureg.planck_constant * freq
            try:
                abs_arr = sim.data['power_absorbed']
            except KeyError:
                abs_arr = sim.power_absorbed()
            if method == 'flux':
                # arr = sim.data['transmission_data']
                # _, ref, trans, absorb = arr[arr.port == port.encode('utf-8')][0]
                # incident_power = get_incident_power(sim)
                # jph_vals[i] = incident_power * absorb / E_photon
                key = 'flux_method'
            else:
                key = 'int_method'
            row = get_record(abs_arr, 'layer', 'total')[0]
            absorbed_power = row[key]
            jph_vals[i] = ureg.elementary_charge * absorbed_power / E_photon
            sim.clear_data()
        # factor of 1/10 to convert A*m^-2 to mA*cm^-2
        Jph = np.sum(jph_vals)
        outf = osp.join(base, 'jph_{}.dat'.format(method))
        with open(outf, 'w') as out:
            out.write('%f\n' % Jph.magnitude)
        self.log.info('Jph = {}'.format(Jph))
        return Jph


    def convergence(self, quantity, err_type='global', scale='linear'):
        """Plots the convergence of a field across all available simulations"""

        self.log.info('Plotting convergence')
        base = self.sims[0].conf['General']['base_dir']
        if err_type == 'local':
            fglob = osp.join(base, 'localerror_%s*.dat' % quantity)
        elif err_type == 'global':
            fglob = osp.join(base, 'globalerror_%s*.dat' % quantity)
        elif err_type == 'adjacent':
            fglob = osp.join(base, 'adjacenterror_%s*.dat' % quantity)
        else:
            self.log.error('Attempting to plot an unsupported error type')
            raise ValueError
        paths = glob.glob(fglob)
        for path in paths:
            labels = []
            errors = []
            with open(path, 'r') as datf:
                for line in datf.readlines():
                    lab, err = line.split(',')
                    labels.append(lab)
                    errors.append(err)
            fig = plt.figure(figsize=(9, 7))
            plt.ylabel('M.S.E of %s' % quantity)
            plt.xlabel('Number of Fourier Terms')
            plt.plot(labels, errors)
            plt.yscale(scale)
            # plt.xticks(x,labels,rotation='vertical')
            plt.tight_layout()
            plt.title(osp.basename(base))
            if self.gconf['General']['save_plots']:
                if '_excluded' in path:
                    excluded = '_excluded'
                else:
                    excluded = ''
                name = '%s_%sconvergence_%s%s.png' % (
                    osp.basename(base), err_type, quantity, excluded)
                path = osp.join(base, name)
                fig.savefig(path)
            if self.gconf['General']['show_plots']:
                plt.show()
            plt.close(fig)

    def plot_power_absorbed(self, input_ax=None, plot_layer=None, quant=None,
                            sim_slice=(0, None), marker='--o', xlabel='',
                            ylabel='', title='', leg_label=''):
        """
        Plots absorption per layer and error
        """

        if self.grouped_against is None:
            raise ValueError("Can only call plot_power_absorbed when "
                             "sims are grouped against a parameter")
        results_dict = {}
        units = self.sims[0].conf[self.grouped_against].units
        xvals = Q_(np.zeros(len(self.sims[sim_slice[0]:sim_slice[1]])),
                   units)
        for i, sim in enumerate(self.sims[sim_slice[0]:sim_slice[1]]):
            xvals[i] = sim.conf[self.grouped_against]
            abs_arr = sim.data['power_absorbed']
            flux_total = 0
            integ_total = 0
            for rec in abs_arr:
                layer = rec['layer'].decode('utf-8')
                flux = rec['flux_method'].magnitude
                integ = rec['int_method'].magnitude
                diff = rec['difference'].magnitude
                if layer in results_dict:
                    results_dict[layer][0].append(flux.real)
                    results_dict[layer][1].append(integ.real)
                    results_dict[layer][2].append(diff.real)
                else:
                    results_dict[layer] = [[flux.real], [integ.real], [diff.real]]
                flux_total += flux.real
                integ_total += integ.real
            # total_diff = np.abs(flux_total.real - integ_total.real)/np.abs(flux_total)
            # total_diff = np.real(flux_total.real - integ_total.real)/flux_total.real
            # total_diff_polar = np.real(flux_total.real - integ_polar_total.real)/flux_total.real
            # if 'total' in results_dict:
            #     results_dict['total'][0].append(flux_total.real)
            #     results_dict['total'][1].append(integ_total.real)
            #     results_dict['total'][2].append(total_diff.real)
            # else:
            #     results_dict['total'] = [[flux_total.real], [integ_total.real],
            #                              [total_diff.real]]
        layers = results_dict.keys()
        if plot_layer is not None:
            layers = [l for l in layers if l == plot_layer]

        quants = {'fluxmethod_absorption': 0,
                  'integralmethod_absorption': 1,
                  'relativediff': 2}
        if quant is not None:
            quants = {q: ind for q, ind in quants.items() if q in quant}
        xvals.ito('nm', 'spectroscopy')
        for layer in layers:
            results = results_dict[layer]
            for name, ind in quants.items():
                # diff = abs(results[ind][-1] - results[ind][0])/abs(results[ind][-1])
                # diff = abs(results[ind][-1] - results[ind][0])/abs(results[ind][-1])
                if input_ax is None:
                    fig, ax = plt.subplots()
                else:
                    ax = input_ax
                pfile = osp.join(self.results_dir, '{}_{}.png'.format(layer, name))
                ax.plot(xvals.magnitude, results[ind], marker, label=leg_label)
                ylims = ax.get_ylim()
                ax.set_ylim([0, ylims[-1]])
                ax.set_xlabel(self.grouped_against[-1])
                if ylabel:
                    ax.set_ylabel(ylabel)
                if xlabel:
                    ax.set_xlabel(xlabel)
                if title:
                    ax.set_title(title)
                if input_ax is None:
                    plt.savefig(pfile)
        return ax, diff

    def plot_scalar_reduce(self, quantity, plane, pval, draw=False, fixed=None):
        """Plot the result of a particular scalar reduction for each group"""

        sim = self.sims[0]
        base = osp.expandvars(sim.conf['General']['results_dir'])
        self.log.info('Plotting scalar reduction of %s for quantity %s' % (base, quantity))
        cm = plt.get_cmap('jet')
        max_depth = sim.conf['General']['max_depth']
        period = sim.conf['Simulation']['params']['array_period']
        x = np.arange(0, period, sim.dx)
        y = np.arange(0, period, sim.dy)
        z = np.arange(0, max_depth + sim.dz, sim.dz)
        ftype = sim.conf['General']['save_as']
        if ftype == 'npz':
            globstr = osp.join(base, 'scalar_reduce*_%s.npy' % quantity)
            files = glob.glob(globstr)
        elif ftype == 'hdf5':
            self.log.warning('FIX LOAD IN GLOBAL SCALAR REDUCE')
            globstr = osp.join(base, 'scalar_reduce*_%s.npy' % quantity)
            files = glob.glob(globstr)
        else:
            raise ValueError('Incorrect file type in config')
        title = 'Reduction of %s' % quantity
        for datfile in files:
            p = False
            if ftype == 'npz':
                scalar = np.load(datfile)
            elif ftype == 'hdf5':
                self.log.warning('FIX LOAD IN GLOBAL SCALAR REDUCE')
                scalar = np.load(datfile)
            else:
                raise ValueError('Incorrect file type in config')

            cs = self.get_plane(scalar, plane, pval)
            if plane == 'yz' or plane == 'zy':
                labels = ('y [um]', 'z [um]', quantity, title)
                if sim.conf['General']['save_plots']:
                    fname = 'scalar_reduce_%s_plane_2d_yz.png' % quantity
                    p = osp.join(base, fname)
                show = sim.conf['General']['show_plots']
                self.sims[0].heatmap2d(y, z, cs, labels, plane, pval,
                               save_path=p, show=show, draw=draw, fixed=fixed)
            elif plane == 'xz' or plane == 'zx':
                labels = ('x [um]', 'z [um]', quantity, title)
                if sim.conf['General']['save_plots']:
                    fname = 'scalar_reduce_%s_plane_2d_xz.png' % quantity
                    p = osp.join(base, fname)
                show = sim.conf['General']['show_plots']
                self.sims[0].heatmap2d(sim, x, z, cs, labels, plane, pval,
                               save_path=p, show=show, draw=draw, fixed=fixed)
            elif plane == 'xy' or plane == 'yx':
                labels = ('y [um]', 'x [um]', quantity, title)
                if sim.conf['General']['save_plots']:
                    fname = 'scalar_reduce_%s_plane_2d_xy.png' % quantity
                    p = osp.join(base, fname)
                self.sims[0].heatmap2d(sim, x, y, cs, labels, plane, pval,
                               save_path=p, show=show, draw=draw, fixed=fixed)

    def get_transmission_data(self, port='Substrate_bottom', method='flux'):
        if not self.grouped_against == 'Simulation/frequency':
            raise ValueError('Can only retrieve transmission data when '
                             'grouped against frequency')
        if method not in {'flux', 'integral'}:
            raise ValueError("Invalid method {}. Must be one of ['flux', "
                             "'integral']")
        base = self.results_dir
        self.log.info('Retrieving transmission data for group at %s' % base)
        # Assuming the leaves contain frequency values, sum over all of them
        freqs = Q_(np.zeros(self.num_sims), 'Hz')
        refl_l = np.zeros(self.num_sims)
        trans_l = np.zeros(self.num_sims)
        absorb_l = np.zeros(self.num_sims)
        if method == 'flux':
            for i, sim in enumerate(self.sims):
                # Unpack data for the port we passed in as an argument
                tdata = sim.data['transmission_data']
                row = get_record(tdata, 'port', port)[0]
                ref = row['reflection']
                trans = row['transmission']
                absorb = row['absorption']
                freq = sim.conf['Simulation/frequency']
                freqs[i] = freq
                trans_l[i] = trans
                refl_l[i] = ref
                absorb_l[i] = absorb
        else:
            self.log.warning("Ignoring port argument, computing total "
                             "absorbance")
            self.log.warning("Not computing reflectance or transmittance")
            for i, sim in enumerate(self.sims):
                freq = sim.conf['Simulation/frequency']
                angle = sim.conf['Simulation/polar_angle']
                Pin = get_incident_power('am1.5g', freq, angle, sim.bandwidth)
                absorb_rec = get_record(sim.data['power_absorbed'], 'layer', 'total')[0]
                Pabs = absorb_rec['int_method']
                A = Pabs/Pin
                freqs[i] = freq
                absorb_l[i] = A.magnitude
        return freqs, refl_l, trans_l, absorb_l

    def plot_transmission_data(self, absorbance, reflectance, transmission,
                               port='Substrate_bottom'):
        """
        Plot transmissions, absorption, and reflectance assuming leaves are
        frequency
        """

        base = self.results_dir
        self.log.info('Plotting transmission data for group at %s' % base)
        # Assuming the leaves contain frequency values, sum over all of them
        freqs, refl_l, trans_l, absorb_l, self.get_transmission_data(port=port)
        wvs = freqs.to('nm', 'spectroscopy')
        refl_l = refl_l[::-1]
        absorb_l = absorb_l[::-1]
        trans_l = trans_l[::-1]
        plt.figure()
        if absorbance:
            self.log.info('Plotting absorbance')
            plt.plot(freqs, absorb_l, '-o', label='Absorption')
        if reflectance:
            plt.plot(freqs, refl_l, '-o', label='Reflection')
        if transmission:
            plt.plot(freqs, trans_l, '-o', label='Transmission')
        plt.legend(loc='best')
        figp = osp.join(base, 'transmission_plots_port%s.png'%port)
        plt.xlabel('Wavelength (nm)')
        plt.ylim((0, 1.0))
        plt.savefig(figp)
        plt.close()

    def plot_q_values(self):
        """
        Plot the q values for different numbers of basis terms on the same axis
        """
        if 'numbasis' not in self.grouped_against:
            raise ValueError("""Simulations must be grouped against number of
            basis terms to plot q values on the same axis""")

        layers = self.sims[0].layers
        base = self.sims[0].conf['General']['results_dir']
        freq = self.sims[0].conf[('Simulation', 'params', 'frequency')]
        wvlgth = 1e9*consts.c/freq
        for lname, l_obj in layers.items():
            plt.figure()
            title = 'Layer: {}, Freq = {:.3E}, Wavelength = {:.2f} nm'
            plt.title(title.format(lname, freq, wvlgth))
            for sim in self.sims:
                qarr = sim.data['{}_qvals'.format(lname)]
                label = 'Numbasis: {}'
                label = label.format(sim.conf[('Simulation','params','numbasis')])
                plt.scatter(qarr.real, qarr.imag/(2*np.pi), s=.75, label=label)
                # pt = (qarr[0].real, qarr[0].imag)
                # theta = np.linspace(0, 1.48, 200)
                # plt.plot(pt[0]*np.cos(theta), pt[1]/np.cos(theta), 'r--')
                # plt.annotate(leg_str, xy=(.95,.95), xycoords='axes fraction',
                #              size=14, ha='right', va='top',
                #              bbox=dict(boxstyle='round', fc='w'))
            plt.xlabel('Re(q) [radians/micron]')
            plt.ylabel('Im(q) [1/microns]')
            plot_path = osp.join(base, '{}_qvals.png'.format(lname))
            plt.grid(True)
            plt.legend(loc='best')
            plt.savefig(plot_path)
            plt.close()


def scatter3d(x, y, z, cs=None, colorsMap='jet'):
    fig = plt.figure()
    ax = Axes3D(fig)
    if cs is not None:
        cm = plt.get_cmap(colorsMap)
        cNorm = matplotlib.colors.Normalize(vmin=min(cs), vmax=max(cs))
        scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cm)
        ax.scatter(x, y, z, c=scalarMap.to_rgba(cs))
        scalarMap.set_array(cs)
        fig.colorbar(scalarMap)
    else:
        ax.scatter(x, y, z)
    return fig, ax


@ureg.wraps('=A*B', ('=A', '=B'))
def integrate1d(arr, xvals, meth=intg.trapz):
    x_integral = meth(arr, x=xvals, axis=0)
    return x_integral


# @ureg.wraps('=A*B**2', ('=A', '=B', '=B'))
def integrate2d(qarr, qax0, qax1, meth=intg.trapz):
    arr = qarr.magnitude
    ax0 = qax0.magnitude
    ax1 = qax1.magnitude
    x_integral = meth(arr, x=ax0, axis=0)
    y_integral = meth(x_integral, x=ax1, axis=0)
    units = qarr.units*qax0.units*qax1.units
    return Q_(y_integral, units)


# @ureg.wraps('=A*B**3', ('=A', '=B', '=B', '=B'))
def integrate3d(qarr, qax0, qax1, qax2, order=(0, 1, 2), mask=None,
                method=intg.trapz):
    """
    Perform numerical integration on a 3D array `arr` whose spatial coordinates
    along each axis correspond to the arrays `ax0`, `ax1`, and `ax2`.

    The `order` kwarg specifies the integration order of the axes. For example,
    if order=(1, 0, 2) the array is integrated along axis 1, followed by axis
    0, then axis 2. This is useful for determining if the integration order
    matters, which it sometimes can numerically.

    Parameters
    ----------

    arr : np.ndarray
        The 3D numpy array to be integrated. Must have shape
        (len(ax0), len(ax1), len(ax2))
    ax0, ax1, ax2 : np.ndarray
        1D numpy arrays specifying the spatial coordinates of each axis
    method : function
        The integration method to use.
    mask : np.ndarray
        A numpy array of ones and zeros, which can be 2D or 3D. If a 3D array
        is provided, it will multiply the 3D array of the quantity elementwise
        before integration. The z-direction is along the first axis, i.e
        mask[z, x, y].  If a 2D array is provided, it will be extended along
        the z-direction and then will multiply the 3D array of the quantity
        elementwise before integration. This can be useful if you want to
        integrate only over a specific region of the device. You would supply a
        3D mask that is 1 inside that region, and zero outside that region.
        Combining with the layer arg is supported.
    order : tuple
        A length 3 tuple specifying the order in which to integrate.

    Returns
    -------

    float
        The result of the integral
    """
    arr = qarr.magnitude
    ax0 = qax0.magnitude
    ax1 = qax1.magnitude
    ax2 = qax2.magnitude
    coords = (ax0, ax1, ax2)
    if mask is not None:
        arr = arr*mask
    result = arr
    remaining_axes = [0, 1, 2]
    for ax in order:
        int_ax = remaining_axes.index(ax)
        result = method(result, x=coords[ax], axis=int_ax)
        remaining_axes.pop(int_ax)
    units = qarr.units*qax0.units*qax1.units*qax2.units
    return Q_(result, units)

def get_plane(arr, plane, pval):
    """
    Gets data along a 2D plane/slice through the 3D data array for a given
    quantity

    Parameters
    ----------

    plane : str
        Any of 'xy', 'yz', or 'xz'. Determines the plane along which the
        slice is taken
    pval : int
        The index along the final unspecified direction. If plane='xy' then
        index would index along the z direction.

    quantity : str
        The quantity whose data array you wish to take a line cut through

    Returns
    -------

    np.ndarray
        A 2D numpy array containing data on the specified plane
    """

    if plane == 'yz' or plane == 'zy':
        # z along rows, y along columns
        return arr[:, pval, :]
    elif plane == 'xz' or plane == 'zx':
        # x along columns, z along rows
        return arr[:, :, pval]
    elif plane == 'xy' or plane == 'yx':
        # x along rows, y along columns
        return arr[pval, :, :]


def get_line(arr, line_dir, c1, c2):
    """
    Gets data along a line through the 3D data array for the given quantity
    along a given direction

    Parameters
    ----------
    line_dir : str
        Any of 'x', 'y', or 'z'. Determines the direction along which the
        line cut is taken, the other two coordinates remain fixed and are
        specified by c1 and c2.
    c1 : int
        The integer index for the first fixed coordinate.  Indexes are in
        x,y,z order so if line_dir='z' then c1 corresponds to x
    c2 : int
        The integer index for the second coordinate.
    quantity : str
        The quantity whose data array you wish to take a line cut through

    Returns
    -------

    np.ndarray
        A 1D numpy array containing data along the specified line cut
    """

    if line_dir == 'x':
        # z along rows, y along columns
        return arr[c2, :, c1]
    elif line_dir == 'y':
        # x along columns, z along rows
        return arr[c2, c1, :]
    elif line_dir == 'z':
        # x along rows, y along columns
        return arr[:, c1, c2]


def counted(fn):
    def wrapper(self):
        wrapper.called += 1
        return fn(self)
    wrapper.called = 0
    wrapper.__name__ = fn.__name__
    return wrapper


def _call_func(quantity, obj, args):
    """
    Calls an instance method of an object with args
    """

    log = logging.getLogger(__name__)
    try:
        result = getattr(obj, quantity)(*args)
    except AttributeError:
        log.error("Object %s has no method: %s", str(obj), quantity)
        raise
    except:
        log.error("Error while calling method %s of object %s", quantity,
                  str(obj))
        raise
    return result


def execute_sim_plan(conf, outdir, proc_config, bandwidth, grouped_against=None, grouped_by=None):
    """
    Executes the given plan for the given Config object or list/tuple of Config
    objects. If conf is a single Config object, a Simulation object will
    created and the provided plan will be executed for it. If a list or tuple
    of Config objects is given, a SimulationGroup object will be created and
    the provided plan will be executed for it.

    `plan` must be a dictionary with the following structure::

        {'crunch':
            'funcA':
                compute: True
                args: [['list', 'of', 'args'], ['another', 'set']]
            'funcB':
                compute: False
                args: [['this function is skipped']]
         'plot': 'same structure as crunch'
        }

    For each of the keys under the `crunch` and `plot` sections, a method of
    the created Simulation/SimulationGroup object with the same name will be
    called for each set of provided arguments. No attempts are made to validate
    the given plan before raising exceptions
    """

    log = logging.getLogger(__name__)
    try:
        log.info("Executing Simulation plan")
        plan = proc_config['Postprocessing']['Single']
        spectrum = proc_config['Postprocessing'].get('spectrum', 'am1.5g')
        rescale_method = proc_config['Postprocessing'].get('rescale_method',
                                                           False)
        sim = Simulation(outdir, bandwidth, conf=conf,
                         spectrum=spectrum,
                         rescale_method=rescale_method)
        for task_name in ('crunch', 'plot'):
            if task_name not in plan:
                continue
            task = plan[task_name]
            log.info("Beginning %s for sim %s", task_name, sim.ID)
            for func, data in task.items():
                if not data['compute']:
                    continue
                else:
                    argsets = data['args']
                if argsets and isinstance(argsets[0], list):
                    for argset in argsets:
                        if argset:
                            _call_func(func, sim, argset)
                        else:
                            _call_func(func, sim, [])
                else:
                    if argsets:
                        _call_func(func, sim, argsets)
                    else:
                        _call_func(func, sim, [])
            log.info("Completed %s for sim %s", task_name, sim.ID)
        log.info("Plan execution for sim %s complete", sim.ID)
        log.info("Saving and clearing data for Simulation %s", sim.ID)
        # Write out to disk
        sim.write_data()
        # Close file handle to disk HDF5 file safely
        sim.data.close()
        # Clear data out of memory
        sim.clear_data()
        log.info("Data writing for Simulation %s complete!", sim.ID)
    except Exception as e:
        trace = traceback.format_exc()
        log.error('Conf %s raised folowing exception:\n%s', conf.ID, trace)
        try:
            sim.close()
        except:
            pass
        raise


def execute_group_plan(sim_confs_and_dirs, base_dir, proc_config, bandwidth,
                       grouped_by=None, grouped_against=None):
    log = logging.getLogger(__name__)
    sim_group = SimulationGroup(base_dir, bandwidth,
                                sim_confs_and_dirs=sim_confs_and_dirs,
                                grouped_by=grouped_by,
                                grouped_against=grouped_against)
    plan = proc_config['Postprocessing']['Group']
    try:
        log.info("Executing SimulationGroup plan")
        sim_group.save_group_info()
        for task_name in ('crunch', 'plot'):
            if task_name not in plan:
                continue
            task = plan[task_name]
            log.info("Beginning %s for group %s", task_name, sim_group.ID)
            for func, data in task.items():
                if not data['compute']:
                    continue
                else:
                    argsets = data['args']
                if argsets and isinstance(argsets[0], list):
                    for argset in argsets:
                        if argset:
                            _call_func(func, sim_group, argset)
                        else:
                            _call_func(func, sim_group, [])
                else:
                    if argsets:
                        _call_func(func, sim_group, argsets)
                    else:
                        _call_func(func, sim_group, [])
            log.info("Completed %s for group %s", task_name, sim_group.ID)
        log.info("Writing data for SimulationGroup %s", sim_group.ID)
        sim_group.write_data()
        log.info("Closing data for SimulationGroup %s", sim_group.ID)
        sim_group.close()
        log.info("Clearing data for SimulationGroup %s", sim_group.ID)
        sim_group.clear_data()
    except Exception as e:
        # log.error('Group {} raised exception'.format(sim_group.ID))
        # exc_type, exc_inst, trace = sys.exc_info()
        trace = traceback.format_exc()
        log.error('Group %s raised exception:\n%s', sim_group.ID, trace)
        sim_group.close()
        raise


class Processor:
    """
    Generic class for automating the processing of Simulations and
    SimulationGroups
    """

    def __init__(self, db, template, base_dir='', num_cores=0):
        if not osp.isfile(db):
            raise ValueError("Path {} to conf file doesn't "
                             "exist".format(db))
        self.db = unqlite.UnQLite(db)
        # if isinstance(conf, str):
        #     if not osp.isfile(conf):
        #         raise ValueError("Path {} to conf file doesn't "
        #                          "exist".format(conf))
        #     self.gconf = Config.fromFile(osp.abspath(conf))
        # elif isinstance(conf, Config):
        #     self.gconf = conf
        # else:
        #     msg = "Must pass in either path to a config file or a Config " \
        #           "object"
        #     raise ValueError(msg)
        self.template = Config.fromFile(template)
        self.base_dir = base_dir if base_dir else osp.dirname(db)
        self.log = logging.getLogger(__name__)
        self.log.debug("Processor base init")
        self.num_cores = num_cores
        self.sim_confs = []
        self.sims = {}
        self.conf_groups = []
        self.sim_groups = []
        self.grouped_against = None
        self.grouped_by = None
        self.bandwidths = None

    def load_confs(self, *args, **kwargs):
        """
        Load configs from disk

        Collect all the simulations contained in the HDF5 database file located
        at filesystem path `db` satisfying the query `query`.

        Parameters
        ----------
        base_dir : str, optional
            The base directory of the directory tree that all simulations will
            dump their output data into. If not specified, defaults to the
            directory of the database file.
        query : str, optional
            A query string conforming to PyTables table.where syntax specifying
            which configs should be loaded from the HDF5 database. If not
            specified, all configs in the database are loaded
        table_path : str, optional
            String specifying path inside the HDF5 database to the group
            containing the HDF5 simulations table. Default: '/'
        table_path : str, optional
            String specifying the name of the HDF5 simulations table.
            Default: 'simulations'

        Returns
        -------
        list
            A list of :py:class:`nanowire.preprocess.config.Config` objects
        """
        self.log.info('Loading configs from database ...')
        confs, t_sweeps, db = load_confs(self.db, *args, **kwargs)
        self.t_sweeps = t_sweeps
        self.sim_confs = confs
        self.sims = {}
        self.conf_groups = []
        self.sim_groups = []
        return confs, t_sweeps

    def make_processing_configs(self):
        """
        Build out the list of postprocessing configurations for each individual
        simulation by parsing the postprocessing template with the
        configuration of each simulation accessible in names
        """
        self.log.info('Building simulation plans from template plan')
        ops = {'simpleeval': {'operators': UNSAFE_OPERATORS}}
        parser = Parser(fns={'Q': fn_pint_quantity,
                             'mag': fn_pint_magnitude},
                        params=ops, cache_graph=True)
        plans = {}
        for ID, (conf, conf_path) in self.sim_confs.items():
            parser.update_names({'P': conf})
            plan = parser.parse(self.template)
            plans[conf.ID] = plan
        self.log.info('Plan building complete!')
        return plans

    def make_sims(self, *args, **kwargs):
        self.log.info("Making %i Simulation objects", len(self.sim_confs))
        if self.bandwidths is None:
            bandwidths = self.calculate_bandwidth()
        else:
            bandwidths = self.bandwidths
        self.sims = None
        sims = {}
        for conf, conf_path in self.sim_confs.values():
            outdir = osp.dirname(conf_path)
            sim = Simulation(outdir, bandwidths[conf.ID], *args, conf=conf,
                             **kwargs)
            sims[sim.ID] = sim
        self.sims = sims
        return self.sims

    def make_groups(self):
        self.log.info("Making SimulationGroup objects")
        if self.bandwidths is None:
            bandwidths = self.calculate_bandwidth()
        else:
            bandwidths = self.bandwidths
        self.sim_groups = []
        groups = []
        if self.sims:
            for conf_group in self.conf_groups:
                sims = [self.sims[conf.ID] for conf in conf_group]
                group = SimulationGroup(self.base_dir, bandwidths, sims=sims,
                                        grouped_against=self.grouped_against,
                                        grouped_by=self.grouped_by)
                groups.append(group)
        else:
            for conf_group in self.conf_groups:
                confs_and_dirs = [(conf, osp.dirname(self.sim_confs[conf.ID][1])) for conf in
                                  conf_group]
                sim_group = SimulationGroup(self.base_dir, bandwidths,
                                            sim_confs_and_dirs=confs_and_dirs,
                                            grouped_against=self.grouped_against,
                                            grouped_by=self.grouped_by)
                groups.append(sim_group)
        self.sim_groups = groups
        return self.sim_groups

    def close_all(self):
        for sim in self.sims.values():
            if isinstance(sim, Simulation):
                self.log.debug('Closing sim %s', sim.ID[0:10])
                sim.data.close()
        for group in self.sim_groups:
            self.log.info('Closing group %s', group.ID[0:10])
            group.close()
        self.log.info("Closing simulation database")
        self.db.close()

    def group_against(self, key, **kwargs):
        """
        Group configs against particular parameter.

        Group a list of config objects against a particular parameter. Within
        each group, the parameter specified by `key` will vary, and all other
        parameters will remain fixed. Useful for examining the affect of a
        single parameter on simulation outputs, and for generating line plots
        with the parameter specified by `key` on the x-axis.

        Parameters
        ----------
        key : str
            A key specifying which parameter simulations will be grouped
            against.  Individual simulations within each group will be sorted
            in increasing order of this parameter, and all other parameters
            will remain constant within the group. Key can be a slash-separated
            string pointing to nested items in the config.  sort_key : str,
            optional An optional key used to sort the order of the inner lists
            within the returned outer list of groups. Works because all
            parameters within each internal group are constant (excluding the
            parameter specified by `key`). The outer list of group lists will
            be sorted in increasing order of the specified sort_key.
        skip_keys : list
            A list of keys to skip when comparing two Configs.

        Returns
        -------
        list
            A list of lists. Each inner list is a group, sorted in increasing
            order of the parameter `key`. All other parameters in each group
            are constant. The outer list may optionally be sorted in increasing
            order of an additonal `sort_key`

        Notes
        -----
        The config objects in the input list `confs` are not copied, and
        references to the original Config objects are stored in the returned
        data structure
        """

        confs = [tup[0] for tup in self.sim_confs.values()]
        self.conf_groups = _group_against(confs, key, **kwargs)
        self.grouped_against = key

    def group_by(self, key, **kwargs):
        """
        Groups simulations by the parameter associated with `key` in the
        config. Within each group, the parameter associated with `key` will
        remain fixed, and all other parameters may vary.

        Parameters
        ----------
        conf : list, tuple
            A list or tuple of :py:class:`nanowire.preprocess.config.Config`
            objects
        key : str
            The key for the parameter you wish to group by. Must be an
            forward-slash separate path-like string.
        sort_key : str, optional
            An optional key by which to sort the parameters within each group.
            For example, group by parameter A but sort each group in increasing
            order of parameter B. If a callable object is provided, that
            callable will be applied to each individual simulation in the group
            to generate the sort key. If a string is provided, it will be
            interpreted as a key in the config and the parameter associated
            with that key will be used.

        Returns
        -------
        list
            A singly nested list of lists. Each inner list contains a group of
            simulations.
        """
        confs = [tup[0] for tup in self.sim_confs.values()]
        self.conf_groups = _group_by(confs, key, **kwargs)
        self.grouped_by = key

    def param_vals(self, parseq):
        """
        Return all possible values of the provided parameter for this sweep
        """
        raise NotImplementedError("Just use that database!")
        vals = []
        for conf in self.sim_confs:
            val = conf[parseq]
            if val not in vals:
                vals.append(val)
        return vals

    def _process(self, func, args_list, kwargs_list):
        """
        Run function `func` serially for each pair of positional and keyword
        arguments. Basically just:

        .. code-block:: python
            for a, kw in zip(args, kwargs):
                func(*a, **kw)

        func : callable
            The function to run
        args_list : list
            A list of tuples/lists containing position arguments that will be expanded and passed to
            `func`
        """
        if self.num_cores:
            self.log.info('Beginning parallel processing using %s cores ...',
                          str(self.num_cores))
            with mp.Pool(processes=self.num_cores) as pool:
                results = {}
                for i, (a, kw) in enumerate(zip(args_list, kwargs_list)):
                    res = pool.apply_async(func, a, kw)
                    results[i] = res
                while results:
                    IDs = list(results.keys())
                    for ID in IDs:
                        res = results[ID]
                        self.log.debug('Sim %s', ID)
                        if res.ready():
                            success = res.successful()
                            self.log.info("SUCCESS RETURN: %s", str(success))
                            if success:
                                self.log.debug('Sim %s completed successfully!',
                                               ID)
                                res.get(10)
                                self.log.debug('Done getting Sim %s', ID)
                            else:
                                self.log.warning('Sim %s raised exception!',
                                                 ID)
                                res.wait(10)
                            del results[ID]
                        else:
                            self.log.debug('Sim %s not ready', ID)
                    self.log.debug('Cleaned results: %s',
                                   str(list(results.keys())))
                    time.sleep(1)
            pool.join()
        else:
            self.log.info('Beginning serial processing ...')
            for a, kw in zip(args_list, kwargs_list):
                func(*a, **kw)

    def calculate_bandwidth(self):
        confs = [tup[0] for tup in self.sim_confs.values()]
        freq_groups = _group_against(confs, 'Simulation/frequency')
        bandwidths = {}
        for group in freq_groups:
            # for c1, c2 in pairwise(group):
            #     bandwidth = abs(c1['Simulation/frequency'] -
            #                     c2['Simulation/frequency'])
            #     bandwidths.append(bandwidth)
            N = len(group) - 1
            if N == 0:
                conf = group[0]
                bandwidths[conf.ID] = None
            else:
                for i, conf in enumerate(group):
                    center_freq = conf['Simulation/frequency']
                    if i == 0:
                        # Left endpoint
                        next_freq = group[i+1]['Simulation/frequency']
                        halfwidth = (next_freq - center_freq)/2
                        bandwidths[conf.ID] = (center_freq - halfwidth,
                                               center_freq + halfwidth)
                    elif i == N:
                        # Right endpoint
                        prev_freq = group[i-1]['Simulation/frequency']
                        halfwidth = (center_freq - prev_freq)/2
                        bandwidths[conf.ID] = (center_freq - halfwidth,
                                               center_freq + halfwidth)
                    else:
                        # Middle frequency
                        left_freq = group[i-1]['Simulation/frequency']
                        right_freq = group[i+1]['Simulation/frequency']
                        endpoints = (center_freq - (center_freq - left_freq)/2,
                                     center_freq + (right_freq - center_freq)/2)
                        bandwidths[conf.ID] = (endpoints)
        # if not all(np.isclose(el, bandwidths[0]) for el in bandwidths[1:]):
        #     msg = 'Cannot currently handle nonuniform frequency sweeps'
        #     raise NotImplementedError(msg)
        if bandwidths:
            self.bandwidths = bandwidths
            return bandwidths
        else:
            self.bandwidths = None
            return None

    def call_sim_method(self, method_name, args, parallel=True):
        """
        Run the method with the given name on all Simulation objects with the
        given args
        """

        if not self.sims:
            self.make_sims()
        if parallel:
            self.log.warning('Not actually running in parallel. Fix it!')
            for sim in self.sims.values():
                self.log.info("Calling %s on sim %s", method_name, sim.ID[0:10])
                _call_func(method_name, sim, args)
        else:
            for sim in self.sims.values():
                _call_func(method_name, sim, args)

    def process(self, crunch=True, plot=True, gcrunch=True, gplot=True,
                grouped_against=None, grouped_by=None, run_ids=False):
        # We need to calculate the bandwidth
        bandwidths = self.calculate_bandwidth()
        configs = self.make_processing_configs()
        # First process all the sims
        if crunch or plot:
            self.log.info('Processing individual simulations')
            # Each invididual simulation could have a different postprocessing
            # plan depending on the parameters of that simulation.
            for config in configs.values():
                if not crunch:
                    del config['Postprocessing']['Single']['crunch']
                if not plot:
                    del config['Postprocessing']['Single']['plot']
            if len(configs) != len(self.sim_confs):
                raise ValueError('Must have same number of plans as simulations!')
            # If we passed in run_IDs, only postprocess sims with those IDs
            if run_ids:
                self.log.info("Number of sims before filtering: %i",
                              len(configs))
                run_IDs = set(run_ids)
                configs = {ID: conf for ID, conf in configs.items() if ID in
                           run_IDs}
                self.log.info("Number of sims after filtering: %i",
                              len(configs))
            # NOTE: self.sim_confs[ID] = (Config_object, conf_path)
            args_list = [(self.sim_confs[ID][0],
                          osp.dirname(self.sim_confs[ID][1]), configs[ID],
                          bandwidths[ID]) for ID in configs.keys()]
            kws_list = [{'grouped_by': grouped_by, 'grouped_against':
                         grouped_against} for i in range(len(args_list))]
            self._process(execute_sim_plan, args_list, kws_list)
        else:
            self.log.info("No processing to do for individual simulations!")
        # Process groups
        if (gcrunch or gplot) and self.conf_groups:
            self.log.info('Processing simulation groups')
            group_configs = [configs[group[0].ID] for group in self.conf_groups]
            if len(group_configs) != len(self.conf_groups):
                raise ValueError('Must have same number of group plans as groups!')
            for config in group_configs:
                if not gcrunch:
                    del config['Postprocessing']['Group']['crunch']
                if not gplot:
                    del config['Postprocessing']['Group']['plot']
            # TODO: Fix this. It's kind of an icky hack because each Simulation
            # needs to know the directory where its data file is stored. This
            # will go away when all the data is stored in a single master HDF5
            # file
            for i in range(len(self.conf_groups)):
                self.conf_groups[i] = [(conf,
                                       osp.dirname(self.sim_confs[conf.ID][1]))
                                      for conf in self.conf_groups[i]]
            args_list = [(group, self.base_dir, plan, bandwidths) for group, plan in
                         zip(self.conf_groups, group_configs)]
            kws_list = [{'grouped_by': grouped_by, 'grouped_against':
                         grouped_against} for i in range(len(args_list))]
            self._process(execute_group_plan, args_list, kws_list)
        else:
            if not self.conf_groups:
                self.log.info("No available simulation groups to process!")
            else:
                self.log.info("No processing to do for simulation groups!")
