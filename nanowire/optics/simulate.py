import shutil
# import psutil
import os
import posixpath 
import sys
import copy
from multiprocessing.pool import Pool
import multiprocessing as mp
import threading
try:
    import Queue
except:
    import queue as Queue
import numpy as np
import scipy.optimize as optz
import tables as tb
# from tables.node import filenode
import time
import ruamel.yaml as yaml
import logging
import traceback
# import logging_tree
import S4
import scipy.interpolate as spi
import scipy.integrate as intg
import scipy.constants as constants
# import gc3libs
# from gc3libs.core import Core, Engine

from lxml import etree
from lxml.builder import E
# get our custom config object and the logger function
from . import postprocess as pp
from .utils.utils import (
    make_hash,
    get_combos,
    IdFilter,
    get_incident_amplitude,
    find_inds, 
    merge_and_sort
)
from .utils.config import Config
from .utils.geometry import Layer, get_layers

# from line_profiler import LineProfiler

def do_profile(follow=[], out=''):
    def inner(func):
        def profiled_func(*args, **kwargs):
            try:
                profiler = LineProfiler()
                profiler.add_function(func)
                for f in follow:
                    profiler.add_function(f)
                profiler.enable_by_count()
                return func(*args, **kwargs)
            finally:
                if out:
                    path = os.path.abspath(os.path.expandvars(out))
                    with open(path, 'a') as f:
                        profiler.print_stats(stream=f)
                else:
                    profiler.print_stats()
        return profiled_func
    return inner
# from rcwa_app import RCWA_App


# Configure logging for this module
# Get numeric level safely
logfile = 'logs/simulate.log'
debug = getattr(logging, 'debug'.upper(), None)
info = getattr(logging, 'info'.upper(), None)
warn = getattr(logging, 'warn'.upper(), None)
# Set formatting
formatter = logging.Formatter('%(asctime)s [%(name)s:%(levelname)s]'
                              ' - %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')
# Create logger
logger = logging.getLogger(__name__)
logger.setLevel(debug)
log_dir, logfile = os.path.split(os.path.expandvars(logfile))
# Set up file handler
try:
    os.makedirs(log_dir)
except OSError:
    # Log dir already exists
    pass
output_file = os.path.join(log_dir, logfile)
fhandler = logging.FileHandler(output_file)
fhandler.setFormatter(formatter)
fhandler.setLevel(debug)
# We dont want log records generated by Simulator instances making it to the
# global simulate.log file
fhandler.addFilter(IdFilter(reject=True))
logger.addHandler(fhandler)
# Logging to console
ch = logging.StreamHandler()
ch.setLevel(info)
ch.setFormatter(formatter)
ch.addFilter(IdFilter(reject=True))
logger.addHandler(ch)


# This will log any uncaught exceptions
def handle_exception(exc_type, exc_value, exc_traceback):
    if issubclass(exc_type, KeyboardInterrupt):
        sys.__excepthook__(exc_type, exc_value, exc_traceback)
        return

    logger.critical("Uncaught exception", exc_info=(exc_type, exc_value,
                                                    exc_traceback))


sys.excepthook = handle_exception


class LogExceptions(object):
    def __init__(self, callable):
        self.__callable = callable

    def __call__(self, *args, **kwargs):
        try:
            result = self.__callable(*args, **kwargs)
        except Exception as e:
            # Here we add some debugging help.
            log = logging.getLogger(__name__)
            log.error(traceback.format_exc())
            # Re-raise the original exception so the Pool worker can clean up.
            # This kills the parent process if it calls .get or .wait on the
            # AsyncResult object returned by apply_async
            # raise
        # It was fine, give a normal answer
        return result


class LoggingPool(Pool):
    def apply_async(self, func, args=(), kwds={}, callback=None):
        return Pool.apply_async(self, LogExceptions(func), args, kwds,
                                callback)


def parse_file(path):
    """Super simple utility to parse a yaml file given a path"""
    with open(path, 'r') as cfile:
        text = cfile.read()
    conf = yaml.load(text, Loader=yaml.Loader)
    return conf

def update_sim(conf, samples, q=None):
    """
    Wrapper for updating field arrays for a simulation. Expects the Config
    for the simulation as an argument.
    """
    
    try:
        log = logging.getLogger(__name__)
        start = time.time()
        conf['General']['z_samples'] = samples
        sim = Simulator(copy.deepcopy(conf))
        sim.setup()
        log.info('Updating arrays for sim %s', sim.id[0:10])
        sim.update_zsamples()
    except:
        trace = traceback.format_exc()
        msg = 'Sim {} raised the following exception:\n{}'.format(sim.id,
                                                                  trace)
        log.error(msg)
        # We might encounter an exception before the logger instance for this
        # sim gets created
        try:
            sim.log.error(trace)
        except AttributeError:
            pass
        raise

def run_sim(conf, q=None):
    """
    Actually runs simulation in a given directory. Expects the Config
    for the simulation as an argument.
    """
    log = logging.getLogger(__name__)
    start = time.time()
    sim = Simulator(copy.deepcopy(conf), q=q)
    try:
        if not sim.conf.variable_thickness:
            sim.setup()
            log.info('Executing sim %s', sim.id[0:10])
            sim.save_all()
            # path = os.path.join(os.path.basename(sim.dir), 'sim.hdf5')
            # sim.q.put(path, block=True)
            # sim.mode_solve()
        else:
            log.info('Computing a thickness sweep at %s' % sim.id[0:10])
            orig_id = sim.id[0:10]
            # Get all combinations of layer thicknesses
            keys, combos = get_combos(sim.conf, sim.conf.variable_thickness)
            # Update base directory to new sub directory
            sim.conf['General']['base_dir'] = sim.dir
            # Set things up for the first combo
            first_combo = combos.pop()
            # First update all the thicknesses in the config. We make a copy of the
            # list because it gets continually updated in the config object
            var_thickness = sim.conf.variable_thickness
            for i, param_val in enumerate(first_combo):
                keyseq = var_thickness[i]
                sim.conf[keyseq] = param_val
            # With all the params updated we can now run substutions and
            # evaluations in the config that make have referred to some thickness
            # data, then make the subdir from the sim id and get the data
            sim.evaluate_config()
            sim.update_id()
            try:
                os.makedirs(sim.dir)
            except OSError:
                pass
            sim.make_logger()
            subpath = os.path.join(orig_id, sim.id[0:10])
            log.info('Computing initial thickness at %s', subpath)
            sim.save_all()
            # path = os.path.join(sim.dir, 'data.hdf5')
            # sim.q.put(path, block=True)
            # Now we can repeat the same exact process, but instead of rebuilding
            # the device we just update the thicknesses
            for combo in combos:
                for i, param_val in enumerate(combo):
                    keyseq = var_thickness[i]
                    sim.conf[keyseq] = param_val
                sim.update_id()
                subpath = os.path.join(orig_id, sim.id[0:10])
                log.info('Computing additional thickness at %s', subpath)
                os.makedirs(sim.dir)
                sim.save_all(update=True)
                # path = os.path.join(sim.dir, 'data.hdf5')
                # sim.q.put(path, block=True)
        end = time.time()
        runtime = end - start
        log.info('Simulation %s completed in %.2f seconds!', sim.id[0:10], runtime)
        sim.clean_sim()
    except:
        trace = traceback.format_exc()
        msg = 'Sim {} raised the following exception:\n{}'.format(sim.id,
                                                                  trace)
        log.error(msg)
        # We might encounter an exception before the logger instance for this
        # sim gets created
        try:
            sim.log.error(trace)
        except AttributeError:
            pass
        raise
    return None


class LayerFlux(tb.IsDescription):
    layer = tb.StringCol(60, pos=0)
    forward = tb.ComplexCol(pos=1, itemsize=8)
    backward = tb.ComplexCol(pos=2, itemsize=8)



class FileMerger(threading.Thread):

    def __init__(self, q, write_dir='', group=None, target=None, name=None):
        super(FileMerger, self).__init__(group=group, target=target, name=name)
        self.q = q
        outpath = os.path.join(write_dir, 'data.hdf5')
        print('Main file is %s' % outpath)
        self.hdf5 = tb.open_file(outpath, 'w')

    def run(self):
        while True:
            # print('QSIZE: %i'%self.q.qsize())
            try:
                path = self.q.get(False)
            except Queue.Empty:
                time.sleep(.1)
                continue
            else:
                if path is None:
                    self.hdf5.close()
                    break
                subfile = tb.open_file(path, 'r')
                # assert subfile != self.hdf5
                # # Path is the string to the file we want to merge
                # # self.hdf5.copy_children(subfile.root, self.hdf5.root,
                # #                         recursive=True, overwrite=True)
                # # subfile.copy_children(subfile.root, self.hdf5.root,
                # #                       recursive=True)
                for group in subfile.iter_nodes('/', classname='Group'):
                    # abssubdir, subfname = os.path.split(path)
                    # subdir = os.path.basename(abssubdir)
                    # where = '{}:{}'.format(os.path.join(subdir, subfname),
                    #                        group._v_name)
                    # print('Saving here', where)
                    self.hdf5.create_external_link('/', group._v_name, group)
                subfile.close()
                #     print('Copying group ', group)
                #     # self.hdf5.copy_node(group, newparent=self.hdf5.root,
                #     #                   recursive=True)
                #     group._f_copy(newparent=self.hdf5.root, recursive=True)
        return


class FileWriter(threading.Thread):

    def __init__(self, q, write_dir='', group=None, target=None, name=None):
        super(FileWriter, self).__init__(group=group, target=target, name=name)
        self.q = q
        outpath = os.path.join(write_dir, 'data.hdf5')
        self.hdf5 = tb.open_file(outpath, 'a')

    def run(self):
        while True:
            # print('QSIZE: %i'%self.q.qsize())
            try:
                data = self.q.get(False)
            except Queue.Empty:
                time.sleep(.1)
                continue
            else:
                if data is None:
                    self.hdf5.close()
                    break
                # Data tuple contains the following:
                # (string of method name to call, args list, kwargs dict)
                getattr(self, data[0])(*data[1], **data[2])
        return

    def create_array(self, *args, **kwargs):
        """
        This method is a completely tranparent wrapper around the create_array
        method of a PyTables HDF5 file object. It passes through any arguments
        and keyword arguments through untouched
        """
        if 'compression' in kwargs and kwargs['compression']:
            del kwargs['compression']
            filter_obj = tb.Filters(complevel=4, complib='blosc')
            try:
                self.hdf5.create_carray(*args, filters=filter_obj, **kwargs)
            except tb.NodeError:
                self.hdf5.remove_node(args[0], name=args[1])
                self.hdf5.create_carray(*args, filters=filter_obj, **kwargs)
        else:
            try:
                self.hdf5.create_array(*args, **kwargs)
            except tb.NodeError:
                self.hdf5.remove_node(args[0], name=args[1])
                self.hdf5.create_array(*args, **kwargs)

    def create_flux_table(self, flux_dict, *args, **kwargs):
        """
        Creates the table of layer fluxes for a simulation. Expects a
        dictionary whose keys are layer names and whose values are tuples
        containing the (forward, backward) complex fluxes as arguments. All
        following args and kwargs are passed through to the create_table method
        of the PyTables file object
        """

        try:
            table = self.hdf5.create_table(*args, description=LayerFlux,
                                           **kwargs)
        except tb.NodeError:
            table = self.hdf5.get_node(args[0], name=args[1],
                                       classname='Table')
            table.remove_rows(0)
        row = table.row
        for layer, (forward, backward) in flux_dict.items():
            row['layer'] = layer
            row['forward'] = forward
            row['backward'] = backward
            row.append()
        table.flush()

    def save_attr(self, attr, path, name):
        """
        Save an attribute under the given name to a node in the config file
        """
        node = self.hdf5.get_node(path)
        node._v_attrs[name] = attr
        # fnode = filenode.new_node(self.hdf5, where=path, name='sim_conf.yml')
        # fnode.write(conf_str)
        # fnode.close()

    def clean_file(self, *args, **kwargs):
        """
        Deletes everything beneath the root group in the file
        """
        for node in self.hdf5.iter_nodes('/'):
            self.hdf5.remove_node(node._v_pathname, recursive=True)


class SimulationManager:

    """
    A class to manage running many simulations either in series or in parallel,
    collect and emit logs, write out data to files, etc
    """

    def __init__(self, gconf, log_level='INFO'):
        if os.path.isfile(gconf):
            self.gconf = Config(path=os.path.abspath(gconf))
        else:
            self.gconf = gconf
        # self.gconf.expand_vars()
        lfile = os.path.join(self.gconf['General']['base_dir'],
                             'logs/sim_manager.log')
        try:
            log_level = self.gconf['General']['log_level']
        except KeyError:
            pass
        # self.log = configure_logger(level=log_level, console=True,
        #                             logfile=lfile, name=__name__)
        self.log = logging.getLogger(__name__)
        self.sim_confs = []
        self.write_queue = None
        self.reader = None

    def make_queue(self):
        """
        Makes the queue for transferring data from simulation subprocesses to
        the FileWriter thread. Sets a maximum size on the queue based on the
        number of data points in the arrays and the total ram on the system.
        """
        total_mem = psutil.virtual_memory().total
        # If we have hardcoded in a fixed number of samples, we can compute the
        # number of data points here.
        samps = [self.gconf['General'][s] for s in ('x_samples',
                                                       'y_samples',
                                                       'z_samples')]
        # We can multiply by the ones that are hardcoded. For those
        # that are not, we have no way of evaluating the string expressions yet
        # so we'll just assume that they are 150 points
        # TODO: Maybe fix this random guessing
        max_points = 1
        for samp in samps:
            if type(samp) == int or type(samp) == float:
                max_points *= round(samp)
            else:
                max_points *= 150
        # Numpy complex128 consists of two 64 bit numbers, plus some overhead.
        # So 16 bytes + 8 bytes of overhead to be safe
        arr_mem = max_points*24
        # Subtract a gigabyte from total system memory to leave safety room
        maxsize = round((total_mem-(1024**3))/arr_mem)
        self.log.info('Maximum Queue Size: %i', maxsize)
        manager = mp.Manager()
        # We can go ahead and use maxsize directly because we left safety space
        # and there will also be items on the queue that are not massive arrays
        # and thus take up less space
        self.write_queue = manager.Queue(maxsize=maxsize)

    def make_listener(self):
        """
        Sets up the thread that listens to a queue for requests to write data
        to an HDF5 file. This prevents multiple subprocesses from attempting to
        write data to the HDF5 file at the same time
        """

        self.log.debug('Making listener')
        if self.write_queue is None:
            self.make_queue()
        basedir = self.gconf['General']['base_dir']
        self.reader = FileMerger(self.write_queue, write_dir=basedir)
        # self.reader = FileWriter(self.write_queue, write_dir=basedir)
        self.reader.start()

    def make_confs(self):
        """Make all the configuration dicts for each parameter combination"""
        self.log.info('Constructing simulator objects ...')
        locs, combos = get_combos(self.gconf, self.gconf.variable)
        for combo in combos:
            # Make a copy of the global config for this parameter combos. This copy
            # represents an individual simulation
            sim_conf = self.gconf.copy()
            if 'Postprocessing' in sim_conf:
                del sim_conf['Postprocessing']
            # Now we just overwrite all the variable parameters with their new
            # fixed values. Note that itertools.product is so wonderful and
            # nice that it preserves the order of the values in every combo
            # such that the combo values always line up with the proper
            # parameter name
            for i, combo in enumerate(combo):
                sim_conf[self.gconf.variable[i]] = combo
            self.sim_confs.append(sim_conf)

    def load_confs(self):
        """
        Collect all the simulations beneath the base of the directory tree
        """

        sims = []
        failed_sims = []
        # Find the data files and instantiate Config objects
        base = os.path.expandvars(self.gconf['General']['base_dir'])
        self.log.info(base)
        for root, dirs, files in os.walk(base):
            conf_path = os.path.join(root, 'sim_conf.yml')
            if 'sim_conf.yml' in files and 'sim.hdf5' in files:
                self.log.info('Gather sim at %s', root)
                conf_obj = Config(conf_path)
                # sim_obj.conf.expand_vars()
                sims.append(conf_obj)
            elif 'sim_conf.yml' in files:
                conf_obj = Config(conf_path)
                self.log.error('The following sim is missing its data file: %s',
                               sim_obj.conf['General']['sim_dir'])
                failed_sims.append(conf_obj)
        self.sim_confs = sims
        self.failed_sims = failed_sims
        if not sims:
            self.log.error('Unable to find any successful simulations')
            raise RuntimeError('Unable to find any successful simulations')
        return sims, failed_sims

    def execute_jobs(self, *args, func=run_sim, **kwargs):
        """
        Given a list of configuration dictionaries, run them either serially or
        in parallel by applying the provided func (default run_sim) to each
        dict. We do this instead of applying to an actual Simulator object
        because the Simulator objects are not pickeable and thus cannot be
        parallelized by the multiprocessing lib
        """

        if self.gconf['General']['execution'] == 'serial':
            self.log.info('Executing sims serially')
            # Make the write queue, then instanstiate and run the thread that
            # pulls data from the queue and writes to the HDF5 file
            # if self.gconf['General']['save_as'] == 'hdf5':
            #     self.make_listener()
            # else:
            #     self.make_queue()
            for conf in self.sim_confs:
                func(conf, q=self.write_queue)
            # self.write_queue.put(None, block=True)
            # if self.reader is not None:
            #     self.log.info('Joining FileWriter thread')
            #     self.reader.join()
        elif self.gconf['General']['execution'] == 'parallel':
            # if self.gconf['General']['save_as'] == 'hdf5':
            #     self.make_listener()
            # else:
            #     self.make_queue()
            # All this crap is necessary for killing the parent and all child
            # processes with CTRL-C
            num_procs = self.gconf['General']['num_cores']
            self.log.info('Executing sims in parallel using %s cores ...', str(num_procs))
            # pool = LoggingPool(processes=num_procs)
            pool = mp.Pool(processes=num_procs)
            total_sims = len(self.sim_confs)
            remaining_sims = len(self.sim_confs)
            def callback(ind):
                callback.remaining_sims -= 1
                callback.log.info('%i out of %i simulations remaining'%(callback.remaining_sims,
                                                                callback.total_sims))
            callback.remaining_sims = remaining_sims
            callback.total_sims = total_sims
            callback.log = self.log
            results = {}
            # results = []
            self.log.debug('Entering try, except pool clause')
            inds = []
            try:
                for ind, conf in enumerate(self.sim_confs):
                    res = pool.apply_async(func, (conf, *args),
                                           {'q':self.write_queue, **kwargs},
                                           callback=callback)
                    results[ind] = res
                    inds.append(ind)
                self.log.debug("Waiting on results")
                self.log.debug('Results before wait loop: %s',
                               str(list(results.keys())))
                for ind in inds:
                    # We need to add this really long timeout so that
                    # subprocesses receive keyboard interrupts. If our
                    # simulations take longer than this timeout, an exception
                    # would be raised but that should never happen
                    res = results[ind]
                    self.log.debug('Sim #: %s', str(ind))
                    res.wait(99999999)
                    # res.get(99999999)
                    self.log.debug('Done waiting on Sim ID %s', str(ind))
                    del results[ind]
                    self.log.debug('Cleaned results: %s',
                                   str(list(results.keys())))
                    # self.log.debug('Number of items in queue: %i',
                    #                self.write_queue.qsize())
                self.log.debug('Finished waiting')
                pool.close()
            except KeyboardInterrupt:
                pool.terminate()
            self.log.debug('Joining pool')
            pool.join()
            # self.write_queue.put(None, block=True)
            # if self.reader is not None:
            #     self.log.info('Joining FileWriter thread')
            #     self.reader.join()
            # for res in results:
            #     print(res)
            #     print(res.get())
        elif self.gconf['General']['execution'] == 'gc3':
            self.log.info('Executing jobs using gc3 submission tools')
            self.gc3_submit(self.gconf, self.sim_confs)
        self.log.info('Finished executing jobs!')

    def spectral_wrapper(self, opt_pars):
        """A wrapper function to handle spectral sweeps and postprocessing for the scipy minimizer. It
        accepts the initial guess as a vector, the base config file, and the keys that map to the
        initial guesses. It runs a spectral sweep (in parallel if specified), postprocesses the results,
        and returns the spectrally weighted reflection"""
        # TODO: Pass in the quantity we want to optimize as a parameter, then compute and return that
        # instead of just assuming reflection

        # Optimizing shell thickness could result is negative thickness so we need to take absolute
        # value here
        # opt_pars[0] = abs(opt_pars[0])
        self.log.info('Param keys: %s', str(self.gconf.optimized))
        self.log.info('Current values %s', str(opt_pars))
        # Clean up old data unless the user asked us not to. We do this first so on the last
        # iteration all our data is left intact
        basedir = self.gconf['General']['base_dir']
        ftype = self.gconf['General']['save_as']
        hdf_file = os.path.join(basedir, 'data.hdf5')
        if not self.gconf['General']['opt_keep_intermediates']:
            for item in os.listdir(basedir):
                if os.path.isdir(item) and item != 'logs':
                    shutil.rmtree(item)
                if 'data.hdf5' in item:
                    os.remove(hdf_file)

        # Set the value key of all the params we are optimizing over to the current
        # guess
        for i in range(len(self.gconf.optimized)):
            keyseq = self.gconf.optimized[i]
            self.log.info(keyseq)
            self.gconf[keyseq] = opt_pars[i]
        # Make all the sim objects
        self.sim_confs = []
        sims = self.make_confs()
        # Let's reuse the convergence information from the previous iteration if it exists
        # NOTE: This kind of assumes your initial guess was somewhat decent with regards to the in plane
        # geometric variables and the optimizer is staying relatively close to that initial guess. If
        # the optimizer is moving far away from its previous guess at each step, then the fact that a
        # specific frequency may have been converged previously does not mean it will still be converged
        # with this new set of variables.
        info_file = os.path.join(basedir, 'conv_info.txt')
        if os.path.isfile(info_file):
            conv_dict = {}
            with open(info_file, 'r') as info:
                for line in info:
                    freq, numbasis, conv_status = line.strip().split(',')
                    if conv_status == 'converged':
                        conv_dict[freq] = (True, numbasis)
                    elif conv_status == 'unconverged':
                        conv_dict[freq] = (False, numbasis)
            for sim in sims:
                freq = str(sim.conf['Simulation']['params']['frequency'])
                conv, numbasis = conv_dict[freq]
                # Turn off adaptive convergence and update the number of basis
                # terms
                if conv:
                    self.log.info('Frequency %s converged at %s basis terms', freq, numbasis)
                    sim.conf['General']['adaptive_convergence'] = False
                    sim.conf['Simulation']['params']['numbasis'] = int(numbasis)
                # For sims that haven't converged, set the number of basis terms to the last
                # tested value so we're closer to our goal of convergence
                else:
                    self.log.info('Frequency %s converged at %s basis terms', freq, numbasis)
                    sim.conf['Simulation']['params']['numbasis'] = int(numbasis)
        # With the leaf directories made and the number of basis terms adjusted,
        # we can now kick off our frequency sweep
        self.execute_jobs()
        # We need to wait for the writer thread to empty the queue for us
        # before we can postprocess the data
        if ftype == 'hdf5' and self.write_queue is not None:
            while not self.write_queue.empty():
                self.log.info('Waiting for queue to empty')
                self.log.info(self.write_queue.qsize())
                time.sleep(.1)
        #####
        # TODO: This needs to be generalized. The user could pass in the name of
        # a postprocessing function in the config file. The function will be called
        # and used as the quantity for optimization
        #####

        # With our frequency sweep done, we now need to postprocess the results.
        # Configure logger
        # log = configure_logger('error','postprocess',
        #                          os.path.join(self.gconf['General']['base_dir'],'logs'),
        #                          'postprocess.log')
        # Compute transmission data for each individual sim
        cruncher = pp.Cruncher(self.gconf)
        cruncher.collect_sims()
        for sim in cruncher.sims:
            # cruncher.transmissionData(sim)
            sim.get_data()
            sim.transmissionData()
            sim.write_data()
        # Now get the fraction of photons absorbed
        gcruncher = pp.Global_Cruncher(
            self.gconf, cruncher.sims, cruncher.sim_groups, cruncher.failed_sims)
        gcruncher.group_against(
            ['Simulation', 'params', 'frequency', 'value'], self.gconf.variable)
        photon_fraction = gcruncher.fractional_absorbtion()[0]
        # Lets store information we discovered from our adaptive convergence procedure so we can resue
        # it in the next iteration.
        if self.gconf['General']['adaptive_convergence']:
            self.log.info('Storing adaptive convergence results ...')
            with open(info_file, 'w') as info:
                for sim in sims:
                    freq = sim.conf['Simulation']['params']['frequency']
                    conv_path = os.path.join(sim.dir, 'converged_at.txt')
                    nconv_path = os.path.join(sim.dir, 'not_converged_at.txt')
                    if os.path.isfile(conv_path):
                        conv_f = open(conv_path, 'r')
                        numbasis = conv_f.readline().strip()
                        conv_f.close()
                        conv = 'converged'
                    elif os.path.isfile(nconv_path):
                        conv_f = open(nconv_path, 'r')
                        numbasis = conv_f.readline().strip()
                        conv_f.close()
                        conv = 'unconverged'
                    else:
                        # If we were converged on a previous iteration, adaptive
                        # convergence was switched off and there will be no file to
                        # read from
                        conv = 'converged'
                        numbasis = sim.conf['Simulation']['params']['numbasis']
                    info.write('%s,%s,%s\n' % (str(freq), numbasis, conv))
            self.log.info('Finished storing convergence results!')
        #print('Total time = %f'%delta)
        #print('Num calls after = %i'%gcruncher.weighted_transmissionData.called)
        # This is a minimizer, we want to maximize the fraction of photons absorbed and thus minimize
        # 1 minus that fraction
        return 1 - photon_fraction


    def run_optimization(self):
        """Runs an optimization on a given set of parameters"""
        self.log.info("Running optimization")
        self.log.info(self.gconf.optimized)
        # Make sure the only variable parameter we have is a sweep through
        # frequency
        for keyseq in self.gconf.variable:
            if keyseq[-1] != 'frequency':
                self.log.error('You should only be sweep through frequency during an '
                          'optimization')
                quit()
        # Collect all the guesses
        guess = np.zeros(len(self.gconf.optimized))
        for i in range(len(self.gconf.optimized)):
            keyseq = self.gconf.optimized[i]
            par_data = self.gconf.get(keyseq)
            guess[i] = par_data['guess']
        # Max iterations and tolerance
        tol = self.gconf['General']['opt_tol']
        ftol = self.gconf['General']['func_opt_tol']
        max_iter = self.gconf['General']['opt_max_iter']
        os.makedirs(os.path.join(self.gconf['General']['base_dir'], 'opt_dir'))
        # Run the simplex optimizer
        opt_val = optz.minimize(self.spectral_wrapper,
                                guess,
                                method='Nelder-Mead',
                                options={'maxiter': max_iter, 'xatol': tol,
                                         'fatol': ftol, 'disp': True})
        self.log.info(opt_val.message)
        self.log.info('Optimal values')
        self.log.info(self.gconf.optimized)
        self.log.info(opt_val.x)
        # Write out the results to a file
        out_file = os.path.join(
            self.gconf['General']['base_dir'], 'optimization_results.txt')
        with open(out_file, 'w') as out:
            out.write('# Param name, value\n')
            for key, value in zip(self.gconf.optimized, opt_val.x):
                out.write('%s: %f\n' % (str(key), value))
        self.write_queue.put(None, block=True)
        return opt_val.x

    def run(self, *args, load=False, func=run_sim, **kwargs):
        """
        The main run methods that decides what kind of simulation to run based
        on the provided config objects
        """

        if not self.gconf.optimized:
            # Get all the sims
            if load:
                self.load_confs()
            else:
                self.make_confs()
            self.log.info("Executing job campaign")
            self.execute_jobs(func=func, *args, **kwargs)
        elif self.gconf.optimized:
            self.run_optimization()
        else:
            self.log.error('Unsupported configuration for a simulation run. Not a '
                           'single sim, sweep, or optimization. Make sure your sweeps are '
                           'configured correctly, and if you are running an optimization '
                           'make sure you do not have any sorting parameters specified')


class Simulator():

    def __init__(self, conf, q=None):
        self.conf = conf
        self.q = q
        numbasis = self.conf['Simulation']['params']['numbasis']
        period = self.conf['Simulation']['params']['array_period']
        self.id = make_hash(conf.data)
        sim_dir = os.path.join(self.conf['General']['base_dir'], self.id[0:10])
        self.conf['General']['sim_dir'] = sim_dir
        self.dir = os.path.expandvars(sim_dir)
        # self.dir = sim_dir
        self.s4 = S4.New(Lattice=((period, 0), (0, period)),
                         NumBasis=int(round(numbasis)))
        self.data = {}
        self.hdf5 = None
        self.runtime = 0
        self.period = period

    def __del__(self):
        """
        Need to make sure we close the file descriptor for the fileHandler that
        this instance attached to the module level logger. If we don't, we'll
        eventually use up all the available file descriptors on the system
        """
        self._clean_logger()

    def _clean_logger(self):
        """
        Cleans up all the logging stuff associated with this instance
        """
        # Sometimes we hit an error before the log object gets created and
        # assigned as an attribute. Without the try, except we would get an
        # attribute error which makes error messages confusing and useless
        try:
            self.fhandler.close()
            module_logger = logging.getLogger(__name__)
            module_logger.removeHandler(self.fhandler)
        except AttributeError:
            pass

    def setup(self):
        """
        Runs all the necessary setup functions so one can begin running the
        simulation and collecting data
        """
        self.evaluate_config()
        self.update_id()
        try:
            os.makedirs(self.dir)
        except OSError:
            pass
        self.make_logger()
        self.make_coord_arrays()
        self.configure()
        self.build_device()
        self.set_excitation()

    def get_layers(self):
        self.layers = get_layers(self)

    def make_coord_arrays(self):
        """
        Set the attributes that define the spatial coordinate arrays. We can't
        do this in __init__ because if we are doing a thickness sweep then
        layer thicknesses and hence max_depth have not yet been resolved in the
        config
        """

        self.xsamps = self.conf['General']['x_samples']
        self.ysamps = self.conf['General']['y_samples']
        self.zsamps = self.conf['General']['z_samples']
        self.X = np.linspace(0, self.period, self.xsamps)
        self.Y = np.linspace(0, self.period, self.ysamps)
        max_depth = self.conf['Simulation']['max_depth']
        if max_depth:
            self.log.debug('Computing up to depth of {} '
                           'microns'.format(max_depth))
            self.Z = np.linspace(0, max_depth, self.zsamps)
        else:
            self.log.debug('Computing for entire device')
            height = self.get_height()
            self.Z = np.linspace(0, height, self.zsamps)
        self.dx = self.X[1] - self.X[0]
        self.dy = self.Y[1] - self.Y[0]
        self.dz = self.Z[1] - self.Z[0]
        self.data.update({"xcoords": self.X, "ycoords": self.Y, "zcoords": self.Z})

    def add_interpolator(self, key, method='linear'):
        """
        Add an interpolator method to this object for the data located at
        self.data[key]. The data must reside on a regular grid, but the grid
        points do not need to be evenly spaced
        """

        values = self.data[key]
        points = (self.Z, self.X, self.Y)
        rgi = spi.RegularGridInterpolator(points, values, method=method,
                                          bounds_error=True)
        setattr(self, key, rgi)

    def open_hdf5(self):
        fpath = os.path.join(self.dir, 'sim.hdf5')
        self.hdf5 = tb.open_file(fpath, 'a')

    def clean_sim(self):
        try:
            self._clean_logger()
            del self.log
            del self.s4
        except AttributeError:
            pass

    def make_logger(self, log_level='info'):
        """Makes the logger for this simulation"""
        self._clean_logger()
        # Add the file handler for this instance's log file and attach it to
        # the module level logger
        self.fhandler = logging.FileHandler(os.path.join(self.dir, 'sim.log'))
        self.fhandler.addFilter(IdFilter(ID=self.id))
        formatter = logging.Formatter('%(asctime)s [%(name)s:%(levelname)s] - %(message)s',datefmt='%m/%d/%Y %I:%M:%S %p')
        self.fhandler.setFormatter(formatter)
        self.fhandler.setLevel(logging.DEBUG)
        log = logging.getLogger(__name__)
        log.addHandler(self.fhandler)
        # Store the logger adapter to the module level logger as an attribute.
        # We use this to log in any methods, and the sim_id of this instance
        # will get stored in the log record
        self.log = logging.LoggerAdapter(log, {'ID': self.id})
        self.log.debug('Logger initialized')

    def evaluate_config(self):
        """
        Expands all environment variables in the config and resolves all
        references
        """
        self.conf.interpolate()
        self.conf.evaluate()

    def update_id(self):
        """Update sim id. Used after changes are made to the config"""
        self.id = make_hash(self.conf.data)
        sim_dir = os.path.join(self.conf['General']['base_dir'], self.id[0:10])
        self.conf['General']['sim_dir'] = sim_dir
        sim_dir = os.path.expandvars(sim_dir)
        self.dir = sim_dir
        try:
            os.makedirs(sim_dir)
        except OSError:
            pass

    def set_numbasis(self, numbasis):
        """
        Set the number of basis terms. This function updates the number of
        basis terms in the config and also updates the s4 attribute. This is
        necessary because the interface to S4 (i.e the object stored at
        self.s4) requires the number of basis terms to be provided to the
        constructor of that object.

        .. note:: You will need to call self.setup() after calling this
        function
        """
        self.conf['Simulation']['params']['numbasis'] = numbasis
        self.s4 = S4.New(Lattice=((self.period, 0), (0, self.period)),
                         NumBasis=int(round(numbasis)))
        self.setup()

    def set_period(self, period):
        """
        Set the periodicity of the square array. This function updates the
        number of basis terms in the config and also updates the s4 attribute.
        This is necessary because the interface to S4 (i.e the object stored at
        self.s4) requires the lattice vectors of the unit cell to be provided
        to the constructor of that object.

        .. note:: You will need to call self.setup() after calling this
        function
        """
        self.conf['Simulation']['params']['array_period'] = period
        numbasis = self.conf['Simulation']['params']['numbasis']
        self.s4 = S4.New(Lattice=((period, 0), (0, period)),
                         NumBasis=int(round(numbasis)))
        self.setup()

    def configure(self):
        """Configure options for the RCWA solver"""
        if self.conf['General']['output_pattern']:
            prefix = os.path.join(self.id[0:10], "VectorField")
            self.s4.SetOptions(BasisFieldDumpPrefix=prefix, **self.conf['Solver'])
        else:
            self.s4.SetOptions(**self.conf['Solver'])

    def _get_epsilon(self, path):
        """Returns complex dielectric constant for a material by pulling in nk
        text file, interpolating, computing nk values at freq, and
        converting"""
        freq = self.conf['Simulation']['params']['frequency']
        # Get data
        freq_vec, n_vec, k_vec = np.loadtxt(path, unpack=True)
        # Get n and k at specified frequency via interpolation
        f_n = spi.interp1d(freq_vec, n_vec, kind='nearest',
                           bounds_error=False, fill_value='extrapolate')
        f_k = spi.interp1d(freq_vec, k_vec, kind='nearest',
                           bounds_error=False, fill_value='extrapolate')
        n, k = f_n(freq), f_k(freq)
        # Convert to dielectric constant
        # NOTE!!: This assumes the relative magnetic permability (mew) is 1
        epsilon_real = n**2 - k**2
        epsilon_imag = 2 * n * k
        epsilon = complex(epsilon_real, epsilon_imag)
        return epsilon

    def _get_incident_amplitude_anna(self):
        freq = self.conf['Simulation']['params']['frequency']
        path = '$HOME/software/nanowire/nanowire/spectra/Input_sun_power.txt'
        freq_vec, p_vec = np.loadtxt(os.path.expandvars(path), unpack=True)
        p_of_f = spi.interp1d(freq_vec, p_vec)
        intensity = p_of_f(freq)
        self.log.debug('Incident Intensity: %s', str(intensity))
        area = self.period*self.period
        power = intensity*area
        self.log.debug('Incident Power: %s', str(power))
        # We need to reduce amplitude of the incident wave depending on
        #  incident polar angle
        # E = np.sqrt(2 * constants.c*constants.mu_0*f_p(freq))*np.cos(polar_angle)
        E = np.sqrt(2 * constants.c * constants.mu_0 * intensity)
        self.log.debug('Incident Amplitude: %s', str(E))
        return E

    def set_excitation(self):
        """Sets the exciting plane wave for the simulation"""
        f_phys = self.conf['Simulation']['params']['frequency']
        self.log.debug('Physical Frequency = %E' % f_phys)
        c_conv = constants.c / self.conf['Simulation']['base_unit']
        f_conv = f_phys / c_conv
        self.s4.SetFrequency(f_conv)
        E_mag = get_incident_amplitude(self)
        # E_mag = self._get_incident_amplitude_anna()
        polar = self.conf['Simulation']['params']['polar_angle']
        azimuth = self.conf['Simulation']['params']['azimuthal_angle']
        # To define circularly polarized light from the point of view of the
        # source, basically just stick a j
        # (imaginary number) in front of one of your components. The component
        # you choose to stick the j in front of is a matter of convention. In
        # S4, if the incident azimuthal angle is 0, p-polarization is along
        # x-axis. Here, we choose to make the y-component imaginary. The
        # handedness is determined both by the component you stick the j in
        # front of and the sign of the imaginary component. In our convention,
        # minus sign means rhcp, plus sign means lhcp. To be circularly
        # polarized, the magnitudes of the two components must be the same.
        # This means E-field vector rotates clockwise when observed from POV of
        # source. Left handed = counterclockwise.
        # TODO: This might not be properly generalized to handle
        # polarized light if the azimuth angle IS NOT 0. Might need some extra
        # factors of cos/sin of azimuth to gen proper projections onto x/y axes
        polarization = self.conf['Simulation']['polarization']
        if polarization == 'rhcp':
            # Right hand circularly polarized
            self.s4.SetExcitationPlanewave(IncidenceAngles=(polar, azimuth),
                                           sAmplitude=complex(0,
                                                              -E_mag/np.sqrt(2)),
                                           pAmplitude=complex(E_mag/np.sqrt(2), 0))
        elif polarization == 'lhcp':
            # Left hand circularly polarized
            self.s4.SetExcitationPlanewave(IncidenceAngles=(polar, azimuth),
                                           sAmplitude=complex(0,
                                                              E_mag/np.sqrt(2)),
                                           pAmplitude=complex(E_mag/np.sqrt(2), 0))
        elif polarization == 'lpx':
            # Linearly polarized along x axis (TM polarixation)
            self.s4.SetExcitationPlanewave(IncidenceAngles=(polar, azimuth),
                                           sAmplitude=complex(0, 0),
                                           pAmplitude=complex(E_mag, 0))
        elif polarization == 'lpy':
            # Linearly polarized along y axis (TE polarization)
            self.s4.SetExcitationPlanewave(IncidenceAngles=(polar, azimuth),
                                           sAmplitude=complex(E_mag, 0),
                                           pAmplitude=complex(0, 0))
        else:
            raise ValueError('Invalid polarization specification')

    def build_device(self):
        """Build the device geometry"""

        # First define all the materials
        for mat, mat_path in self.conf['Materials'].items():
            eps = self._get_epsilon(os.path.expandvars(mat_path))
            # eps = self._get_epsilon(mat_path)
            self.s4.SetMaterial(Name=mat, Epsilon=eps)
        self.s4.SetMaterial(Name='vacuum', Epsilon=complex(1, 0))
        # We need to properly sort our layers because order DOES matter. Light
        # will be incident upon the first layer specified
        for layer, ldata in sorted(self.conf['Layers'].items(),
                                   key=lambda tup: tup[1]['order']):
            self.log.debug('Building layer: %s' % layer)
            self.log.debug('Layer Order %i' % ldata['order'])
            base_mat = ldata['base_material']
            layer_t = ldata['params']['thickness']
            self.s4.AddLayer(Name=layer, Thickness=layer_t,
                             Material=base_mat)
            if 'geometry' in ldata:
                self.log.debug('Building geometry in layer: {}'.format(layer))
                for shape, sdata in sorted(ldata['geometry'].items(), key=lambda tup: tup[1]['order']):
                    self.log.debug('Building object {} of type {} at order'
                                  ' {}'.format(shape, sdata['type'], sdata['order']))
                    shape_mat = sdata['material']
                    if sdata['type'] == 'circle':
                        rad = sdata['radius']
                        cent = sdata['center']
                        coord = (cent['x'], cent['y'])
                        self.s4.SetRegionCircle(Layer=layer, Material=shape_mat, Center=coord,
                                                Radius=rad)
                    else:
                        raise NotImplementedError(
                            'Shape %s is not yet implemented' % sdata['type'])

    def get_height(self):
        """Get the total height of the device"""

        height = 0
        for layer, ldata in self.conf['Layers'].items():
            layer_t = ldata['params']['thickness']
            height += layer_t
        return height

    def set_lattice(self, period):
        """Updates the S4 simulation object with a new array period"""
        numbasis = self.conf['Simulation']['params']['numbasis']
        self.s4 = S4.New(Lattice=((period, 0), (0, period)), NumBasis=numbasis)

    def set_basis(self, numbasis):
        """Updates the S4 simulation object with a new set of basis terms"""
        period = self.conf['Simulation']['params']['array_period']
        self.s4 = S4.New(Lattice=((period, 0), (0, period)), NumBasis=numbasis)

    def update_thicknesses(self):
        """Updates all layer thicknesses without rebuilding the device. This
        allows reuse of any layer eigenmodes already computed and utilizes a
        fundamental efficiency of the RCWA solver"""
        for layer, ldata in self.conf['Layers'].items():
            thickness = ldata['params']['thickness']
            self.s4.SetLayerThickness(Layer=layer, Thickness=thickness)

    # @do_profile(out='$nano/tests/profile_writing/line_profiler.txt', follow=[])
    def compute_fields(self):
        """
        Constructs and returns a full 3D numpy array for each vector component
        of the electric field (return order Ex, Ey, Ez) according to the real
        space sampling points specified in the config file.
        """

        self.log.debug('Computing fields ...')
        Ex = np.zeros((self.zsamps, self.xsamps, self.ysamps),
                      dtype=np.complex128)
        Ey = np.zeros((self.zsamps, self.xsamps, self.ysamps),
                      dtype=np.complex128)
        Ez = np.zeros((self.zsamps, self.xsamps, self.ysamps),
                      dtype=np.complex128)
        if self.conf["General"]["compute_h"]:
            Hx = np.zeros((self.zsamps, self.xsamps, self.ysamps),
                          dtype=np.complex128)
            Hy = np.zeros((self.zsamps, self.xsamps, self.ysamps),
                          dtype=np.complex128)
            Hz = np.zeros((self.zsamps, self.xsamps, self.ysamps),
                          dtype=np.complex128)
        else:
            Hx, Hy, Hz = None, None, None
        for zcount, z in enumerate(self.Z):
            if self.conf["General"]["compute_h"]:
                E, H = self.s4.GetFieldsOnGrid(z=z, NumSamples=(self.xsamps-1,
                                                                self.ysamps-1),
                                               Format='Array')
                H_arr = np.array(H)
                Hx[zcount, :-1, :-1] = H_arr[:, :, 0]
                Hx[zcount, 0:self.xsamps-1, -1] = H_arr[:, 0, 0]
                Hx[zcount, -1, 0:self.ysamps-1] = H_arr[0, :, 0]
                Hx[zcount, -1, -1] = H_arr[0, 0, 0]
                Hy[zcount, :-1, :-1] = H_arr[:, :, 1]
                Hy[zcount, 0:self.xsamps-1, -1] = H_arr[:, 0, 1]
                Hy[zcount, -1, 0:self.ysamps-1] = H_arr[0, :, 1]
                Hy[zcount, -1, -1] = H_arr[0, 0, 1]
                Hz[zcount, :-1, :-1] = H_arr[:, :, 2]
                Hz[zcount, 0:self.xsamps-1, -1] = H_arr[:, 0, 2]
                Hz[zcount, -1, 0:self.ysamps-1] = H_arr[0, :, 2]
                Hz[zcount, -1, -1] = H_arr[0, 0, 2]
            else:
                start = time.time()
                E = self.s4.GetFieldsOnGrid(z=z,
                                            NumSamples=(self.xsamps-1,
                                                        self.ysamps-1),
                                            Format='Array')[0]
                end = time.time()
                self.log.debug('Time for S4 GetFieldsOnGrid call: %f',
                               end-start)
            E_arr = np.array(E)
            # Grab the periodic BC, which is always excluded from results
            # returned by S4 above
            Ex[zcount, :-1, :-1] = E_arr[:, :, 0]
            Ex[zcount, 0:self.xsamps-1, -1] = E_arr[:, 0, 0]
            Ex[zcount, -1, 0:self.ysamps-1] = E_arr[0, :, 0]
            Ex[zcount, -1, -1] = E_arr[0, 0, 0]
            Ey[zcount, :-1, :-1] = E_arr[:, :, 1]
            Ey[zcount, 0:self.xsamps-1, -1] = E_arr[:, 0, 1]
            Ey[zcount, -1, 0:self.ysamps-1] = E_arr[0, :, 1]
            Ey[zcount, -1, -1] = E_arr[0, 0, 1]
            Ez[zcount, :-1, :-1] = E_arr[:, :, 2]
            Ez[zcount, 0:self.xsamps-1, -1] = E_arr[:, 0, 2]
            Ez[zcount, -1, 0:self.ysamps-1] = E_arr[0, :, 2]
            Ez[zcount, -1, -1] = E_arr[0, 0, 2]
        self.log.debug('Finished computing fields!')
        return Ex, Ey, Ez, Hx, Hy, Hz

    def compute_fields_by_point(self):
        self.log.debug('Computing fields ...')
        Ex = np.zeros((self.zsamps, self.xsamps, self.ysamps),
                      dtype=np.complex128)
        Ey = np.zeros((self.zsamps, self.xsamps, self.ysamps),
                      dtype=np.complex128)
        Ez = np.zeros((self.zsamps, self.xsamps, self.ysamps),
                      dtype=np.complex128)
        if self.conf["General"]["compute_h"]:
            Hx = np.zeros((self.zsamps, self.xsamps, self.ysamps),
                          dtype=np.complex128)
            Hy = np.zeros((self.zsamps, self.xsamps, self.ysamps),
                          dtype=np.complex128)
            Hz = np.zeros((self.zsamps, self.xsamps, self.ysamps),
                          dtype=np.complex128)
        else:
            Hx, Hy, Hz = None, None, None
        for zcount, z in enumerate(self.Z):
            for i, x in enumerate(self.X):
                for j, y in enumerate(self.Y):
                    E, H = self.s4.GetFields(x, y, z)
                    Ex[zcount, i, j] = E[0]
                    Ey[zcount, i, j] = E[1]
                    Ez[zcount, i, j] = E[2]
                    if self.conf["General"]["compute_h"]:
                        Hx[zcount, i, j] = H[0]
                        Hy[zcount, i, j] = H[1]
                        Hz[zcount, i, j] = H[2]
        return Ex, Ey, Ez, Hx, Hy, Hz

    def compute_fields_at_point(self, x, y, z):
        """
        Compute the electric field at a specific point within the device and
        return a tuple of the components Ex, Ey, Ez
        """

        if self.conf["General"]["compute_h"]:
            E, H = self.s4.GetFields(x, y, z)
        else:
            E = self.s4.GetFields(x, y, z)[0]
            H = (None, None, None)
        return E[0], E[1], E[2], H[0], H[1], H[2]

    def compute_fields_on_plane(self, z, xsamples, ysamples):
        """
        Compute the electric field on an x-y plane at a given z value with a
        given number of samples in the x and y directions and return a tuple
        containg the 2D arrays of the fields components in (Ex, Ey, Ez) order

        This function is inclusive of the periodic endpoints at x=xmax and y=ymax
        """
        Ex = np.zeros((xsamples, ysamples), dtype=np.complex128)
        Ey = np.zeros((xsamples, ysamples), dtype=np.complex128)
        Ez = np.zeros((xsamples, ysamples), dtype=np.complex128)
        if self.conf["General"]["compute_h"]:
            Hx = np.zeros((xsamples, ysamples), dtype=np.complex128)
            Hy = np.zeros((xsamples, ysamples), dtype=np.complex128)
            Hz = np.zeros((xsamples, ysamples), dtype=np.complex128)
            E, H = self.s4.GetFieldsOnGrid(z=z, NumSamples=(xsamples-1,
                                                            ysamples-1),
                                           Format='Array')
            E_arr = np.array(E)
            H_arr = np.array(H)
        else:
            E = self.s4.GetFieldsOnGrid(z=z, NumSamples=(xsamples-1, ysamples-1),
                                        Format='Array')[0]
            E_arr = np.array(E)
            # Grab the periodic BC, which is always excluded from results
            # returned by S4 above
            Ex[:-1, :-1] = E_arr[:, :, 0]
            Ex[0:xsamples-1, -1] = E_arr[:, 0, 0]
            Ex[-1, 0:ysamples-1] = E_arr[0, :, 0]
            Ex[-1, -1] = E_arr[0, 0, 0]
            Ey[:-1, :-1] = E_arr[:, :, 1]
            Ey[0:xsamples-1, -1] = E_arr[:, 0, 1]
            Ey[-1, 0:ysamples-1] = E_arr[0, :, 1]
            Ey[-1, -1] = E_arr[0, 0, 1]
            Ez[:-1, :-1] = E_arr[:, :, 2]
            Ez[0:xsamples-1, -1] = E_arr[:, 0, 2]
            Ez[-1, 0:ysamples-1] = E_arr[0, :, 2]
            Ez[-1, -1] = E_arr[0, 0, 2]
        if self.conf["General"]["compute_h"]:
            H_arr = np.array(H)
            Hx[:-1, :-1] = H_arr[:, :, 0]
            Hx[0:xsamples-1, -1] = H_arr[:, 0, 0]
            Hx[-1, 0:ysamples-1] = H_arr[0, :, 0]
            Hx[-1, -1] = H_arr[0, 0, 0]
            Hy[:-1, :-1] = H_arr[:, :, 1]
            Hy[0:xsamples-1, -1] = H_arr[:, 0, 1]
            Hy[-1, 0:ysamples-1] = H_arr[0, :, 1]
            Hy[-1, -1] = H_arr[0, 0, 1]
            Hz[:-1, :-1] = H_arr[:, :, 2]
            Hz[0:xsamples-1, -1] = H_arr[:, 0, 2]
            Hz[-1, 0:ysamples-1] = H_arr[0, :, 2]
            Hz[-1, -1] = H_arr[0, 0, 2]
        else:
            Hx, Hy, Hz = None, None, None
        return Ex, Ey, Ez, Hx, Hy, Hz

    def get_field(self):
        start = time.time()
        if self.conf['General']['adaptive_convergence']:
            Ex, Ey, Ez, numbasis, conv = self.adaptive_convergence()
            self.data.update({'Ex':Ex,'Ey':Ey,'Ez':Ez})
            self.converged = (conv, numbasis)
        else:
            # Ex, Ey, Ez, Hx, Hy, Hz = self.compute_fields_by_point()
            Ex, Ey, Ez, Hx, Hy, Hz = self.compute_fields()
            if Hx is not None:
                self.data.update({'Ex':Ex,'Ey':Ey,'Ez':Ez,'Hx':Hx,'Hy':Hy,'Hz':Hz})
            else:
                self.data.update({'Ex':Ex,'Ey':Ey,'Ez':Ez})
                # self.data.update({'Ex':Ex.real,'Ey':Ey.real,'Ez':Ez.real})
        end = time.time()
        diff = end - start
        self.log.info("Time to compute fields: %f seconds", diff)

    def update_zsamples(self):
        """
        Update the field arrays stored on disk with data at new z sampling
        points. This is useful if you have changed the location of the sampling
        points, but do not want to delete all the old data, but rather merge
        the two datasets. This will still preserve the regularity of the
        gridded data because the x-y sampling remains unchanged, however the
        gridding may become nonuniform in the z direction
        """
        if self.hdf5 is None:
            self.open_hdf5()
        pbase = '/sim_{}'.format(self.id[0:10]) 
        # Make sure we have the field arrays
        if self.conf["General"]["compute_h"]:
            fields = ('Ex', 'Ey', 'Ez', 'Hx', 'Hy', 'Hz') 
        else:
            fields = ('Ex', 'Ey', 'Ez') 
        self.get_field()
        # Merge the old and the new coordinates, removing duplicates and
        # sorting
        old_z = self.hdf5.get_node(posixpath.join(pbase, 'zcoords')).read()
        new_z = merge_and_sort(self.Z, old_z)
        old_z_inds = find_inds(old_z, new_z)[0]
        curr_z_inds = find_inds(self.Z, new_z)[0]
        for field in fields:
            fpath = posixpath.join(pbase, field)
            # old_arr = self.hdf5.get_node(fpath)
            old_arr = self.hdf5.get_node(fpath).read()
            new_arr = np.zeros((len(new_z), old_arr.shape[1],
                                old_arr.shape[2]), dtype=np.complex128)
            curr_arr = self.data[field]
            # for i, zi in enumerate(old_z_inds):
            #     new_arr[zi, :, :] = old_arr[i, :, :]
            new_arr[old_z_inds, :, :] = old_arr[:, :, :]
            # for i, zi in enumerate(curr_z_inds):
            #     new_arr[zi, :, :] = curr_arr[i, :, :]
            new_arr[curr_z_inds, :, :] = curr_arr[:, :, :]
            self.data[field] = new_arr
            self.hdf5.remove_node(fpath)
        self.Z = new_z
        # Keep xy samples from old array
        self.conf['General']['x_samples'] = new_arr.shape[1]
        self.conf['General']['y_samples'] = new_arr.shape[2]
        self.conf['General']['z_samples'] = len(new_z)
        # self.data.update({"xcoords": old_x, "ycoords": old_y, "zcoords":
        #                   new_z})
        self.data.update({"zcoords": new_z})
        # Finally, save the new data
        # for c in ('xcoords', 'ycoords', 'zcoords'):
        #     self.hdf5.remove_node(posixpath.join(pbase, c))
        self.hdf5.remove_node(posixpath.join(pbase, 'zcoords'))

        self.save_data()
        self.save_conf()
        self.hdf5.close()

    def get_fluxes(self):
        """
        Get the fluxes at the top and bottom of each layer. This is a surface
        integral of the component of the Poynting flux perpendicular to this
        x-y plane of the interface, and have forward and backward components.
        Returns a dict where the keys are the layer name and the values are a
        length 2 tuple with the forward component first and the backward
        component second. The components are complex numbers
        """
        self.log.debug('Computing fluxes ...')
        rows = len(list(self.conf['Layers'].keys()))*2
        dt = [('layer', 'S25'), ('forward', np.complex128), ('backward', np.complex128)]
        flux_arr = np.recarray((rows,), dtype=dt)
        counter = 0
        for layer, ldata in self.conf['Layers'].items():
            self.log.debug('Computing fluxes through layer: %s' % layer)
            # This gets flux at top of layer
            forw, back = self.s4.GetPowerFlux(Layer=layer)
            flux_arr[counter] = (layer, forw, back)
            # This gets flux at the bottom
            offset = ldata['params']['thickness']
            forw, back = self.s4.GetPowerFlux(Layer=layer, zOffset=offset)
            key = layer + '_bottom'
            flux_arr[counter+1] = (key, forw, back)
            counter += 2
        self.data['fluxes'] = flux_arr
        self.log.debug('Finished computing fluxes!')
        return flux_arr

    def compute_dielectric_profile(self):
        """
        Gets the dielectric profile throughout the device. This is useful for
        determining the resolution of the dielectric profile used by S4. It uses
        the same number of sampling points as specified in the config file for
        retrieiving field data.
        """
        self.log.debug('Computing dielectric profile ...')
        xv, yv = np.meshgrid(self.X, self.Y, indexing='ij')
        z = self.dz
        for layer, ldata in sorted(self.conf['Layers'].items(),
                                   key=lambda tup: tup[1]['order']):
            self.log.debug('Computing epsilon at z = %f in layer %s', z, layer)
            eps_mat = np.zeros((self.xsamps, self.ysamps), dtype=np.complex128)
            for ix in range(self.xsamps):
                for iy in range(self.ysamps):
                    eps_val =  self.s4.GetEpsilon(xv[ix, iy], yv[ix, iy],
                                                  z)
                    eps_mat[ix, iy] = eps_val
            key = 'dielectric_profile_{}'.format(layer)
            self.data[key] = eps_mat
            z += ldata['params']['thickness']
        self.log.debug('Finished computing dielectric profile!')

    def compute_dielectric_profile_at_point(self, x, y, z):
        return self.s4.GetEpsilon(x, y, z)

    def get_fourier_coefficients(self, offset=0.):
        """
        Return a list of the Fourier coefficients used to approximate the
        fields
        """
        self.log.info('Retrieving Fourier coefficients')
        for layer, ldata in self.conf['Layers'].items():
            self.log.debug("Layer: {}".format(layer))
            if offset == 0.:
                self.log.debug('Offset zero, not using that arg to avoid bug')
                res = self.s4.GetAmplitudes(Layer=layer)
            else:
                self.log.debug('Using offset')
                res = self.s4.GetAmplitudes(Layer=layer, zOffset=offset)
            forw = np.array(res[0])
            backw = np.array(res[1])
            coeff_arr = np.row_stack((forw, backw))
            key = '{}_amplitudes'.format(layer)
            self.data[key] = coeff_arr
        self.log.info('Finished computing coefficients!')
        return {key:val for key,val in self.data.items() if '_amplitudes' in
                key}

    def load_state(self):
        """
        Load solution from disk
        """
        self.log.info("Loading simulation state")
        sfile = self.conf['General']['solution_file']
        sdir = self.conf['General']['sim_dir']
        fname = os.path.expandvars(os.path.join(sdir, sfile))
        # print(fname)
        if os.path.isfile(fname):
            self.log.info("Loading from: %s"%fname)
            # print("Loading from: %s"%fname)
            self.s4.LoadSolution(Filename=fname)
            self.log.info("Solution loaded!")
            # print("Solution loaded!")
        else:
            self.log.warning("Solution file does not exist. Cannot load")
            # print("Solution file does not exist. Cannot load")

    def save_state(self):
        """
        Save solution to disk
        """
        self.log.info("Saving simulation state")
        sfile = self.conf['General']['solution_file']
        sdir = self.conf['General']['sim_dir']
        fname = os.path.expandvars(os.path.join(sdir, sfile))
        self.log.info("Saving to: %s" % fname)
        if os.path.isfile(fname):
            self.log.info("State file exists, skipping save")
        else:
            self.s4.SaveSolution(Filename=fname)
            self.log.info("Solution saved!")
        self.s4.SaveSolution(Filename=fname)
        self.log.info("Solution saved!")
    # def get_integrals(self):
    #     self.log.debug('Computing volume integrals')
    #     integrals = {}
    #     for layer, ldata in self.conf['Layers'].items():
    #         self.log.debug('Computing integral through layer: %s' % layer)
    #         result = self.s4.GetLayerVolumeIntegral(Layer=layer, Quantity='E')
    #         self.log.debug('Integral = %s', str(result))
    #         integrals[layer] = result
    #     print(integrals)
    #     return integrals

    def write_xdmf(self):
        """
        Writes an XMDF file for the electric fields, allowing import into
        Paraview for visualization
        """

        grid = E.Grid
        domain = E.Domain
        topo = E.Topology
        geo = E.Geometry
        ditem = E.DataItem
        attr = E.Attribute
        base = 'sim.hdf5:/sim_{}'.format(self.id[0:10])
        dims = '{} {} {}'.format(self.zsamps, self.xsamps, self.ysamps)
        doc = (
        E.Xdmf({'Version': '3.0'},
            domain(
                grid({'GridType': 'Uniform', 'Name': 'FullGrid'},
                    topo({'TopologyType': '3DRectMesh'}),
                    geo({'GeometryType': 'VXVYVZ'},
                       ditem(base+'/xcoords', {'Name': 'xcoords',
                                               'Dimensions': str(self.xsamps),
                                               'NumberType': 'Float',
                                               'Precision': '4',
                                               'Precision': '4',
                                               'Format': 'HDF',
                                               'Compression': 'Zlib'}),
                       ditem(base+'/ycoords', {'Name': 'ycoords',
                                               'Dimensions': str(self.ysamps),
                                               'NumberType': 'Float',
                                               'Precision': '4',
                                               'Format': 'HDF',
                                               'Compression': 'Zlib'}),
                       ditem(base+'/zcoords', {'Name': 'zcoords',
                                               'Dimensions': str(self.zsamps),
                                               'NumberType': 'Float',
                                               'Precision': '4',
                                               'Format': 'HDF',
                                               'Compression': 'Zlib'}),
                    ),
                    attr({'Name': 'Electric Field Components', 'AttributeType': 'Scalar',
                          'Center': 'Node'},
                        ditem(base+'/Ex', {'Dimensions': dims}),
                        ditem(base+'/Ey', {'Dimensions': dims}),
                        ditem(base+'/Ez', {'Dimensions': dims})
                    )
                )
            )
        )
        )
        path = os.path.join(self.dir, 'sim.xdmf')
        with open(path, 'wb') as out:
            out.write(etree.tostring(doc, pretty_print=True))

    def save_data(self):
        """Saves the self.data dictionary to an npz file. This dictionary
        contains all the fields and the fluxes dictionary"""

        start = time.time()
        if self.hdf5 is None:
            self.open_hdf5()
        if self.conf['General']['save_as'] == 'npz':
            self.log.debug('Saving fields to NPZ')
            if self.conf['General']['adaptive_convergence']:
                if self.converged[0]:
                    out = os.path.join(self.dir, 'converged_at.txt')
                else:
                    out = os.path.join(self.dir, 'not_converged_at.txt')
                self.log.debug('Writing convergence file ...')
                with open(out, 'w') as outf:
                    outf.write('{}\n'.format(self.converged[1]))
            out = os.path.join(self.dir, self.conf["General"]["base_name"])
            # Compression adds a small amount of time. The time cost is
            # nonlinear in the file size, meaning the penalty gets larger as the
            # field grid gets finer. However, the storage gains are enormous!
            # Compression brought the file from 1.1G to 3.9M in a test case.
            # I think the compression ratio is so high because npz is a binary
            # format, and all compression algs benefit from large sections of
            # repeated characters
            np.savez_compressed(out, **self.data)
        elif self.conf['General']['save_as'] == 'hdf5':
            compression = self.conf['General']['compression']
            if compression:
                # filter_obj = tb.Filters(complevel=8, complib='blosc')
                filter_obj = tb.Filters(complevel=4, complib='zlib')
            gpath = '/sim_'+self.id[0:10]
            for name, arr in self.data.items():
                # Check for recarrays first because they are subclass of
                # ndarrays
                if isinstance(arr, np.recarray):
                    self.log.info("Saving record array %s", name)
                    num_rows = len(list(self.conf['Layers'].keys()))
                    table = self.hdf5.create_table(gpath, name,
                                                   description=arr.dtype,
                                                   expectedrows=num_rows,
                                                   createparents=True)
                    row = table.row
                    fields = arr.dtype.names
                    for record in arr:
                        for (i, el) in enumerate(record):
                            row[fields[i]] = el
                        row.append()
                    table.flush()
                elif isinstance(arr, np.ndarray):
                    self.log.debug("Saving array %s", name)
                    if compression:
                        self.hdf5.create_carray(gpath, name, createparents=True,
                                           atom=tb.Atom.from_dtype(arr.dtype),
                                           obj=arr, filters=filter_obj)
                    else:
                        self.hdf5.create_array(gpath, name, createparents=True,
                                          atom=tb.Atom.from_dtype(arr.dtype),
                                          obj=arr)

            # Write XMDF xml file for importing into Paraview
            self.write_xdmf()
            end = time.time()
            diff = end - start
            self.log.info('Time to write data to disk: %f seconds', diff)
                    # # Save the field arrays
                    # self.log.debug('Saving fields to HDF5')
                    # path = '/sim_'+self.id[0:10]
                    # for name, arr in self.data.items():
                # self.log.debug("Saving array %s", name)
                # tup = ('create_array', (path, name),
                    #    {'compression': self.conf['General']['compression'],
                    #     'createparents': True, 'obj': arr,
                    #     'atom': tb.Atom.from_dtype(arr.dtype)})
                # self.q.put(tup, block=True)
            # # Save the flux dict to a table
            # self.log.debug('Saving fluxes to HDF5')
            # self.log.debug(self.flux_dict)
            # tup = ('create_flux_table', (self.flux_dict, path, 'fluxes'),
                   # {'createparents': True,
                    # 'expectedrows': len(list(self.conf['Layers'].keys()))})
            # self.q.put(tup, block=True)
            self.hdf5.flush()
        else:
            raise ValueError('Invalid file type specified in config')

    def save_conf(self):
        """
        Saves the simulation config object to a file
        """
        if self.conf['General']['save_as'] == 'npz':
            self.log.debug('Saving conf to YAML file')
            self.conf.write(os.path.join(self.dir, 'sim_conf.yml'))
        elif self.conf['General']['save_as'] == 'hdf5':
            self.log.debug('Saving conf to HDF5 file')
            self.conf.write(os.path.join(self.dir, 'sim_conf.yml'))
            # path = '/sim_{}'.format(self.id[0:10])
            # try:
            #     node = self.hdf5.get_node(path)
            # except tb.NoSuchNodeError:
            #     self.log.warning('You need to create the group for this '
            #     'simulation before you can set attributes on it. Creating now')
            #     node = self.hdf5.create_group(path)
            # node._v_attrs['conf'] = self.conf.dump()
            # attr_name = 'conf'
            # tup = ('save_attr', (self.conf.dump(), path, attr_name), {})
            # self.q.put(tup, block=True)
        else:
            raise ValueError('Invalid file type specified in config')

    def save_time(self):
        """
        Saves the run time for the simulation
        """
        if self.conf['General']['save_as'] == 'npz':
            self.log.debug('Saving runtime to text file')
            time_file = os.path.join(self.dir, 'time.dat')
            with open(time_file, 'w') as out:
                out.write('{}\n'.format(self.runtime))
        elif self.conf['General']['save_as'] == 'hdf5':
            self.log.debug('Saving runtime to HDF5 file')
            path = '/sim_{}'.format(self.id[0:10])
            try:
                node = self.hdf5.get_node(path)
            except tb.NoSuchNodeError:
                self.log.warning('You need to create the group for this '
                'simulation before you can set attributes on it. Creating now')
                node = self.hdf5.create_group(path)
            node._v_attrs['runtime'] = self.runtime
            # attr_name = 'runtime'
            # tup = ('save_attr', (self.runtime, path, attr_name), {})
            # self.q.put(tup, block=True)
        else:
            raise ValueError('Invalid file type specified in config')

    def calc_diff(self, fields1, fields2, exclude=False):
        """Calculate the percent difference between two vector fields"""
        # This list contains three 3D arrays corresponding to the x,y,z
        # componenets of the e field. Within each 3D array is the complex
        # magnitude of the difference between the two field arrays at each
        # spatial point squared
        diffs_sq = [np.absolute(arr1 - arr2)**2 for arr1, arr2 in zip(fields1, fields2)]
        # Sum the squared differences of each component
        mag_diffs = sum(diffs_sq)
        # Now compute the norm(E)^2 of the comparison sim at each sampling
        # point
        normsq = sum([np.absolute(field)**2 for field in fields1])
        # We define the percent difference as the ratio of the sums of the
        # difference vector magnitudes to the comparison field magnitudes,
        # squared rooted.
        # TODO: This seems like a somewhat shitty metric that washes out any
        # large localized deviations. Should test other metrics
        diff = np.sqrt(np.sum(mag_diffs) / np.sum(normsq))
        self.log.debug('Percent difference = {}'.format(diff))
        return diff

    def adaptive_convergence(self):
        """Performs adaptive convergence by checking the error between vector
        fields for simulations with two different numbers of basis terms.
        Returns the field array, last number of basis terms simulated, and a
        boolean representing whether or not the simulation is converged"""
        self.log.debug('Beginning adaptive convergence procedure')
        start_basis = self.conf['Simulation']['params']['numbasis']
        basis_step = self.conf['General']['basis_step']
        ex, ey, ez = self.compute_fields()
        max_diff = self.conf['General']['max_diff']
        max_iter = self.conf['General']['max_iter']
        percent_diff = 100
        iter_count = 0
        while percent_diff > max_diff and iter_count < max_iter:
            new_basis = start_basis + basis_step
            self.log.debug('Checking error between {} and {} basis'
                          ' terms'.format(start_basis, new_basis))
            self.set_basis(new_basis)
            self.build_device()
            self.set_excitation()
            ex2, ey2, ez2 = self.compute_fields()
            percent_diff = self.calc_diff([ex, ey, ex], [ex2, ey2, ez2])
            start_basis = new_basis
            ex, ey, ez = ex2, ey2, ez2
            iter_count += 1
        if percent_diff > max_diff:
            self.log.warning('Exceeded maximum number of iterations')
            return ex2, ey2, ez2, new_basis, False
        else:
            self.log.debug('Converged at {} basis terms'.format(new_basis))
            return ex2, ey2, ez2, new_basis, True

    def mode_solve(self, update=False):
        """Find modes of the system. Supposedly you can get the resonant modes
        of the system from the poles of the S matrix determinant, but I
        currently can't really make any sense of this output"""
        if not update:
            self.configure()
            self.build_device()
        else:
            self.update_thicknesses()
        mant, base, expo = self.s4.GetSMatrixDeterminant()
        self.log.debug('Matissa: %s'%str(mant))
        self.log.debug('Base: %s'%str(base))
        res = mant*base**expo
        self.log.debug('Result: %s'%str(res))

    def save_all(self, update=False):
        """Gets all the data for this similation by calling the relevant class
        methods. Basically just a convenient wrapper to execute all the
        functions defined above"""
        # TODO: Split this into a get_all and save_all function. Will give more
        # granular sense of timing and also all getting data without having to
        # save
        start = time.time()
        if update:
            self.update_thicknesses()
        state_file = os.path.join(self.dir,
                                  self.conf['General']['solution_file'])
        if os.path.isfile(state_file):
            self.log.debug("State file exists: %s"%state_file)
            print("State file exists: %s"%state_file)
            self.load_state()
        self.get_field()
        self.get_fluxes()
        self.get_fourier_coefficients()
        if self.conf['General']['dielectric_profile']:
            self.compute_dielectric_profile()
        self.open_hdf5()
        self.save_data()
        self.save_conf()
        self.save_state()
        end = time.time()
        self.runtime = end - start
        self.save_time()
        self.hdf5.close()
        self.log.info('Simulation {} completed in {:.2}'
                      ' seconds!'.format(self.id[0:10], self.runtime))
        return
